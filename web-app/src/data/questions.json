[
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable.*",
    "options": [
      "Homoscedasticity",
      "Multicollinearity",
      "R-squared",
      "Spearman Correlation"
    ],
    "answer": 2,
    "explanation": "The definition describes **R-squared**.",
    "id": 89
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A general statement or default position that there is no relationship between two measured phenomena.*",
    "options": [
      "P-value",
      "Null Hypothesis",
      "Significance Level",
      "T-statistic"
    ],
    "answer": 1,
    "explanation": "The definition describes **Null Hypothesis**.",
    "id": 56
  },
  {
    "type": "tf",
    "question": "**Standard Error** is The standard deviation of the sampling distribution of a statistic.",
    "answer": true,
    "explanation": "**Standard Error (SEM)**: SD of the sampling distribution of the mean. `SEM = SD / sqrt(N)`. Decreases as N increases.",
    "id": 42,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "What is the best definition for **T-statistic**?",
    "options": [
      "The rejection of a true null hypothesis (false positive). (additional context) (a",
      "A ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.",
      "An inferential statistic used to assess the equality of variances for a variable calculated for two or more groups.",
      "The probability of rejecting the null hypothesis when it is true (alpha)."
    ],
    "answer": 1,
    "explanation": "**T-statistic** is defined as: A ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.",
    "id": 73
  },
  {
    "type": "tf",
    "question": "**Standard Deviation** is A measure of the amount of variation or dispersion of a set of values, calculated as the square root of variance.",
    "answer": true,
    "explanation": "True. Standard Deviation is indeed A measure of the amount of variation or dispersion of a set of values, calculated as the square root of variance.",
    "id": 30
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Shapiro-Wilk Test**?",
    "options": [
      "The probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct.",
      "The non-rejection of a false null hypothesis (false negative).",
      "A ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.",
      "A test of normality in frequentist statistics."
    ],
    "answer": 3,
    "explanation": "**Shapiro-Wilk Test** is defined as: A test of normality in frequentist statistics.",
    "id": 76
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Normal Distribution**?",
    "options": [
      "A continuous probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence.",
      "Two events are independent if the occurrence of one does not affect the probability of occurrence of the other.",
      "A function that describes the relative likelihood for a continuous random variable to take on a given value.",
      "The long-run average value of repetitions of the same experiment it represents. (additional context"
    ],
    "answer": 0,
    "explanation": "**Normal Distribution** is defined as: A continuous probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence.",
    "id": 10
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Central Limit Theorem**?",
    "options": [
      "The theory that the distribution of sample means approximates a normal distribution as the sample size becomes larger.",
      "A subset of a statistical population in which each member of the subset has an equal probability of being chosen.",
      "The standard deviation of the sampling distribution of a statistic. (additional context) (additional c",
      "A type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand."
    ],
    "answer": 0,
    "explanation": "**Central Limit Theorem** is defined as: The theory that the distribution of sample means approximates a normal distribution as the sample size becomes larger.",
    "id": 37
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*The assumption that the variance of the error term in a regression model is constant.*",
    "options": [
      "Homoscedasticity",
      "R-squared",
      "Spurious Correlation",
      "Pearson Correlation"
    ],
    "answer": 0,
    "explanation": "The definition describes **Homoscedasticity**.",
    "id": 98
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A theorem stating that the average of the results obtained from a large number of trials should be close to the expected value.*",
    "options": [
      "Stratified Sampling",
      "Sampling Bias",
      "Law of Large Numbers",
      "Convenience Sampling"
    ],
    "answer": 2,
    "explanation": "The definition describes **Law of Large Numbers**.",
    "id": 44
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Multicollinearity**?",
    "options": [
      "A phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.",
      "A non-parametric measure of rank correlation (statistical dependence between the rankings of two variables).",
      "The assumption that the variance of the error term in a regression model is constant.",
      "A measure of the linear correlation between two sets of data. (additional context) ("
    ],
    "answer": 0,
    "explanation": "**Multicollinearity** is defined as: A phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.",
    "id": 94
  },
  {
    "type": "tf",
    "question": "**Multicollinearity** is The assumption that the variance of the error term in a regression model is constant.",
    "answer": false,
    "explanation": "False. That is the definition of **Homoscedasticity**. **Multicollinearity** is A phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.",
    "id": 96
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*The theory that the distribution of sample means approximates a normal distribution as the sample size becomes larger.*",
    "options": [
      "Sampling Distribution",
      "Convenience Sampling",
      "Central Limit Theorem",
      "Simple Random Sampling"
    ],
    "answer": 2,
    "explanation": "The definition describes **Central Limit Theorem**.",
    "id": 38
  },
  {
    "type": "tf",
    "question": "**Sampling Distribution** is A type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand.",
    "answer": false,
    "explanation": "False. That is the definition of **Convenience Sampling**. **Sampling Distribution** is The probability distribution of a given random-sample-based statistic.",
    "id": 36
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*An inferential statistic used to assess the equality of variances for a variable calculated for two or more groups.*",
    "options": [
      "Power",
      "P-value",
      "T-statistic",
      "Levene's Test"
    ],
    "answer": 3,
    "explanation": "The definition describes **Levene's Test**.",
    "id": 80
  },
  {
    "type": "tf",
    "question": "**Type I Error** is The rejection of a true null hypothesis (false positive).",
    "answer": true,
    "explanation": "True. Type I Error is indeed The rejection of a true null hypothesis (false positive).",
    "id": 63
  },
  {
    "type": "tf",
    "question": "**Independence** is Two events are independent if the occurrence of one does not affect the probability of occurrence of the other.",
    "answer": true,
    "explanation": "True. Independence is indeed Two events are independent if the occurrence of one does not affect the probability of occurrence of the other.",
    "id": 24
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Probability Density Function (PDF)**?",
    "options": [
      "A continuous probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence.",
      "A variable whose values depend on outcomes of a random phenomenon.",
      "A function that gives the probability that a random variable X is less than or equal to x.",
      "A function that describes the relative likelihood for a continuous random variable to take on a given value."
    ],
    "answer": 3,
    "explanation": "**Probability Density Function (PDF)** is defined as: A function that describes the relative likelihood for a continuous random variable to take on a given value.",
    "id": 4
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A variable whose values depend on outcomes of a random phenomenon.*",
    "options": [
      "Random Variable",
      "Independence",
      "Sturges' Rule",
      "Standard Deviation"
    ],
    "answer": 0,
    "explanation": "The definition describes **Random Variable**.",
    "id": 14
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Standard Deviation**?",
    "options": [
      "Two events are independent if the occurrence of one does not affect the probability of occurrence of the other.",
      "A continuous probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence.",
      "A function that gives the probability that a random variable X is less than or equal to x.",
      "A measure of the amount of variation or dispersion of a set of values, calculated as the square root of variance."
    ],
    "answer": 3,
    "explanation": "**Standard Deviation** is defined as: A measure of the amount of variation or dispersion of a set of values, calculated as the square root of variance.",
    "id": 28
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Simple Random Sampling**?",
    "options": [
      "A theorem stating that the average of the results obtained from a large number of trials should be close to the expected value.",
      "A subset of a statistical population in which each member of the subset has an equal probability of being chosen.",
      "The theory that the distribution of sample means approximates a normal distribution as the sample size becomes larger.",
      "The probability distribution of a given random-sample-based statistic."
    ],
    "answer": 1,
    "explanation": "**Simple Random Sampling** is defined as: A subset of a statistical population in which each member of the subset has an equal probability of being chosen.",
    "id": 31
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor.*",
    "options": [
      "Multicollinearity",
      "Spurious Correlation",
      "Spearman Correlation",
      "Pearson Correlation"
    ],
    "answer": 1,
    "explanation": "The definition describes **Spurious Correlation**.",
    "id": 92
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A measure of how much a set of numbers is spread out from their average value.*",
    "options": [
      "Cumulative Distribution Function (CDF)",
      "Expected Value",
      "Variance",
      "Independence"
    ],
    "answer": 2,
    "explanation": "The definition describes **Variance**.",
    "id": 20
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand.*",
    "options": [
      "Sampling Bias",
      "Law of Large Numbers",
      "Convenience Sampling",
      "Central Limit Theorem"
    ],
    "answer": 2,
    "explanation": "The definition describes **Convenience Sampling**.",
    "id": 50
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*The probability of correctly rejecting the null hypothesis when it is false (1 - Beta).*",
    "options": [
      "Shapiro-Wilk Test",
      "Type II Error",
      "T-statistic",
      "Power"
    ],
    "answer": 3,
    "explanation": "The definition describes **Power**.",
    "id": 68
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Random Variable**?",
    "options": [
      "A variable whose values depend on outcomes of a random phenomenon.",
      "A method for determining the number of bins in a histogram, calculated as k = log2(n) + 1.",
      "A measure of how much a set of numbers is spread out from their average value.",
      "A continuous probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence."
    ],
    "answer": 0,
    "explanation": "**Random Variable** is defined as: A variable whose values depend on outcomes of a random phenomenon.",
    "id": 13
  },
  {
    "type": "tf",
    "question": "**Stratified Sampling** is A subset of a statistical population in which each member of the subset has an equal probability of being chosen.",
    "answer": false,
    "explanation": "False. That is the definition of **Simple Random Sampling**. **Stratified Sampling** is A method of sampling from a population which can be partitioned into subpopulations (strata).",
    "id": 48
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*The long-run average value of repetitions of the same experiment it represents.*",
    "options": [
      "Expected Value",
      "Variance",
      "Standard Deviation",
      "Random Variable"
    ],
    "answer": 0,
    "explanation": "The definition describes **Expected Value**.",
    "id": 17
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A function that describes the relative likelihood for a continuous random variable to take on a given value.*",
    "options": [
      "Random Variable",
      "Standard Deviation",
      "Variance",
      "The total area under a Probability Density Function curve is always 1."
    ],
    "answer": 3,
    "explanation": "The definition describes **Probability Density Function (PDF)**.",
    "id": 5
  },
  {
    "type": "tf",
    "question": "**Spearman Correlation** is A non-parametric measure of rank correlation (statistical dependence between the rankings of two variables).",
    "answer": true,
    "explanation": "True. Spearman Correlation is indeed A non-parametric measure of rank correlation (statistical dependence between the rankings of two variables).",
    "id": 87
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A measure of the amount of variation or dispersion of a set of values, calculated as the square root of variance.*",
    "options": [
      "Random Variable",
      "Cumulative Distribution Function (CDF)",
      "Standard Deviation",
      "Variance"
    ],
    "answer": 2,
    "explanation": "The definition describes **Standard Deviation**.",
    "id": 29
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A discrete probability distribution of the number of successes in a sequence of n independent experiments.*",
    "options": [
      "Normal Distribution",
      "Binomial Distribution",
      "Cumulative Distribution Function (CDF)",
      "Sturges' Rule"
    ],
    "answer": 1,
    "explanation": "The definition describes **Binomial Distribution**.",
    "id": 2
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Spearman Correlation**?",
    "options": [
      "A relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor.",
      "A measure of the linear correlation between two sets of data.",
      "A statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable.",
      "A non-parametric measure of rank correlation (statistical dependence between the rankings of two variables)."
    ],
    "answer": 3,
    "explanation": "**Spearman Correlation** is defined as: A non-parametric measure of rank correlation (statistical dependence between the rankings of two variables).",
    "id": 85
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Sampling Distribution**?",
    "options": [
      "A subset of a statistical population in which each member of the subset has an equal probability of being chosen.",
      "The theory that the distribution of sample means approximates a normal distribution as the sample size becomes larger.",
      "A bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.",
      "The probability distribution of a given random-sample-based statistic."
    ],
    "answer": 3,
    "explanation": "**Sampling Distribution** is defined as: The probability distribution of a given random-sample-based statistic.",
    "id": 34
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A method of sampling from a population which can be partitioned into subpopulations (strata).*",
    "options": [
      "Sampling Bias",
      "Stratified Sampling",
      "Sampling Distribution",
      "Standard Error"
    ],
    "answer": 1,
    "explanation": "The definition describes **Stratified Sampling**.",
    "id": 47
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Spurious Correlation**?",
    "options": [
      "A non-parametric measure of rank correlation (statistical dependence between the rankings of two variables).",
      "The assumption that the variance of the error term in a regression model is constant. (additional context) (addition",
      "A phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.",
      "A relationship in which two or more events or variables are associated but not causally related,"
    ],
    "answer": 3,
    "explanation": "**Spurious Correlation** is defined as: A relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor.",
    "id": 91
  },
  {
    "type": "tf",
    "question": "**Homoscedasticity** is A statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable.",
    "answer": false,
    "explanation": "False. That is the definition of **R-squared**. **Homoscedasticity** is The assumption that the variance of the error term in a regression model is constant.",
    "id": 99
  },
  {
    "type": "tf",
    "question": "**Pearson Correlation** is A measure of the linear correlation between two sets of data.",
    "answer": true,
    "explanation": "True. Pearson Correlation is indeed A measure of the linear correlation between two sets of data.",
    "id": 84
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*The standard deviation of the sampling distribution of a statistic.*",
    "options": [
      "Law of Large Numbers",
      "Simple Random Sampling",
      "Sampling Distribution",
      "Standard Error"
    ],
    "answer": 3,
    "explanation": "The definition describes **Standard Error**.",
    "id": 41
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*The rejection of a true null hypothesis (false positive).*",
    "options": [
      "Type I Error",
      "Shapiro-Wilk Test",
      "P-value",
      "Type II Error"
    ],
    "answer": 0,
    "explanation": "The definition describes **Type I Error**.",
    "id": 62
  },
  {
    "type": "tf",
    "question": "**R-squared** is A statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable.",
    "answer": true,
    "explanation": "True. R-squared is indeed A statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable.",
    "id": 90
  },
  {
    "type": "tf",
    "question": "**Random Variable** is A variable whose values depend on outcomes of a random phenomenon.",
    "answer": true,
    "explanation": "True. Random Variable is indeed A variable whose values depend on outcomes of a random phenomenon.",
    "id": 15
  },
  {
    "type": "tf",
    "question": "**Power** is A ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.",
    "answer": false,
    "explanation": "False. That is the definition of **T-statistic**. **Power** is The probability of correctly rejecting the null hypothesis when it is false (1 - Beta).",
    "id": 69
  },
  {
    "type": "tf",
    "question": "**Law of Large Numbers** is A method of sampling from a population which can be partitioned into subpopulations (strata).",
    "answer": false,
    "explanation": "False. That is the definition of **Stratified Sampling**. **Law of Large Numbers** is A theorem stating that the average of the results obtained from a large number of trials should be close to the expected value.",
    "id": 45
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Power**?",
    "options": [
      "A ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.",
      "The probability of rejecting the null hypothesis when it is true (alpha).",
      "The probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct.",
      "The probability of correctly rejecting the null hypothesis when it is false (1 - Beta)."
    ],
    "answer": 3,
    "explanation": "**Power** is defined as: The probability of correctly rejecting the null hypothesis when it is false (1 - Beta).",
    "id": 67
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Cumulative Distribution Function (CDF)**?",
    "options": [
      "A variable whose values depend on outcomes of a random phenomenon.",
      "Two events are independent if the occurrence of one does not affect the probability of occurrence of the other.",
      "A function that gives the probability that a random variable X is less than or equal to x.",
      "A function that describes the relative likelihood for a continuous random variable to take on a given value."
    ],
    "answer": 2,
    "explanation": "**Cumulative Distribution Function (CDF)** is defined as: A function that gives the probability that a random variable X is less than or equal to x.",
    "id": 7
  },
  {
    "type": "tf",
    "question": "**T-statistic** is A ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.",
    "answer": true,
    "explanation": "True. T-statistic is indeed A ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.",
    "id": 75
  },
  {
    "type": "tf",
    "question": "**Spurious Correlation** is The assumption that the variance of the error term in a regression model is constant.",
    "answer": false,
    "explanation": "False. That is the definition of **Homoscedasticity**. **Spurious Correlation** is A relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor.",
    "id": 93
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Stratified Sampling**?",
    "options": [
      "The standard deviation of the sampling distribution of a statistic.",
      "A method of sampling from a population which can be partitioned into subpopulations (strata).",
      "A bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.",
      "A type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand."
    ],
    "answer": 1,
    "explanation": "**Stratified Sampling** is defined as: A method of sampling from a population which can be partitioned into subpopulations (strata).",
    "id": 46
  },
  {
    "type": "tf",
    "question": "**Sampling Bias** is A method of sampling from a population which can be partitioned into subpopulations (strata).",
    "answer": false,
    "explanation": "False. That is the definition of **Stratified Sampling**. **Sampling Bias** is A bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.",
    "id": 54
  },
  {
    "type": "tf",
    "question": "**Simple Random Sampling** is A subset of a statistical population in which each member of the subset has an equal probability of being chosen.",
    "answer": true,
    "explanation": "True. Simple Random Sampling is indeed A subset of a statistical population in which each member of the subset has an equal probability of being chosen.",
    "id": 33
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Null Hypothesis**?",
    "options": [
      "The non-rejection of a false null hypothesis (false negative).",
      "The rejection of a true null hypothesis (false positive).",
      "The probability of correctly rejecting the null hypothesis when it is false (1 - Beta).",
      "A general statement or default position that there is no relationship between two measured phenomena."
    ],
    "answer": 3,
    "explanation": "**Null Hypothesis** is defined as: A general statement or default position that there is no relationship between two measured phenomena.",
    "id": 55
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Homoscedasticity**?",
    "options": [
      "The assumption that the variance of the error term in a regression model is constant.",
      "A measure of the linear correlation between two sets of data.",
      "A phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.",
      "A relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor."
    ],
    "answer": 0,
    "explanation": "**Homoscedasticity** is defined as: The assumption that the variance of the error term in a regression model is constant.",
    "id": 97
  },
  {
    "type": "tf",
    "question": "**Null Hypothesis** is The non-rejection of a false null hypothesis (false negative).",
    "answer": false,
    "explanation": "False. That is the definition of **Type II Error**. **Null Hypothesis** is A general statement or default position that there is no relationship between two measured phenomena.",
    "id": 57
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A continuous probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence.*",
    "options": [
      "Normal Distribution",
      "Probability Density Function (PDF)",
      "Variance",
      "Sturges' Rule"
    ],
    "answer": 0,
    "explanation": "The definition describes **Normal Distribution**.",
    "id": 11
  },
  {
    "type": "tf",
    "question": "**Levene's Test** is An inferential statistic used to assess the equality of variances for a variable calculated for two or more groups.",
    "answer": true,
    "explanation": "True. Levene's Test is indeed An inferential statistic used to assess the equality of variances for a variable calculated for two or more groups.",
    "id": 81
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.*",
    "options": [
      "T-statistic",
      "Power",
      "Null Hypothesis",
      "Levene's Test"
    ],
    "answer": 0,
    "explanation": "The definition describes **T-statistic**.",
    "id": 74
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*The probability of rejecting the null hypothesis when it is true (alpha).*",
    "options": [
      "Power",
      "Type II Error",
      "Significance Level",
      "Type I Error"
    ],
    "answer": 2,
    "explanation": "The definition describes **Significance Level**.",
    "id": 71
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*Two events are independent if the occurrence of one does not affect the probability of occurrence of the other.*",
    "options": [
      "Normal Distribution",
      "Cumulative Distribution Function (CDF)",
      "Independence",
      "Sturges' Rule"
    ],
    "answer": 2,
    "explanation": "The definition describes **Independence**.",
    "id": 23
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*The probability distribution of a given random-sample-based statistic.*",
    "options": [
      "Sampling Distribution",
      "Stratified Sampling",
      "Central Limit Theorem",
      "Sampling Bias"
    ],
    "answer": 0,
    "explanation": "The definition describes **Sampling Distribution**.",
    "id": 35
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A measure of the linear correlation between two sets of data.*",
    "options": [
      "Spurious Correlation",
      "R-squared",
      "Pearson Correlation",
      "Spearman Correlation"
    ],
    "answer": 2,
    "explanation": "The definition describes **Pearson Correlation**.",
    "id": 83
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A non-parametric measure of rank correlation (statistical dependence between the rankings of two variables).*",
    "options": [
      "Spearman Correlation",
      "Multicollinearity",
      "Homoscedasticity",
      "Spurious Correlation"
    ],
    "answer": 0,
    "explanation": "The definition describes **Spearman Correlation**.",
    "id": 86
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Convenience Sampling**?",
    "options": [
      "A subset of a statistical population in which each member of the subset has an equal probability of being chosen.",
      "The theory that the distribution of sample means approximates a normal distribution as the sample size becomes larger.",
      "A bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.",
      "A type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand."
    ],
    "answer": 3,
    "explanation": "**Convenience Sampling** is defined as: A type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand.",
    "id": 49
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.*",
    "options": [
      "Convenience Sampling",
      "Central Limit Theorem",
      "Sampling Bias",
      "Standard Error"
    ],
    "answer": 2,
    "explanation": "The definition describes **Sampling Bias**.",
    "id": 53
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A test of normality in frequentist statistics.*",
    "options": [
      "Shapiro-Wilk Test",
      "Type I Error",
      "P-value",
      "T-statistic"
    ],
    "answer": 0,
    "explanation": "The definition describes **Shapiro-Wilk Test**.",
    "id": 77
  },
  {
    "type": "mc",
    "question": "What is the best definition for **R-squared**?",
    "options": [
      "A statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable.",
      "A measure of the linear correlation between two sets of data. (additional context) (additional conte",
      "A phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.",
      "The assumption that the variance of the error term in a regression model is constant."
    ],
    "answer": 0,
    "explanation": "**R-squared** is defined as: A statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable.",
    "id": 88
  },
  {
    "type": "tf",
    "question": "**Cumulative Distribution Function (CDF)** is A function that gives the probability that a random variable X is less than or equal to x.",
    "answer": true,
    "explanation": "True. Cumulative Distribution Function (CDF) is indeed A function that gives the probability that a random variable X is less than or equal to x.",
    "id": 9
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*The probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct.*",
    "options": [
      "Type I Error",
      "Significance Level",
      "P-value",
      "Levene's Test"
    ],
    "answer": 2,
    "explanation": "The definition describes **P-value**.",
    "id": 59
  },
  {
    "type": "tf",
    "question": "**Convenience Sampling** is A type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand.",
    "answer": true,
    "explanation": "True. Convenience Sampling is indeed A type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand.",
    "id": 51
  },
  {
    "type": "mc",
    "question": "What is the best definition for **P-value**?",
    "options": [
      "A test of normality in frequentist statistics.",
      "The rejection of a true null hypothesis (false positive).",
      "The probability of rejecting the null hypothesis when it is true (alpha).",
      "The probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct."
    ],
    "answer": 3,
    "explanation": "**P-value** is defined as: The probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct.",
    "id": 58
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Law of Large Numbers**?",
    "options": [
      "A theorem stating that the average of the results obtained from a large number of trials should be close to the expected value.",
      "The probability distribution of a given random-sample-based statistic.",
      "The standard deviation of the sampling distribution of a statistic.",
      "A subset of a statistical population in which each member of the subset has an equal probability of being chosen."
    ],
    "answer": 0,
    "explanation": "**Law of Large Numbers** is defined as: A theorem stating that the average of the results obtained from a large number of trials should be close to the expected value.",
    "id": 43
  },
  {
    "type": "tf",
    "question": "**Normal Distribution** is A function that gives the probability that a random variable X is less than or equal to x.",
    "answer": false,
    "explanation": "False. That is the definition of **Cumulative Distribution Function (CDF)**. **Normal Distribution** is A continuous probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence.",
    "id": 12
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Sampling Bias**?",
    "options": [
      "The probability distribution of a given random-sample-based statistic. (additional context) (add",
      "A bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.",
      "A method of sampling from a population which can be partitioned into subpopulations (strata).",
      "A type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand."
    ],
    "answer": 1,
    "explanation": "**Sampling Bias** is defined as: A bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.",
    "id": 52
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Independence**?",
    "options": [
      "A discrete probability distribution of the number of successes in a sequence of n independent experiments.",
      "The long-run average value of repetitions of the same experiment it represents.",
      "A continuous probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence.",
      "Two events are independent if the occurrence of one does not affect the probability of occurrence of the other."
    ],
    "answer": 3,
    "explanation": "**Independence** is defined as: Two events are independent if the occurrence of one does not affect the probability of occurrence of the other.",
    "id": 22
  },
  {
    "type": "tf",
    "question": "**Sturges' Rule** is A function that describes the relative likelihood for a continuous random variable to take on a given value.",
    "answer": false,
    "explanation": "False. That is the definition of **Probability Density Function (PDF)**. **Sturges' Rule** is A method for determining the number of bins in a histogram, calculated as k = log2(n) + 1.",
    "id": 27
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Levene's Test**?",
    "options": [
      "A test of normality in frequentist statistics. (additional context) (additional context) (a",
      "The probability of correctly rejecting the null hypothesis when it is false (1 - Beta).",
      "The probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct.",
      "An inferential statistic used to assess the equality of variances for a variable calculated for two or"
    ],
    "answer": 3,
    "explanation": "**Levene's Test** is defined as: An inferential statistic used to assess the equality of variances for a variable calculated for two or more groups.",
    "id": 79
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Significance Level**?",
    "options": [
      "An inferential statistic used to assess the equality of variances for a variable calculated for two or more groups.",
      "The probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct.",
      "A ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.",
      "The probability of rejecting the null hypothesis when it is true (alpha)."
    ],
    "answer": 3,
    "explanation": "**Significance Level** is defined as: The probability of rejecting the null hypothesis when it is true (alpha).",
    "id": 70
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Variance**?",
    "options": [
      "A function that describes the relative likelihood for a continuous random variable to take on a given value.",
      "Two events are independent if the occurrence of one does not affect the probability of occurrence of the other.",
      "A discrete probability distribution of the number of successes in a sequence of n independent experiments.",
      "A measure of how much a set of numbers is spread out from their average value."
    ],
    "answer": 3,
    "explanation": "**Variance** is defined as: A measure of how much a set of numbers is spread out from their average value.",
    "id": 19
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Expected Value**?",
    "options": [
      "A function that gives the probability that a random variable X is less than or equal to x.",
      "A measure of how much a set of numbers is spread out from their average value.",
      "The long-run average value of repetitions of the same experiment it represents.",
      "A function that describes the relative likelihood for a continuous random variable to take on a given value."
    ],
    "answer": 2,
    "explanation": "**Expected Value** is defined as: The long-run average value of repetitions of the same experiment it represents.",
    "id": 16
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Sturges' Rule**?",
    "options": [
      "A method for determining the number of bins in a histogram, calculated as k = log2(n) + 1.",
      "A function that describes the relative likelihood for a continuous random variable to take on a given value.",
      "A function that gives the probability that a random variable X is less than or equal to x.",
      "A variable whose values depend on outcomes of a random phenomenon."
    ],
    "answer": 0,
    "explanation": "**Sturges' Rule** is defined as: A method for determining the number of bins in a histogram, calculated as k = log2(n) + 1.",
    "id": 25
  },
  {
    "type": "tf",
    "question": "**Probability Density Function (PDF)** is A function that describes the relative likelihood for a continuous random variable to take on a given value.",
    "answer": true,
    "explanation": "True. Probability Density Function (PDF) is indeed A function that describes the relative likelihood for a continuous random variable to take on a given value.",
    "id": 6
  },
  {
    "type": "tf",
    "question": "**Type II Error** is An inferential statistic used to assess the equality of variances for a variable calculated for two or more groups.",
    "answer": false,
    "explanation": "False. That is the definition of **Levene's Test**. **Type II Error** is The non-rejection of a false null hypothesis (false negative).",
    "id": 66
  },
  {
    "type": "tf",
    "question": "**Shapiro-Wilk Test** is A test of normality in frequentist statistics.",
    "answer": true,
    "explanation": "True. Shapiro-Wilk Test is indeed A test of normality in frequentist statistics.",
    "id": 78
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.*",
    "options": [
      "Pearson Correlation",
      "Homoscedasticity",
      "Multicollinearity",
      "R-squared"
    ],
    "answer": 2,
    "explanation": "The definition describes **Multicollinearity**.",
    "id": 95
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A function that gives the probability that a random variable X is less than or equal to x.*",
    "options": [
      "Binomial Distribution",
      "Probability Density Function (PDF)",
      "Cumulative Distribution Function (CDF)",
      "Normal Distribution"
    ],
    "answer": 2,
    "explanation": "The definition describes **Cumulative Distribution Function (CDF)**.",
    "id": 8
  },
  {
    "type": "tf",
    "question": "**Expected Value** is The long-run average value of repetitions of the same experiment it represents.",
    "answer": true,
    "explanation": "True. Expected Value is indeed The long-run average value of repetitions of the same experiment it represents.",
    "id": 18
  },
  {
    "type": "tf",
    "question": "**Significance Level** is The probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct.",
    "answer": false,
    "explanation": "False. That is the definition of **P-value**. **Significance Level** is The probability of rejecting the null hypothesis when it is true (alpha).",
    "id": 72
  },
  {
    "type": "tf",
    "question": "**P-value** is The rejection of a true null hypothesis (false positive).",
    "answer": false,
    "explanation": "False. That is the definition of **Type I Error**. **P-value** is The probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct.",
    "id": 60
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Binomial Distribution**?",
    "options": [
      "A variable whose values depend on outcomes of a random phenomenon.",
      "A measure of the amount of variation or dispersion of a set of values, calculated as the square root of variance.",
      "A discrete probability distribution of the number of successes in a sequence of n independent experiments.",
      "Two events are independent if the occurrence of one does not affect the probability of occurrence of the other."
    ],
    "answer": 2,
    "explanation": "**Binomial Distribution** is defined as: A discrete probability distribution of the number of successes in a sequence of n independent experiments.",
    "id": 1
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Standard Error**?",
    "options": [
      "The standard deviation of the sampling distribution of a statistic.",
      "A type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand.",
      "A method of sampling from a population which can be partitioned into subpopulations (strata).",
      "A bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others."
    ],
    "answer": 0,
    "explanation": "**Standard Error** is defined as: The standard deviation of the sampling distribution of a statistic.",
    "id": 40
  },
  {
    "type": "tf",
    "question": "**Variance** is A discrete probability distribution of the number of successes in a sequence of n independent experiments.",
    "answer": false,
    "explanation": "False. That is the definition of **Binomial Distribution**. **Variance** is A measure of how much a set of numbers is spread out from their average value.",
    "id": 21
  },
  {
    "type": "tf",
    "question": "**Central Limit Theorem** is The theory that the distribution of sample means approximates a normal distribution as the sample size becomes larger.",
    "answer": true,
    "explanation": "True. Central Limit Theorem is indeed The theory that the distribution of sample means approximates a normal distribution as the sample size becomes larger.",
    "id": 39
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Pearson Correlation**?",
    "options": [
      "The assumption that the variance of the error term in a regression model is constant.",
      "A statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable.",
      "A measure of the linear correlation between two sets of data.",
      "A relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor."
    ],
    "answer": 2,
    "explanation": "**Pearson Correlation** is defined as: A measure of the linear correlation between two sets of data.",
    "id": 82
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Type I Error**?",
    "options": [
      "The probability of correctly rejecting the null hypothesis when it is false (1 - Beta).",
      "The probability of rejecting the null hypothesis when it is true (alpha).",
      "The non-rejection of a false null hypothesis (false negative).",
      "The rejection of a true null hypothesis (false positive)."
    ],
    "answer": 3,
    "explanation": "**Type I Error** is defined as: The rejection of a true null hypothesis (false positive).",
    "id": 61
  },
  {
    "type": "mc",
    "question": "What is the best definition for **Type II Error**?",
    "options": [
      "The probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct.",
      "A test of normality in frequentist statistics.",
      "The non-rejection of a false null hypothesis (false negative).",
      "The probability of rejecting the null hypothesis when it is true (alpha)."
    ],
    "answer": 2,
    "explanation": "**Type II Error** is defined as: The non-rejection of a false null hypothesis (false negative).",
    "id": 64
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A subset of a statistical population in which each member of the subset has an equal probability of being chosen.*",
    "options": [
      "Convenience Sampling",
      "Central Limit Theorem",
      "Simple Random Sampling",
      "Stratified Sampling"
    ],
    "answer": 2,
    "explanation": "The definition describes **Simple Random Sampling**.",
    "id": 32
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*A method for determining the number of bins in a histogram, calculated as k = log2(n) + 1.*",
    "options": [
      "Binomial Distribution",
      "Probability Density Function (PDF)",
      "Sturges' Rule",
      "Variance"
    ],
    "answer": 2,
    "explanation": "The definition describes **Sturges' Rule**.",
    "id": 26
  },
  {
    "type": "tf",
    "question": "**Binomial Distribution** is A method for determining the number of bins in a histogram, calculated as k = log2(n) + 1.",
    "answer": false,
    "explanation": "False. That is the definition of **Sturges' Rule**. **Binomial Distribution** is A discrete probability distribution of the number of successes in a sequence of n independent experiments.",
    "id": 3
  },
  {
    "type": "mc",
    "question": "Which concept corresponds to the following definition?\n\n*The non-rejection of a false null hypothesis (false negative).*",
    "options": [
      "Type I Error",
      "Type II Error",
      "Shapiro-Wilk Test",
      "Null Hypothesis"
    ],
    "answer": 1,
    "explanation": "The definition describes **Type II Error**.",
    "id": 65
  },
  {
    "type": "mc",
    "question": "Which distribution describes the probability of a specific number of \"successes\" in a fixed number of independent binary trials?",
    "options": [
      "Normal Distribution",
      "T-Distribution",
      "Binomial Distribution",
      "Chi-Square Distribution"
    ],
    "answer": 2,
    "explanation": "**Binomial Distribution** is defined as: Describes the probability of k successes in n binary trials. Use `dbinom(k, n, p)` for exact probability and `pbinom(k, n, p)` for cumulative.",
    "id": 100,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "In R, which function prefix is used to generate random numbers from a specific distribution?",
    "options": [
      "`d` (e.g., `dnorm`)",
      "`p` (e.g., `pnorm`)",
      "`q` (e.g., `qnorm`)",
      "`r` (e.g., `rnorm`)"
    ],
    "answer": 3,
    "explanation": "`r` (random), `d` (density/probability), `p` (cumulative), `q` (quantile). e.g., `rnorm` generates random data.",
    "id": 101,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "The area under the curve of a probability density function must always sum to:",
    "options": [
      "0",
      "0.5",
      "1",
      "100"
    ],
    "answer": 2,
    "explanation": "**PDF Area**: The total area under a Probability Density Function curve is always 1.",
    "id": 102,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "What does the function `pnorm(1.96)` return in R?",
    "options": [
      "The probability density at Z = 1.96",
      "The cumulative probability (area to the left) of Z = 1.96",
      "A random number from a normal distribution",
      "The Z-score corresponding to the 1.96th percentile"
    ],
    "answer": 1,
    "explanation": "**pnorm(z)**: Returns the cumulative probability (area to the left) for a Z-score. `pnorm(1.96)` is approx 0.975.",
    "id": 103,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "Which of the following describes a Normal Distribution?",
    "options": [
      "It is discrete and bimodal.",
      "It is continuous, symmetric, and \"bell-shaped\" (follows the Empirical Rule).",
      "It is always skewed to the right.",
      "It describes binary outcomes only."
    ],
    "answer": 1,
    "explanation": "**Normal Distribution**: Symmetric, bell-shaped continuous distribution defined by mean and SD. approx 68% data within 1 SD, 95% within 2 SD.",
    "id": 104,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "If you want to calculate the probability of getting exactly 4 heads in 10 coin flips, which R function should you use?",
    "options": [
      "`rbinom(4, 10, 0.5)`",
      "`dbinom(4, 10, 0.5)`",
      "`pbinom(4, 10, 0.5)`",
      "`qbinom(4, 10, 0.5)`"
    ],
    "answer": 1,
    "explanation": "**cor() function**: Calculates correlation in R.",
    "id": 105,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "In a standard normal distribution (Z-distribution), what are the mean ($\\mu$) and standard deviation ($\\sigma$)?",
    "options": [
      "$\\mu = 1, \\sigma = 0$",
      "$\\mu = 100, \\sigma = 15$",
      "$\\mu = 0, \\sigma = 1$",
      "$\\mu = 0, \\sigma = 0$"
    ],
    "answer": 2,
    "explanation": "**Normal Distribution**: Symmetric, bell-shaped continuous distribution defined by mean and SD. approx 68% data within 1 SD, 95% within 2 SD.",
    "id": 106,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "Which distribution is typically used when testing hypotheses about categorical data or model comparison?",
    "options": [
      "T-distribution",
      "F-distribution",
      "Binomial distribution",
      "Chi-square distribution"
    ],
    "answer": 3,
    "explanation": "**F-distribution**: Used in ANOVA and regression to compare variances or models. Ratio of two chi-squares.",
    "id": 107,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "Skewness refers to:",
    "options": [
      "The \"peakedness\" or flatness of a distribution.",
      "The measure of asymmetry of a distribution.",
      "The spread of the data around the mean.",
      "The number of modes in a distribution."
    ],
    "answer": 1,
    "explanation": "**Skewness**: Measure of asymmetry. Positive skew = tail to right. Negative skew = tail to left.",
    "id": 108,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "What does the `set.seed(123)` function do in R?",
    "options": [
      "It randomizes the order of your dataframe.",
      "It ensures that random number generation is reproducible (same seed = same results).",
      "It installs the \"seed\" package.",
      "It calculates the standard error."
    ],
    "answer": 1,
    "explanation": "**set.seed()**: Initializes random number generator for reproducibility. Same seed = same random numbers.",
    "id": 109,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "The probability of an event occurring ranges between:",
    "options": [
      "-1 and 1",
      "0 and 1",
      "0 and 100",
      "$-\\infty$ and $+\\infty$"
    ],
    "answer": 1,
    "explanation": "**Probability Range**: Probabilities are always between 0 and 1.",
    "id": 110,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "If a distribution has \"heavy tails\" (high kurtosis), it means:",
    "options": [
      "There are more frequent extreme values (outliers) than in a normal distribution.",
      "The distribution is very flat.",
      "The distribution is skewed to the left.",
      "The mean is smaller than the median."
    ],
    "answer": 0,
    "explanation": "**Kurtosis**: Measure of tailedness. High kurtosis (leptokurtic) = heavy tails, potential outliers.",
    "id": 111,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "Which function would you use to find the Z-score that corresponds to the top 5% of the normal distribution?",
    "options": [
      "`dnorm(0.05)`",
      "`pnorm(0.95)`",
      "`qnorm(0.95)`",
      "`rnorm(0.95)`"
    ],
    "answer": 2,
    "explanation": "**Normal Distribution**: Symmetric, bell-shaped continuous distribution defined by mean and SD. approx 68% data within 1 SD, 95% within 2 SD.",
    "id": 112,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "In the context of probability, an \"elementary event\" refers to:",
    "options": [
      "An event that can be broken down into smaller parts.",
      "The entire sample space.",
      "A single outcome of an experiment (e.g.,",
      "The probability of zero."
    ],
    "answer": 2,
    "explanation": "**Elementary Event**: A single outcome of an experiment (e.g., rolling a 5).",
    "id": 113,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "The F-distribution is most commonly associated with which statistical test?",
    "options": [
      "T-test",
      "Correlation",
      "ANOVA (Analysis of Variance) and Regression.",
      "Binomial test"
    ],
    "answer": 2,
    "explanation": "**F-distribution**: Used in ANOVA and regression to compare variances or models. Ratio of two chi-squares.",
    "id": 114,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "If you flip a fair coin 100 times, the expected number of heads is 50. If you observe 48 heads, the difference is likely due to:",
    "options": [
      "Systemic bias",
      "Sampling error / Chance",
      "A Type I error",
      "Calculation error"
    ],
    "answer": 1,
    "explanation": "**Expected Value vs Observation**: Differences between observed data and expected value are due to sampling error (chance), assuming H0 is true.",
    "id": 115,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "Which of the following is a *continuous* probability distribution?",
    "options": [
      "Binomial distribution",
      "Normal distribution",
      "Poisson distribution",
      "Bernoulli distribution"
    ],
    "answer": 1,
    "explanation": "**Normal Distribution** is defined as: Symmetric, bell-shaped continuous distribution defined by mean and SD. approx 68% data within 1 SD, 95% within 2 SD.",
    "id": 116,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "If you increase the standard deviation of a normal distribution, the curve becomes:",
    "options": [
      "Taller and narrower",
      "Flatter and wider",
      "Skewed to the right",
      "Bimodal"
    ],
    "answer": 1,
    "explanation": "**Normal Distribution**: Symmetric, bell-shaped continuous distribution defined by mean and SD. approx 68% data within 1 SD, 95% within 2 SD.",
    "id": 117,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "What is the `size` argument in the `dbinom(x, size, prob)` function?",
    "options": [
      "The number of successes you are interested in.",
      "The probability of success.",
      "The total number of trials.",
      "The sample size of the population."
    ],
    "answer": 2,
    "explanation": "**dbinom size argument**: `size` refers to the number of trials (n), not the sample size of the study.",
    "id": 118,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "Why do we use probability theory in statistics?",
    "options": [
      "To prove that our data is correct.",
      "To calculate the exact population parameters.",
      "To estimate how likely our sample data is,",
      "To avoid doing math."
    ],
    "answer": 2,
    "explanation": "**Probability Range**: Probabilities are always between 0 and 1.",
    "id": 119,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "In a normal distribution, approximately what percentage of data falls within $\\pm 1$ standard deviation of the mean?",
    "options": [
      "50%",
      "68%",
      "95%",
      "99.7%"
    ],
    "answer": 1,
    "explanation": "**Normal Distribution**: Symmetric, bell-shaped continuous distribution defined by mean and SD. approx 68% data within 1 SD, 95% within 2 SD.",
    "id": 120,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "The \"Law of Large Numbers\" suggests that:",
    "options": [
      "Larger samples will always have a normal distribution.",
      "As the number of trials increases, the sample mean gets closer to the expected value.",
      "You need at least 100 participants for any study.",
      "Smaller samples are more accurate."
    ],
    "answer": 1,
    "explanation": "**Law of Large Numbers**: As sample size increases, sample mean converges to population mean.",
    "id": 121,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "Which R package contains the `z.test` function used in the lectures?",
    "options": [
      "`ggplot2`",
      "`stats`",
      "`BSDA`",
      "`psych`"
    ],
    "answer": 2,
    "explanation": "**z.test package**: `BSDA` package contains `z.test`.",
    "id": 122,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "If a distribution is perfectly symmetric, the mean, median, and mode are:",
    "options": [
      "All different",
      "All equal",
      "Equal to the standard deviation",
      "Equal to 1"
    ],
    "answer": 1,
    "explanation": "**F-distribution**: Used in ANOVA and regression to compare variances or models. Ratio of two chi-squares.",
    "id": 123,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "The sum of probabilities for all possible outcomes in a sample space is:",
    "options": [
      "0",
      "0.5",
      "1.0",
      "Infinite"
    ],
    "answer": 2,
    "explanation": "**Sum of Probabilities**: Sum of all possible outcomes in sample space is 1.",
    "id": 124,
    "module": "Module 1: Introduction & Probability"
  },
  {
    "type": "mc",
    "question": "You want to study \"all undergraduate students.\" You only have access to students at Tilburg University. The Tilburg students represent your:",
    "options": [
      "Target Population",
      "Sample",
      "Parameter",
      "Sampling Frame"
    ],
    "answer": 1,
    "explanation": "**Sample** is defined as: The subset of the population you actually collect data from.",
    "id": 125,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "Which sampling method gives every member of the population an equal chance of being selected?",
    "options": [
      "Convenience Sampling",
      "Simple Random Sampling",
      "Snowball Sampling",
      "Quota Sampling"
    ],
    "answer": 1,
    "explanation": "**Simple Random Sampling** is defined as: Every member of population has equal chance of selection.",
    "id": 126,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "A researcher stands outside the cafeteria and asks whoever walks by to answer a survey. This is an example of:",
    "options": [
      "Stratified Sampling",
      "Random Sampling",
      "Opportunity/Convenience Sampling",
      "Systematic Sampling"
    ],
    "answer": 2,
    "explanation": "**Mad Men Example**: Correlation implies causation fallacy or 'Just Drunk' = Spurious/Chance.",
    "id": 127,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "Snowball sampling is most useful when:",
    "options": [
      "You want to avoid all bias.",
      "The population is very large and easy to access.",
      "The population is hard to reach (e.g., rare diseases,",
      "You want to ensure equal gender representation."
    ],
    "answer": 2,
    "explanation": "**Snowball Sampling**: Participants recruit others. Useful for hard-to-reach populations.",
    "id": 128,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "Stratified sampling involves:",
    "options": [
      "Dividing the population into subgroups (strata) and",
      "Sampling whoever is available.",
      "Asking participants to recruit their friends.",
      "Selecting every $n$-th person from a list."
    ],
    "answer": 0,
    "explanation": "**Stratified Sampling**: Dividing population into strata (subgroups) and sampling from each to ensure representation.",
    "id": 129,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "WEIRD samples, a common source of bias in psychology/CSAI research, stands for:",
    "options": [
      "Western, Educated, Intelligent, Rich, Democratic",
      "Western, Educated, Industrialized, Rich, Democratic",
      "White, European, Industrialized, Rural, Dutch",
      "Western, European, Intellectual, Rich, Developed"
    ],
    "answer": 0,
    "explanation": "**Sample**: The subset of the population you actually collect data from.",
    "id": 130,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "A \"statistic\" describes a \\_\\_\\_\\_\\_\\_\\_, while a \"parameter\" describes a \\_\\_\\_\\_\\_\\_\\_.",
    "options": [
      "Population; Sample",
      "Sample; Population",
      "Mean; Standard Deviation",
      "Hypothesis; Result"
    ],
    "answer": 1,
    "explanation": "**Statistic vs Parameter**: Statistic describes Sample (Latin letters, M, s). Parameter describes Population (Greek letters, , ).",
    "id": 131,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "As the sample size ($N$) increases, the Standard Error of the Mean (SEM):",
    "options": [
      "Increases",
      "Decreases",
      "Stays the same",
      "Becomes equal to the Standard Deviation"
    ],
    "answer": 1,
    "explanation": "**Sample**: The subset of the population you actually collect data from.",
    "id": 132,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "The Central Limit Theorem states that as sample size increases, the sampling distribution of the mean will:",
    "options": [
      "Approach a normal distribution, regardless of the population distribution shape.",
      "Become more skewed.",
      "Have a larger standard deviation.",
      "Identically match the population distribution."
    ],
    "answer": 0,
    "explanation": "**Sample**: The subset of the population you actually collect data from.",
    "id": 133,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "Which symbol represents the *population* mean?",
    "options": [
      "$\\bar{x}$ (x-bar)",
      "$s$",
      "$\\mu$ (mu)",
      "$\\sigma$ (sigma)"
    ],
    "answer": 2,
    "explanation": "**Population Mean Symbol**:  (Mu).",
    "id": 134,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "Which symbol represents the *sample* standard deviation?",
    "options": [
      "$\\sigma$",
      "$\\mu$",
      "$s$",
      "$\\beta$"
    ],
    "answer": 2,
    "explanation": "**Sample**: The subset of the population you actually collect data from.",
    "id": 135,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "A 95% Confidence Interval (CI) of [45, 55] means:",
    "options": [
      "There is a 95% chance the sample mean is between 45 and 55.",
      "95% of the data points lie between 45 and 55.",
      "If we replicated the study many times, 95% of the calculated intervals would contain the true mean.",
      "The true mean is definitely 50."
    ],
    "answer": 2,
    "explanation": "**Confidence Interval (CI)**: Interval that captures the true parameter in 95% of repeated samples (for 95% CI). NOT '95% chance parameter is here'.",
    "id": 136,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "Sampling *without* replacement means:",
    "options": [
      "Once an individual is selected, they are put back into the pool and can be selected again.",
      "Once an individual is selected, they cannot be selected again.",
      "You replace the participant with a different person if they drop out.",
      "You use a different sampling method halfway through."
    ],
    "answer": 1,
    "explanation": "**Sampling Without Replacement**: Individuals cannot be selected twice.",
    "id": 137,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "Which of the following scenarios is most likely to produce a *biased* sample for a study on \"Global internet usage\"?",
    "options": [
      "Randomly selecting IP addresses from every country.",
      "Posting a survey link only on a VR gaming forum.",
      "Using a stratified sample based on continent population.",
      "Using a random digit dialer for phone numbers worldwide."
    ],
    "answer": 1,
    "explanation": "**Sample**: The subset of the population you actually collect data from.",
    "id": 138,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "If you replicate a study 20 times, how many of the 95% confidence intervals would you expect to *miss* (not include) the true population mean?",
    "options": [
      "0",
      "1",
      "5",
      "10"
    ],
    "answer": 1,
    "explanation": "**Population Mean Symbol**:  (Mu).",
    "id": 139,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "We use sample statistics to \\_\\_\\_\\_\\_\\_\\_ population parameters.",
    "options": [
      "Prove",
      "Define",
      "Estimate",
      "Correlate"
    ],
    "answer": 2,
    "explanation": "**Sample**: The subset of the population you actually collect data from.",
    "id": 140,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "Standard Error is calculated as:",
    "options": [
      "$SD / N$",
      "$SD / \\sqrt{N}$",
      "$Mean / SD$",
      "$SD^2$"
    ],
    "answer": 1,
    "explanation": "**Standard Error (SEM)**: SD of the sampling distribution of the mean. `SEM = SD / sqrt(N)`. Decreases as N increases.",
    "id": 141,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "What is the primary goal of inferential statistics in CSAI?",
    "options": [
      "To describe the sample data perfectly.",
      "To use sample data to make generalizations about a larger population.",
      "To calculate the mean of a population directly.",
      "To eliminate all sampling error."
    ],
    "answer": 1,
    "explanation": "**Goal of Inferential Stats**: Generalize from sample to population.",
    "id": 142,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "Which R function was used in the lecture to calculate confidence intervals for the mean?",
    "options": [
      "`meanCI()`",
      "`ciMean()` (from `lsr` package) or `t.test()`.",
      "`confint()`",
      "`get_CI()`"
    ],
    "answer": 1,
    "explanation": "**cor() function**: Calculates correlation in R.",
    "id": 143,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "If your sampling distribution of the mean is very wide (large standard error), it implies:",
    "options": [
      "Your estimate of the population mean is very precise.",
      "Your sample size is likely very large.",
      "Your estimate of the population mean is imprecise.",
      "You made a calculation error."
    ],
    "answer": 2,
    "explanation": "**F-distribution**: Used in ANOVA and regression to compare variances or models. Ratio of two chi-squares.",
    "id": 144,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "Bias in sampling occurs when:",
    "options": [
      "The sample size is too small.",
      "The sample does not accurately reflect the population (some members have lower/higher probability of selection).",
      "You use random sampling.",
      "You calculate the mean instead of the median."
    ],
    "answer": 1,
    "explanation": "**Sampling Bias**: Systematic error where sample does not represent population.",
    "id": 145,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "If you increase your sample size from 50 to 500, the confidence interval will likely:",
    "options": [
      "Become wider.",
      "Become narrower (more precise).",
      "Stay the same.",
      "Disappear."
    ],
    "answer": 1,
    "explanation": "**Sample**: The subset of the population you actually collect data from.",
    "id": 146,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "In the \"Taco Literacy\" example from the lecture, finding that 72% of 1000 people are taco illiterate vs 43% of 10,000 people suggests:",
    "options": [
      "The first sample was likely biased or",
      "Taco literacy decreased over time.",
      "The standard deviation increased.",
      "The population parameter changed."
    ],
    "answer": 0,
    "explanation": "**Taco Literacy Example**: Large discrepancies in percentages between samples suggest Sampling Variance or Bias.",
    "id": 147,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "Which of the following is NOT a type of non-probability sampling?",
    "options": [
      "Convenience Sampling",
      "Snowball Sampling",
      "Simple Random Sampling",
      "Volunteer Sampling"
    ],
    "answer": 2,
    "explanation": "**Non-probability Sampling**: Convenience, Snowball, Quota. (Random is Probability sampling).",
    "id": 148,
    "module": "Module 2: Sampling Theory"
  },
  {
    "type": "mc",
    "question": "The Null Hypothesis ($H_0$) usually states that:",
    "options": [
      "There is a strong relationship between variables.",
      "There is no effect, no difference, or the relationship is zero.",
      "The alternative hypothesis is false.",
      "The sample mean equals the sample median."
    ],
    "answer": 1,
    "explanation": "**Null Hypothesis (H0)**: Assumption of No Effect, No Difference, or r = 0.",
    "id": 149,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "A Type I error occurs when:",
    "options": [
      "You reject the null hypothesis when it is actually true (False Positive).",
      "You fail to reject the null hypothesis when it is actually false (False Negative).",
      "Your sample size is too small. (additional contex",
      "You use a T-test instead of a Z-test."
    ],
    "answer": 0,
    "explanation": "**Type I Error (Alpha)**: False Positive. Rejecting H0 when it's True. Risk = .",
    "id": 150,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "A Type II error occurs when:",
    "options": [
      "You reject the null hypothesis when it is true.",
      "You fail to reject the null hypothesis when it is actually false (False Negative).",
      "The p-value is less than 0.05.",
      "The effect size is large."
    ],
    "answer": 1,
    "explanation": "**Type I Error (Alpha)**: False Positive. Rejecting H0 when it's True. Risk = .",
    "id": 151,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "If your calculated p-value is 0.03 and your alpha level ($\\alpha$) is 0.05, you should:",
    "options": [
      "Fail to reject the null hypothesis.",
      "Reject the null hypothesis.",
      "Change your alpha level to 0.01.",
      "Conclude the null hypothesis is definitely true."
    ],
    "answer": 1,
    "explanation": "**Alpha Level**: Typically 0.05.",
    "id": 152,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "Which test statistic should be used when the population standard deviation ($\\sigma$) is *unknown*?",
    "options": [
      "Z-statistic",
      "T-statistic",
      "F-statistic",
      "Chi-square statistic"
    ],
    "answer": 1,
    "explanation": "**Test for Unknown Sigma**: T-test (uses sample s instead of population ).",
    "id": 153,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "Cohens d is a measure of:",
    "options": [
      "Statistical significance",
      "Effect size (standardized mean difference).",
      "Variance explained",
      "Sampling error"
    ],
    "answer": 1,
    "explanation": "**Cohen's d**: Effect size for means. Standardized difference. 0.2 small, 0.5 medium, 0.8 large.",
    "id": 154,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "A Cohens d of 0.8 is generally considered a:",
    "options": [
      "Small effect",
      "Medium effect",
      "Large effect",
      "Trivial effect"
    ],
    "answer": 2,
    "explanation": "**Cohen's d**: Effect size for means. Standardized difference. 0.2 small, 0.5 medium, 0.8 large.",
    "id": 155,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "To calculate a p-value, you do NOT need to know:",
    "options": [
      "The value of the test statistic",
      "The degrees of freedom (for t-tests)",
      "Whether the test is one-tailed or two-tailed",
      "The alpha level ($\\alpha$)"
    ],
    "answer": 3,
    "explanation": "**Expected Value vs Observation**: Differences between observed data and expected value are due to sampling error (chance), assuming H0 is true.",
    "id": 156,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "If you perform multiple statistical tests (e.g., 20 t-tests) on the same data, you increase the risk of:",
    "options": [
      "Type II error",
      "Type I error (Family-wise error rate).",
      "Standard error",
      "Sampling bias"
    ],
    "answer": 1,
    "explanation": "**Distributions for Categorical Data**: Chi-square is often used for categorical/count data analysis.",
    "id": 157,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "Which correction is commonly used to control for the Family-Wise Error Rate when making multiple comparisons?",
    "options": [
      "Pearson correction",
      "Bonferroni correction",
      "Spearman correction",
      "Gaussian correction"
    ],
    "answer": 1,
    "explanation": "**Multiple Comparisons**: Increases Family-wise Error Rate (risk of at least one Type I error).",
    "id": 158,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "Which of the following indicates the *strongest* evidence against the null hypothesis?",
    "options": [
      "p = 0.05",
      "p = 0.50",
      "p = 0.001",
      "p = 0.10"
    ],
    "answer": 2,
    "explanation": "**Null Hypothesis (H0)**: Assumption of No Effect, No Difference, or r = 0.",
    "id": 159,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "Statistical Power is defined as:",
    "options": [
      "The probability of rejecting a false null hypothesis (1 - Beta).",
      "The probability of making a Type I error.",
      "The size of the effect.",
      "The standard deviation of the sampling distribution."
    ],
    "answer": 0,
    "explanation": "**Statistical Power**: Probability of correctly rejecting false H0. Power = 1 - .",
    "id": 160,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "In a two-tailed test with $\\alpha = 0.05$, the critical region contains:",
    "options": [
      "The top 5% of the distribution.",
      "The bottom 5% of the distribution.",
      "The extreme 2.5% on both ends of the distribution.",
      "The middle 95% of the distribution."
    ],
    "answer": 2,
    "explanation": "**Critical Region**: The area in the tails where we reject H0. Split between tails for 2-tailed.",
    "id": 161,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "Why do T-distributions have \"heavier tails\" than the Z-distribution?",
    "options": [
      "Because they are skewed.",
      "To account for the extra uncertainty introduced by estimating the population standard deviation from the sample.",
      "Because they are used for large sample sizes.",
      "Because they are used for binary data."
    ],
    "answer": 1,
    "explanation": "**F-distribution**: Used in ANOVA and regression to compare variances or models. Ratio of two chi-squares.",
    "id": 162,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "Which R function performs a t-test?",
    "options": [
      "`z.test()`",
      "`t.test()`",
      "`cor.test()`",
      "`lm()`"
    ],
    "answer": 1,
    "explanation": "**t.test()**: Function for T-test in R.",
    "id": 163,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "If a 95% Confidence Interval for a mean difference includes 0 (e.g., [-2, 5]), what can you conclude?",
    "options": [
      "The difference is statistically significant at p < 0.05.",
      "The difference is NOT statistically significant (at alpha = 0.05).",
      "The effect size is large.",
      "You have made a Type I error."
    ],
    "answer": 1,
    "explanation": "**Confidence Interval (CI)**: Interval that captures the true parameter in 95% of repeated samples (for 95% CI). NOT '95% chance parameter is here'.",
    "id": 164,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "A hypothesis that predicts a specific direction (e.g., \"Group A will score *higher* than Group B\") requires a:",
    "options": [
      "Two-tailed test",
      "One-tailed test",
      "Null test",
      "Bonferroni correction"
    ],
    "answer": 1,
    "explanation": "**Null Hypothesis (H0)**: Assumption of No Effect, No Difference, or r = 0.",
    "id": 165,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "The \"Alternative Hypothesis\" ($H_1$) usually corresponds to:",
    "options": [
      "The status quo.",
      "The research prediction (e.g.,",
      "A calculation error.",
      "The alpha level."
    ],
    "answer": 1,
    "explanation": "**Alternative Hypothesis (H1)**: The research hypothesis (There is an effect).",
    "id": 166,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "Which package was used in the slides to calculate Cohen's d?",
    "options": [
      "`ggplot2`",
      "`psych` (specifically `cohen.d`) or `lsr`.",
      "`stats`",
      "`lsr`"
    ],
    "answer": 1,
    "explanation": "**Cohen's d**: Effect size for means. Standardized difference. 0.2 small, 0.5 medium, 0.8 large.",
    "id": 167,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "If you have a very large sample size, even very small effects can become:",
    "options": [
      "Statistically significant",
      "Practically significant",
      "Non-significant",
      "Biased"
    ],
    "answer": 0,
    "explanation": "**Sample**: The subset of the population you actually collect data from.",
    "id": 168,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "The threshold for statistical significance ($\\alpha$) is typically set to:",
    "options": [
      "0.01",
      "0.05",
      "0.10",
      "0.50"
    ],
    "answer": 1,
    "explanation": "**Type I Error (Alpha)**: False Positive. Rejecting H0 when it's True. Risk = .",
    "id": 169,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "The \"p-value\" represents:",
    "options": [
      "The probability that the Null Hypothesis is true.",
      "The probability of obtaining the observed data (or more extreme) assuming the Null Hypothesis is true.",
      "The probability that the Alternative Hypothesis is true.",
      "The probability of making a Type I error."
    ],
    "answer": 1,
    "explanation": "**Expected Value vs Observation**: Differences between observed data and expected value are due to sampling error (chance), assuming H0 is true.",
    "id": 170,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "In the \"Mad Men\" example from the slides, the conclusion \"Nope, just super drunk\" implies:",
    "options": [
      "The hypothesis was supported.",
      "The hypothesis was rejected (Null Hypothesis of no relationship was false), or it was a Spurious Correlation.",
      "The correlation was 1.0.",
      "A Type I error occurred."
    ],
    "answer": 1,
    "explanation": "**Mad Men Example**: Correlation implies causation fallacy or 'Just Drunk' = Spurious/Chance.",
    "id": 171,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "When reporting a t-test, you should include:",
    "options": [
      "The t-value",
      "The degrees of freedom",
      "The p-value",
      "All of the above"
    ],
    "answer": 3,
    "explanation": "**t.test()**: Function for T-test in R.",
    "id": 172,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "If you run a Z-test in R using `BSDA::z.test`, you must calculate or know \\_\\_\\_\\_\\_\\_ beforehand.",
    "options": [
      "The sample mean only.",
      "The population standard deviation (sigma).",
      "The p-value.",
      "The correlation coefficient."
    ],
    "answer": 1,
    "explanation": "**t.test()**: Function for T-test in R.",
    "id": 173,
    "module": "Module 3: Hypothesis Testing"
  },
  {
    "type": "mc",
    "question": "The Pearson correlation coefficient ($r$) ranges from:",
    "options": [
      "0 to 1",
      "-1 to 1",
      "-100 to 100",
      "$-\\infty$ to $+\\infty$"
    ],
    "answer": 1,
    "explanation": "**Pearson's r Range**: -1 to +1.",
    "id": 174,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "If $r = -0.90$, this indicates:",
    "options": [
      "A weak negative relationship.",
      "A strong positive relationship.",
      "A strong negative relationship.",
      "No relationship."
    ],
    "answer": 2,
    "explanation": "Answer derived from course materials (Module 1-4).",
    "id": 175,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Which of the following is TRUE about correlation?",
    "options": [
      "Correlation implies causation. (additional context",
      "A correlation of 0 means there is definitely no relationship (linear or non-linear).",
      "Correlation measures the strength and direction of a linear relationship between two variables.",
      "Pearson's $r$ is robust to outliers."
    ],
    "answer": 2,
    "explanation": "**Strong Correlation**: Closer to -1 or +1. 0.85 is Strong.",
    "id": 176,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Covariance differs from correlation because:",
    "options": [
      "Covariance is standardized.",
      "Covariance depends on the units of measurement, whereas correlation is standardized.",
      "Covariance is always between -1 and 1.",
      "Covariance is used for ordinal data."
    ],
    "answer": 1,
    "explanation": "**Covariance**: Unstandardized measure of joint variability. Hard to interpret magnitude.",
    "id": 177,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "To standardize covariance and turn it into a correlation coefficient, you divide it by:",
    "options": [
      "The sample size ($N$).",
      "The product of the standard deviations of the two variables.",
      "The mean of the variables.",
      "The variance of the variables."
    ],
    "answer": 1,
    "explanation": "**Covariance**: Unstandardized measure of joint variability. Hard to interpret magnitude.",
    "id": 178,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Calculating $r^2$ (the coefficient of determination) tells you:",
    "options": [
      "The direction of the relationship.",
      "The proportion of variance in one variable that is predictable from the other variable.",
      "The statistical significance of the relationship.",
      "The causal effect."
    ],
    "answer": 1,
    "explanation": "**Partial Coefficient**: Slope of X1 holding X2 constant.",
    "id": 179,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "If data are ordinal (ranked) or contain outliers, which correlation coefficient is most appropriate?",
    "options": [
      "Pearson's $r$",
      "Spearman's $\\rho$ (rho)",
      "Point-biserial correlation",
      "Partial correlation"
    ],
    "answer": 1,
    "explanation": "**Distributions for Categorical Data**: Chi-square is often used for categorical/count data analysis.",
    "id": 180,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "A Partial Correlation between X and Y, controlling for Z, removes the influence of Z from:",
    "options": [
      "Only X",
      "Only Y",
      "Both X and Y",
      "Neither"
    ],
    "answer": 2,
    "explanation": "**Partial Correlation**: Correlation between X and Y controlling for Z on BOTH.",
    "id": 181,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "A Semi-Partial Correlation between X and Y, controlling for Z, removes the influence of Z from:",
    "options": [
      "Only one of the variables (usually the predictor/IV) and the third variable.",
      "Both X and Y.",
      "The outcome variable only.",
      "The error term only."
    ],
    "answer": 0,
    "explanation": "**Partial Correlation**: Correlation between X and Y controlling for Z on BOTH.",
    "id": 182,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "In R, which function performs a correlation test and provides a p-value?",
    "options": [
      "`cor()`",
      "`cor.test()`",
      "`cov()`",
      "`mean()`"
    ],
    "answer": 1,
    "explanation": "**t.test()**: Function for T-test in R.",
    "id": 183,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Which assumption is NOT required for Pearsons correlation?",
    "options": [
      "Linearity",
      "Interval or Ratio data",
      "Normality (for significance testing)",
      "Homogeneity of regression slopes"
    ],
    "answer": 3,
    "explanation": "**Pearson's r Range**: -1 to +1.",
    "id": 184,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "\"Spurious correlations\" (like ice cream sales and shark attacks) usually occur due to:",
    "options": [
      "A third variable (confounder) that influences both observed variables.",
      "Small sample sizes.",
      "Type II errors.",
      "Non-linear relationships."
    ],
    "answer": 0,
    "explanation": "**Spurious Correlation**: Third variable causes relationship (e.g. Ice cream & Shark attacks).",
    "id": 185,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "If you run `cor.test(x, y, alternative = \"greater\")`, you are testing:",
    "options": [
      "A two-tailed hypothesis ($r \\neq 0$).",
      "A one-tailed hypothesis that the correlation is positive.",
      "A one-tailed hypothesis that the correlation is negative ($r < 0$).",
      "The null hypothesis that $r = 0$."
    ],
    "answer": 1,
    "explanation": "**t.test()**: Function for T-test in R.",
    "id": 186,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "What is the effect size of a correlation where $r = 0.10$?",
    "options": [
      "Large",
      "Medium",
      "Small",
      "Zero"
    ],
    "answer": 2,
    "explanation": "**r = 0**: No LINEAR relationship (could be U-shaped).",
    "id": 187,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Kendalls tau ($\\tau$) is generally preferred over Spearmans rho when:",
    "options": [
      "The sample size is small.",
      "The data is normally distributed.",
      "The sample size is very large.",
      "The data is ratio scale."
    ],
    "answer": 0,
    "explanation": "**Spearman's Rho**: Non-parametric rank correlation. Good for outliers or ordinal data.",
    "id": 188,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "In the lecture example about exam anxiety, exam performance, and revision time, a Partial correlation was used to:",
    "options": [
      "See the relationship between Exam and",
      "See if Anxiety causes poor performance.",
      "Calculate the mean of the exam scores.",
      "Rank the students."
    ],
    "answer": 0,
    "explanation": "**Partial Correlation**: Correlation between X and Y controlling for Z on BOTH.",
    "id": 189,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Which R package contains the `pcor.test` function for partial correlations?",
    "options": [
      "`stats`",
      "`ppcor`",
      "`psych`",
      "`lsr`"
    ],
    "answer": 1,
    "explanation": "**Partial Correlation**: Correlation between X and Y controlling for Z on BOTH.",
    "id": 190,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "If you see a correlation matrix where the diagonal values are all 1.0, this is because:",
    "options": [
      "Every variable is perfectly correlated with itself (r = 1).",
      "The calculation is wrong.",
      "The p-values are 1.0.",
      "There is multicollinearity."
    ],
    "answer": 0,
    "explanation": "**Correlation Matrix**: Table of correlations between all pairs.",
    "id": 191,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Which visual plot is best for checking the assumption of linearity before running a correlation?",
    "options": [
      "Histogram",
      "Boxplot",
      "Scatterplot",
      "Bar chart"
    ],
    "answer": 2,
    "explanation": "**Scatterplot** is defined as: Best plot to visualize correlation.",
    "id": 192,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "If the covariance between two variables is 4.25, what can you definitely say about the relationship?",
    "options": [
      "It is strong.",
      "It is weak.",
      "It is positive.",
      "It is statistically significant."
    ],
    "answer": 2,
    "explanation": "**Covariance**: Unstandardized measure of joint variability. Hard to interpret magnitude.",
    "id": 193,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "In the \"Guess the Correlation\" game mentioned in class, a dispersed cloud of points with no clear slope indicates:",
    "options": [
      "High positive correlation.",
      "High negative correlation.",
      "Correlation near 0.",
      "A calculation error."
    ],
    "answer": 2,
    "explanation": "**Slope (b1)**: Change in Y for 1 unit increase in X.",
    "id": 194,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Can a correlation coefficient ever be greater than 1?",
    "options": [
      "Yes, if the effect size is huge.",
      "No, it is mathematically impossible.",
      "Yes, if you use Spearman's rho.",
      "Yes, if you don't standardize."
    ],
    "answer": 1,
    "explanation": "**Strong Correlation**: Closer to -1 or +1. 0.85 is Strong.",
    "id": 195,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "If variable X increases and variable Y decreases, the correlation is:",
    "options": [
      "Positive",
      "Negative",
      "Zero",
      "Spurious"
    ],
    "answer": 1,
    "explanation": "**Strong Correlation**: Closer to -1 or +1. 0.85 is Strong.",
    "id": 196,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Which R function allows you to calculate a correlation matrix for multiple variables at once?",
    "options": [
      "`cor()`",
      "`t.test()`",
      "`mean()`",
      "`summary()`"
    ],
    "answer": 0,
    "explanation": "**Correlation Matrix**: Table of correlations between all pairs.",
    "id": 197,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "The `easystats` / `correlation` package mentioned in the slides is useful because:",
    "options": [
      "It does the homework for you.",
      "It provides a convenient way to visualize and report complex correlation matrices.",
      "It only calculates Pearson correlations.",
      "It does not require R."
    ],
    "answer": 0,
    "explanation": "**z.test package**: The `easystats` / `correlation` package provides a convenient way to visualize and report complex correlation matrices.",
    "id": 198,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "A researcher finds a covariance of 120 between variable X and Y. The standard deviation of X is 10 and the standard deviation of Y is 15. What is the Pearson correlation coefficient ($r$)?",
    "options": [
      "0.80",
      "1.20",
      "0.12",
      "0.85"
    ],
    "answer": 0,
    "explanation": "**Covariance**: Unstandardized measure of joint variability. Hard to interpret magnitude.\n\nAnswer Key: $r = \\frac{cov(X,Y)}{sd(X)sd(Y)} = \\frac{120}{10 \\times 15} = \\frac{120}{150} = 0.8$",
    "id": 199,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Which of the following correlation coefficients indicates the strongest relationship?",
    "options": [
      "$r = 0.65$",
      "$r = -0.72$",
      "$r = 0.10$",
      "$r = -0.05$"
    ],
    "answer": 1,
    "explanation": "**Strong Correlation**: Closer to -1 or +1. 0.85 is Strong.\n\nAnswer Key: Magnitude $|-0.72| = 0.72$ is the highest",
    "id": 200,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "You want to examine the relationship between `age` and `reaction_time` while controlling for the effect of `hours_slept` on both variables. Which type of correlation should you use?",
    "options": [
      "Semi-partial correlation",
      "Point-biserial correlation",
      "Partial correlation",
      "Spearman's rank correlation"
    ],
    "answer": 2,
    "explanation": "**Type I Error (Alpha)**: False Positive. Rejecting H0 when it's True. Risk = .",
    "id": 201,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "A Spearman correlation is most appropriate when:",
    "options": [
      "The relationship between variables is linear and data are normally distributed.",
      "The sample size is large (\\>100) and data are ratio scale.",
      "The data are ordinal (ranked) or the relationship is monotonic but",
      "You want to predict the exact value of Y from X."
    ],
    "answer": 2,
    "explanation": "**Spearman's Rho**: Non-parametric rank correlation. Good for outliers or ordinal data.",
    "id": 202,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Which of the following is TRUE about the coefficient of determination ($R^2$) in a simple correlation context?",
    "options": [
      "It represents the percent of variance in one variable shared with the other.",
      "It can be negative if the correlation is negative.",
      "It is the square root of the correlation coefficient.",
      "It tells you the direction of the relationship."
    ],
    "answer": 0,
    "explanation": "**Simple Random Sampling**: Every member of population has equal chance of selection.",
    "id": 203,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "If $r = 0.50$, how much variance in Variable Y is explained by Variable X?",
    "options": [
      "50%",
      "5%",
      "25%",
      "100%"
    ],
    "answer": 2,
    "explanation": "**r = 0**: No LINEAR relationship (could be U-shaped).\n\nAnswer Key: $0.50^2 = 0.25$",
    "id": 204,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Which R function would you use to calculate a correlation matrix for a data frame `df`?",
    "options": [
      "`summary(df)`",
      "`cor(df)`",
      "`lm(df)`",
      "`t.test(df)`"
    ],
    "answer": 1,
    "explanation": "**Correlation Matrix**: Table of correlations between all pairs.",
    "id": 205,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Interpreting Output:\n**Based on the output above, which conclusion is correct?**",
    "options": [
      "There is a significant positive relationship between stress and performance.",
      "There is a significant negative relationship; as stress increases,",
      "The relationship is not statistically significant ($p > .05$).",
      "Stress causes poor performance. (additional context) (ad"
    ],
    "answer": 1,
    "explanation": "Check the R output to identify the correct correlation coefficient and p-value.",
    "id": 206,
    "codeSnippet": "> cor.test(df$stress, df$performance)",
    "codeOutput": "Pearson's product-moment correlation\n\ndata:  df$stress and df$performance\nt = -4.52, df = 98, p-value = 0.000017\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n-0.584321 -0.239812\nsample estimates:\ncor\n-0.4156",
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "What is the null hypothesis for a standard Pearson correlation test?",
    "options": [
      "$r = 1$",
      "$r \\neq 0$",
      "$\\rho = 0$ (The population correlation coefficient is zero).",
      "$\\mu_1 = \\mu_2$"
    ],
    "answer": 2,
    "explanation": "**Null Hypothesis (H0)**: Assumption of No Effect, No Difference, or r = 0.",
    "id": 207,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Semi-partial correlation differs from partial correlation because:",
    "options": [
      "It controls for a third variable on *both* the predictor and criterion.",
      "It controls for a third variable on *only* the predictor (or only the outcome), not both.",
      "It is used only for non-parametric data.",
      "It yields a higher value than the zero-order correlation."
    ],
    "answer": 1,
    "explanation": "**Partial Correlation**: Correlation between X and Y controlling for Z on BOTH.",
    "id": 208,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Which assumption is NOT required for Pearson correlation?",
    "options": [
      "Linearity",
      "Normality of variables",
      "Homoscedasticity",
      "Multicollinearity"
    ],
    "answer": 3,
    "explanation": "**Pearson's r Range**: -1 to +1.",
    "id": 209,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "You have a small sample size ($N=8$) and your data contains outliers. Which correlation coefficient is robust to outliers?",
    "options": [
      "Pearson's $r$",
      "Spearman's $\\rho$ (rho)",
      "Point-biserial $r_{pb}$",
      "$R^2$"
    ],
    "answer": 1,
    "explanation": "**Sample**: The subset of the population you actually collect data from.",
    "id": 210,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "A correlation of $r = 0$ implies:",
    "options": [
      "No relationship exists between the variables.",
      "No *linear* relationship exists between the variables.",
      "The variables are independent.",
      "The covariance is 1."
    ],
    "answer": 1,
    "explanation": "**r = 0**: No LINEAR relationship (could be U-shaped).",
    "id": 211,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "If you run `cor.test()` and get a confidence interval of $[-0.10, 0.35]$, what can you conclude?",
    "options": [
      "The true correlation is likely 0.35.",
      "The correlation is statistically significant.",
      "The correlation is not statistically significant (since the interval includes 0).",
      "There is a strong negative relationship."
    ],
    "answer": 2,
    "explanation": "**cor.test()**: Tests significance of correlation.",
    "id": 212,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "Which library in R contains the `pcor()` function for partial correlations?",
    "options": [
      "`ggplot2`",
      "`ppcor`",
      "`stats`",
      "`dplyr`"
    ],
    "answer": 1,
    "explanation": "**Partial Correlation**: Correlation between X and Y controlling for Z on BOTH.",
    "id": 213,
    "module": "Module 4: Correlation"
  },
  {
    "type": "mc",
    "question": "In the simple linear regression equation $Y_i = b_0 + b_1X_i + \\epsilon_i$, what does $b_0$ represent?",
    "options": [
      "The slope of the line.",
      "The residual error.",
      "The Y-intercept (value of Y when X is 0).",
      "The standardized coefficient."
    ],
    "answer": 2,
    "explanation": "**Regression Equation**: Y = b0 + b1*X + e. Intercept + Slope.\n\nAnswer Key: The intercept b_0 represents the predicted value of Y when X is 0.",
    "id": 214,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "The method used to estimate regression coefficients by minimizing the sum of squared residuals is called:",
    "options": [
      "Maximum Likelihood Estimation.",
      "Ordinary Least Squares (OLS).",
      "Bayesian Estimation.",
      "Standard Deviation Minimization."
    ],
    "answer": 1,
    "explanation": "**Residual**: Observed Y - Predicted Y.",
    "id": 215,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "Interpreting Output:\n**Based on this output, what is the predicted exam score for a student with an anxiety score of 4?**",
    "options": [
      "82.5",
      "75.0",
      "95.0",
      "85.0"
    ],
    "answer": 1,
    "explanation": "Answer Key: $85 + (-2.5 \\times 4) = 75$",
    "id": 216,
    "codeSnippet": "Call:\nlm(formula = exam_score ~ anxiety, data = students)",
    "codeOutput": "Coefficients:\n(Intercept)      anxiety\n85.00        -2.50",
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "What is the residual ($e_i$) for a data point?",
    "options": [
      "The difference between the observed Y and the predicted Y ($Y_i - \\hat{Y}_i$).",
      "The difference between the observed X and the mean of X.",
      "The distance from the intercept to the data point.",
      "The variance of the model. (additional conte"
    ],
    "answer": 0,
    "explanation": "**Residual**: Observed Y - Predicted Y.",
    "id": 217,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "Which R code correctly runs a simple linear regression predicting `sales` from `adverts`?",
    "options": [
      "`lm(sales ~ adverts, data = df)`",
      "`lm(adverts ~ sales, data = df)`",
      "`cor(sales, adverts)`",
      "`plot(sales, adverts)`"
    ],
    "answer": 0,
    "explanation": "**Simple Random Sampling**: Every member of population has equal chance of selection.",
    "id": 218,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "In the output of `summary(model)`, the t-test associated with the slope ($b_1$) tests which null hypothesis?",
    "options": [
      "$b_1 = 1$",
      "$b_1 = 0$",
      "$b_0 = 0$",
      "The model explains 0% of the variance."
    ],
    "answer": 1,
    "explanation": "**summary(model)**: Shows coefficients, R2, F-test.",
    "id": 219,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "If the F-statistic in a simple regression is significant ($p < .05$), this means:",
    "options": [
      "The intercept is not zero.",
      "The model explains significantly more variance than an intercept-only model.",
      "The relationship is non-linear.",
      "Homoscedasticity is met."
    ],
    "answer": 1,
    "explanation": "**F-statistic**: Tests if whole model is better than null model.",
    "id": 220,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "Total Sum of Squares ($SS_T$) represents:",
    "options": [
      "The error in the regression model.",
      "The improvement of the regression model over the mean.",
      "The total variability in the outcome variable (before accounting for predictors).",
      "The sum of the squared residuals."
    ],
    "answer": 2,
    "explanation": "**R-squared**:  (Related to SS_T).",
    "id": 221,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "Which metric represents the average deviation of the residuals (in the units of the outcome variable)?",
    "options": [
      "$R^2$",
      "F-statistic",
      "Residual Standard Error (RSE) or Standard Error of the Estimate.",
      "Adjusted $R^2$"
    ],
    "answer": 2,
    "explanation": "**Residual**: Observed Y - Predicted Y.",
    "id": 222,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "If $SS_{Model} = 40$ and $SS_{Total} = 100$, what is $R^2$?",
    "options": [
      "0.60",
      "0.40",
      "2.5",
      "4000"
    ],
    "answer": 1,
    "explanation": "**summary(model)**: Shows coefficients, R2, F-test.\n\nAnswer Key: $40/100$",
    "id": 223,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "Interpreting Output:\n**What is the 95% Confidence Interval for the slope of `hours_study` (approximate using $t \\approx 2$)?**",
    "options": [
      "$[3.00, 7.00]$",
      "$[8.00, 12.00]$",
      "$[4.00, 6.00]$",
      "$[0.00, 10.00]$"
    ],
    "answer": 0,
    "explanation": "**Confidence Interval (CI)**: Interval that captures the true parameter in 95% of repeated samples (for 95% CI). NOT '95% chance parameter is here'.\n\nAnswer Key: $5 \\pm 2(1)$",
    "id": 224,
    "codeSnippet": "Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  10.000      2.000   5.000  0.001 ** hours_study   5.000      1.000   5.000  0.001 **",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "Standardized regression coefficients ($\\beta$) allow you to:",
    "options": [
      "Interpret the effect in raw units.",
      "Compare the strength of predictors measured on different scales.",
      "Ignore the intercept.",
      "Calculate the p-value manually."
    ],
    "answer": 1,
    "explanation": "**Standardized Regression**: Mean=0, SD=1 for all vars.",
    "id": 225,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "Which package in R is commonly used to obtain standardized coefficients (`std.beta`)?",
    "options": [
      "`ggplot2`",
      "`effectsize` or `QuantPsyc` (specifically `lm.beta`).",
      "`readr`",
      "`tibble`"
    ],
    "answer": 1,
    "explanation": "**z.test package**: `BSDA` package contains `z.test`.",
    "id": 226,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "In a plot of Fitted Values vs. Residuals, a \"fan\" shape indicates a violation of:",
    "options": [
      "Linearity",
      "Independence",
      "Homoscedasticity",
      "Normality"
    ],
    "answer": 2,
    "explanation": "**Residual**: Observed Y - Predicted Y.",
    "id": 227,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "Why is the intercept sometimes meaningless in interpreting data?",
    "options": [
      "Because it is always zero.",
      "Because a predictor value of 0 might be impossible or meaningless in the real world.",
      "Because it has a large standard error.",
      "Because it is not a standardized coefficient."
    ],
    "answer": 1,
    "explanation": "**Intercept (b0)**: Predicted Y when X = 0.",
    "id": 228,
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "Centering a predictor variable ($X_{centered} = X - \\bar{X}$) changes which parameter in the regression model?",
    "options": [
      "The slope ($b_1$)",
      "The residual standard error",
      "The intercept ($b_0$)",
      "The $R^2$"
    ],
    "answer": 2,
    "explanation": "**Centering**: Essential for raw polynomials to reduce multicollinearity.",
    "id": 229,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "After centering `age` to the mean, the intercept represents:",
    "options": [
      "The predicted Y when age is 0.",
      "The predicted Y for a person with the average age.",
      "The average age of the sample.",
      "The slope of age."
    ],
    "answer": 1,
    "explanation": "**Centering**: Essential for raw polynomials to reduce multicollinearity.",
    "id": 230,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Which diagnostic plot is best for checking the assumption of **Normality of Residuals**?",
    "options": [
      "Residuals vs. Fitted",
      "Scale-Location",
      "Normal Q-Q Plot",
      "Cooks Distance"
    ],
    "answer": 2,
    "explanation": "**Residual**: Observed Y - Predicted Y.",
    "id": 231,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Influential data points that have a disproportionate impact on the regression line are best detected using:",
    "options": [
      "The histogram of residuals.",
      "Cooks Distance or Leverage vs.",
      "The $R^2$ value.",
      "The F-statistic."
    ],
    "answer": 1,
    "explanation": "**Distributions for Categorical Data**: Chi-square is often used for categorical/count data analysis.",
    "id": 232,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "If the **Linearity** assumption is violated, the Residuals vs. Fitted plot will look like:",
    "options": [
      "A random cloud of points around 0.",
      "A distinct curve (e.g., U-shape).",
      "A funnel or fan shape.",
      "Points falling exactly on a straight line."
    ],
    "answer": 1,
    "explanation": "**Residual**: Observed Y - Predicted Y.",
    "id": 233,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "The `gvlma()` function from the `gvlma` package performs:",
    "options": [
      "A global validation of linear model assumptions.",
      "A generalized linear model analysis.",
      "A graphical visualization of linear models.",
      "A calculation of standardized betas."
    ],
    "answer": 0,
    "explanation": "**cor() function**: Calculates correlation in R.",
    "id": 234,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Interpreting Output:\n**What does this output suggest?**",
    "options": [
      "The model assumptions are acceptable.",
      "At least one regression assumption has been violated (e.g., Normality of residuals).",
      "The model is a perfect fit.",
      "You should proceed without changes."
    ],
    "answer": 1,
    "explanation": "Interpret the coefficients and significance levels from the summary output.",
    "id": 235,
    "codeSnippet": "> gvlma(model)",
    "codeOutput": "...\nGlobal Stat        Value   p-value  Decision\n12.5    0.012    Assumptions NOT satisfied!",
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Which assumption cannot be checked just by looking at a plot of the model residuals?",
    "options": [
      "Normality",
      "Linearity",
      "Independence of observations (needs study design context).",
      "Homoscedasticity"
    ],
    "answer": 2,
    "explanation": "**Residual**: Observed Y - Predicted Y.\n\nAnswer Key: Need to know study design",
    "id": 236,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Independence of errors is usually violated when:",
    "options": [
      "The sample size is small.",
      "Data are collected from the same subjects (repeated measures) or clustered groups.",
      "The variables are not normally distributed.",
      "There are outliers."
    ],
    "answer": 1,
    "explanation": "**Independence Violation**: Clustered/Time-series data violates this.",
    "id": 237,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Using `check_model()` from the `performance` package generates:",
    "options": [
      "A summary table of coefficients.",
      "A dashboard of visual plots checking regression assumptions (Normality, Homoscedasticity, etc.).",
      "A new regression model with better fit.",
      "The AIC score."
    ],
    "answer": 1,
    "explanation": "**z.test package**: `BSDA` package contains `z.test`.",
    "id": 238,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "`dfbeta` statistics are used to:",
    "options": [
      "Calculate degrees of freedom.",
      "Assess how much regression coefficients change if a specific case is removed (measure of influence).",
      "Standardize the betas.",
      "Test for heteroscedasticity."
    ],
    "answer": 1,
    "explanation": "**DFBETAS**: Change in ONE coefficient if case removed.",
    "id": 239,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Interpreting Output (Dummy Coding):\n**If `groupControl` is the reference group (0) and `groupTreatment` is 1, what is the mean of the Treatment group?**",
    "options": [
      "10.0",
      "40.0",
      "50.0",
      "60.0"
    ],
    "answer": 3,
    "explanation": "**Dummy Coding**: 0s and 1s. k-1 dummies for k levels.\n\nAnswer Key: $Intercept + Slope = 50 + 10 = 60$",
    "id": 240,
    "codeSnippet": "Coefficients:\n               Estimate\n(Intercept)    50.0\ngroupTreatment 10.0",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "In R, if you have a categorical variable `Status` with levels \"Low\", \"Med\", \"High\", how many dummy variables will `lm()` create automatically?",
    "options": [
      "1",
      "2 ($k-1$)",
      "3",
      "0"
    ],
    "answer": 1,
    "explanation": "**lm()**: Linear Model function in R.\n\nAnswer Key: 3 levels - 1 = 2 dummies",
    "id": 241,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "What does the `relevel()` function do?",
    "options": [
      "It removes outliers.",
      "It changes the reference category for a factor variable.",
      "It centers a continuous variable.",
      "It runs a regression."
    ],
    "answer": 1,
    "explanation": "**cor() function**: Calculates correlation in R.",
    "id": 242,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Homoscedasticity means:",
    "options": [
      "The residuals are normally distributed.",
      "The variance of the residuals is constant across all levels of the predictor(s).",
      "The predictor variables are uncorrelated.",
      "The relationship is linear."
    ],
    "answer": 1,
    "explanation": "**Homoscedasticity**: Constant variance of residuals across X.",
    "id": 243,
    "module": "Module 6: More Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "In the multiple regression equation $Y = b_0 + b_1X_1 + b_2X_2 + \\dots$, $b_1$ is interpreted as:",
    "options": [
      "The correlation between Y and $X_1$.",
      "The predicted change in Y for a 1-unit increase in $X_1$,",
      "The predicted change in Y for a 1-unit increase in $X_1$, ignoring other predictors.",
      "The standardized effect size. (additional context"
    ],
    "answer": 1,
    "explanation": "**Regression Equation**: Y = b0 + b1*X + e. Intercept + Slope.",
    "id": 244,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Why do we prefer Adjusted $R^2$ over Multiple $R^2$ in multiple regression?",
    "options": [
      "Adjusted $R^2$ is always higher.",
      "Multiple $R^2$ increases with every predictor added,",
      "Adjusted $R^2$ is easier to calculate.",
      "Multiple $R^2$ cannot be interpreted as variance explained."
    ],
    "answer": 1,
    "explanation": "**Multiple Regression**: Predicting Y from multiple Xs.",
    "id": 245,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Multicollinearity occurs when:",
    "options": [
      "The predictors are highly correlated with the outcome.",
      "The predictors are highly correlated with each other.",
      "The residuals are correlated with the fitted values.",
      "The sample size is too small."
    ],
    "answer": 1,
    "explanation": "**Multicollinearity**: High correlation between predictors. Bad.",
    "id": 246,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "A Variance Inflation Factor (VIF) value of 15 indicates:",
    "options": [
      "No collinearity.",
      "Moderate collinearity.",
      "Severe (high) multicollinearity.",
      "Perfect independence."
    ],
    "answer": 2,
    "explanation": "**VIF**: Variance Inflation Factor. > 10 is bad.",
    "id": 247,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Interpreting Output:\n**Which conclusion is supported?**",
    "options": [
      "Both predictors significantly predict the outcome.",
      "$X_1$ is a significant predictor, but $X_2$ is not.",
      "$X_2$ is a better predictor than $X_1$.",
      "The intercept is not significant."
    ],
    "answer": 1,
    "explanation": "Evaluate the model comparison metrics (AIC, BIC, or ANOVA results).",
    "id": 248,
    "codeSnippet": "Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   10.0      2.0      5.00   <.001\nX1             3.0      0.5      6.00   <.001\nX2             0.1      0.5      0.20    0.84",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Which R function compares two nested models to see if adding variables significantly improves fit?",
    "options": [
      "`summary()`",
      "`anova(model1, model2)`",
      "`cor.test()`",
      "`plot()`"
    ],
    "answer": 1,
    "explanation": "**Nested Models**: One model is a subset of the other (e.g., dropped a term).",
    "id": 249,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "When comparing non-nested models or checking model fit penalizing for complexity, which metric is best?",
    "options": [
      "$SS_{Total}$",
      "AIC (Akaike Information Criterion) or BIC.",
      "F-statistic.",
      "Pearson's r."
    ],
    "answer": 1,
    "explanation": "**Nested Models**: One model is a subset of the other (e.g., dropped a term).",
    "id": 250,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Interpreting Output (Model Comparison):\n**What does this ANOVA table tell us?**",
    "options": [
      "Model 1 is significantly better than Model 2.",
      "Adding `airplay` (Model 2) significantly reduced the residual sum of squares compared to Model 1.",
      "There is no significant difference between the models.",
      "Model 2 explains less variance."
    ],
    "answer": 1,
    "explanation": "**anova(m1, m2)**: Performs LRT in R for nested models.",
    "id": 251,
    "codeSnippet": "Model 1: sales ~ adverts\nModel 2: sales ~ adverts + airplay",
    "codeOutput": "     Res.Df    RSS Df Sum of Sq      F    Pr(>F)\n1       198 862264\n2       197 492362  1    369902  148.0 < 2.2e-16 ***",
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Stepwise regression (e.g., backward elimination) is criticized because:",
    "options": [
      "It relies on p-values/AIC rather than theory,",
      "It is computationally impossible in R.",
      "It always yields the worst $R^2$.",
      "It cannot handle categorical variables."
    ],
    "answer": 0,
    "explanation": "**Regression Equation**: Y = b0 + b1*X + e. Intercept + Slope.",
    "id": 252,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "Which command in R runs a multiple regression with all predictors in the dataframe?",
    "options": [
      "`lm(y ~ ., data = df)`",
      "`lm(y ~ x, data = df)`",
      "`lm(y ~ 1, data = df)`",
      "`lm(y ~ all, data = df)`"
    ],
    "answer": 0,
    "explanation": "**Multiple Regression**: Predicting Y from multiple Xs.",
    "id": 253,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "In a standard Spider Plot (radar chart) for model performance, usually:",
    "options": [
      "Points further out indicate better fit or higher values on that metric.",
      "It only shows $R^2$.",
      "It shows residuals.",
      "It maps the linear relationship."
    ],
    "answer": 0,
    "explanation": "**Q-Q Plot**: Checks Normality of residuals.",
    "id": 254,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "If you add a predictor to a model and the $R^2$ goes up but the AIC also goes up (gets worse), this suggests:",
    "options": [
      "The new predictor is extremely important.",
      "The new predictor does not justify the added complexity (penalty for extra parameters outweighs fit improvement).",
      "The calculation is wrong.",
      "The sample size is too large."
    ],
    "answer": 1,
    "explanation": "**AIC**: Akaike Information Criterion. Lower is better. Penalizes parameter count.",
    "id": 255,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "What is the degrees of freedom for the model ($df_{mod}$) in a multiple regression with 3 predictors ($K=3$)?",
    "options": [
      "$N - 1$",
      "3",
      "$N - 3 - 1$",
      "1"
    ],
    "answer": 1,
    "explanation": "**Degrees of Freedom**: df_residual = N - k - 1 (or N-2 for simple).",
    "id": 256,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "If two predictors are perfectly correlated ($r = 1.0$), the regression model:",
    "options": [
      "Will work perfectly.",
      "Will fail to estimate unique coefficients (Singularity / Perfect Multicollinearity).",
      "Will assign equal weights to both.",
      "Will have an $R^2$ of 0."
    ],
    "answer": 1,
    "explanation": "**Regression Equation**: Y = b0 + b1*X + e. Intercept + Slope.",
    "id": 257,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "The \"Hierarchical Entry\" method in regression means:",
    "options": [
      "The computer decides the order.",
      "You enter variables in blocks/steps based on theoretical order.",
      "You enter all variables at once.",
      "You only interpret the highest p-value."
    ],
    "answer": 1,
    "explanation": "**Hierarchical Regression**: Adding blocks of predictors.",
    "id": 258,
    "module": "Module 7: Multiple Regression & Assumptions"
  },
  {
    "type": "mc",
    "question": "An interaction effect occurs when:",
    "options": [
      "Two predictors are highly correlated.",
      "The effect of one predictor on the outcome depends on the level of another predictor.",
      "The effect of X on Y is mediated by M.",
      "The relationship between X and Y is non-linear."
    ],
    "answer": 1,
    "explanation": "**Interaction Effect**: Effect of X1 depends on X2.",
    "id": 259,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "In the equation $Y = b_0 + b_1X + b_2Z + b_3(X \\times Z)$, what does $b_3$ represent?",
    "options": [
      "The main effect of X.",
      "The main effect of Z.",
      "The interaction effect (change in the slope of X associated with a 1-unit increase in Z).",
      "The intercept."
    ],
    "answer": 2,
    "explanation": "**Regression Equation**: Y = b0 + b1*X + e. Intercept + Slope.",
    "id": 260,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "How do you specify an interaction between `age` and `education` in an R formula?",
    "options": [
      "`age + education`",
      "`age * education` (or `age + education + age:education`).",
      "`age / education`",
      "`age - education`"
    ],
    "answer": 1,
    "explanation": "**SEM Formula**: SEM =  / N.",
    "id": 261,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "Interpreting Output:\n**The interaction term is positive ($0.5$). This suggests that as social `support` increases:**",
    "options": [
      "The negative effect of `stress` on the outcome becomes *less negative* (weaker) as support increases.",
      "The negative effect of `stress` becomes *stronger* (more negative).",
      "There is no effect of stress.",
      "Support decreases the outcome."
    ],
    "answer": 0,
    "explanation": "**Interaction Term**: X1 * X2.\n\nAnswer Key: Slope of stress is -2. As support goes up, we add $0.5 \\times Support$ to the slope. $-2 + 0.5 = -1.5$, which is less negative.",
    "id": 262,
    "codeSnippet": "Coefficients:\n               Estimate Std. Error t value Pr(>|t|)\n(Intercept)        50.0      5.0     10.0   <.001\nstress             -2.0      0.5     -4.0   <.001\nsupport             3.0      0.5      6.0   <.001\nstress:support      0.5      0.1      5.0   <.001",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "When an interaction is significant, how should you interpret the \"main effects\" ($b_1, b_2$)?",
    "options": [
      "As global averages that apply to everyone.",
      "As conditional effects (simple slopes) when the other variable is 0.",
      "They should be ignored completely.",
      "They are now standardized."
    ],
    "answer": 1,
    "explanation": "**Significant t-test for Slope**: Predictor significantly predicts outcome.",
    "id": 263,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "What is a \"Simple Slopes Analysis\"?",
    "options": [
      "Running a regression with only one variable.",
      "Probing an interaction by examining the slope of one predictor at specific levels of the moderator.",
      "Plotting the residuals.",
      "Calculating the simple mean."
    ],
    "answer": 1,
    "explanation": "**Simple Slopes**: Slope of X1 at specific levels of X2.",
    "id": 264,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "In R, which package is commonly used to probe and plot interactions (e.g., `sim_slopes` or `interact_plot`)?",
    "options": [
      "`interactions` or `jtools`.",
      "`gvlma`",
      "`MASS`",
      "`base`"
    ],
    "answer": 0,
    "explanation": "**Q-Q Plot**: Checks Normality of residuals.",
    "id": 265,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "If the lines in an interaction plot are parallel, it indicates:",
    "options": [
      "A strong interaction.",
      "No interaction.",
      "A crossover interaction.",
      "Multicollinearity."
    ],
    "answer": 1,
    "explanation": "**Q-Q Plot**: Checks Normality of residuals.",
    "id": 266,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "Why is centering continuous predictors recommended when testing interactions?",
    "options": [
      "It turns the variables into categorical ones.",
      "It reduces multicollinearity between the main effects and the interaction term.",
      "It increases the sample size.",
      "It ensures normality."
    ],
    "answer": 1,
    "explanation": "**Centering**: Essential for raw polynomials to reduce multicollinearity.",
    "id": 267,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "Interpreting Output (Visual):\n*(Imagine a plot where the slope of X on Y is positive for the \"High Z\" group, flat for \"Medium Z\", and negative for \"Low Z\".)*\n**This is an example of:**",
    "options": [
      "A spurious correlation.",
      "A crossover (disordinal) or significant interaction.",
      "Homoscedasticity.",
      "A main effect only."
    ],
    "answer": 1,
    "explanation": "**Mad Men Example**: Correlation implies causation fallacy or 'Just Drunk' = Spurious/Chance.",
    "id": 268,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "To decompose an interaction involving a categorical variable (e.g., `Treatment` vs `Control`) and a continuous variable (`Dose`), you would look at:",
    "options": [
      "The slope of `Dose` for the Treatment group vs. the Control group.",
      "The mean of Dose.",
      "The correlation between Treatment and Control.",
      "The intercept only."
    ],
    "answer": 0,
    "explanation": "**Distributions for Categorical Data**: Chi-square is often used for categorical/count data analysis.",
    "id": 269,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "The term for breaking down an interaction into its lower-order components is:",
    "options": [
      "Decompose.",
      "Standardize.",
      "Bootstrap.",
      "Inflate."
    ],
    "answer": 0,
    "explanation": "**Interaction Term**: X1 * X2.",
    "id": 270,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "If you have a significant interaction, do you need to keep the main effects in the model?",
    "options": [
      "No, remove them to save degrees of freedom.",
      "Yes, for hierarchical, marginality,",
      "Only if they are significant.",
      "Only the intercept matters."
    ],
    "answer": 1,
    "explanation": "**summary(model)**: Shows coefficients, R2, F-test.",
    "id": 271,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "Using the `emmeans` package helps you to:",
    "options": [
      "Calculate Estimated Marginal Means and",
      "Calculate the mean of the residuals.",
      "Run a stepwise regression.",
      "Create dummy variables."
    ],
    "answer": 0,
    "explanation": "**emmeans**: Package for simple slopes/marginal means.",
    "id": 272,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "\"Johnson-Neyman\" intervals are used to:",
    "options": [
      "Find the specific range of the moderator where the effect of the focal predictor is significant.",
      "Detect outliers.",
      "Calculate VIF.",
      "Normalize the data."
    ],
    "answer": 0,
    "explanation": "**Johnson-Neyman**: Finds regions of significance for moderator.",
    "id": 273,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "Which R code creates a centered variable `age_c`?",
    "options": [
      "`df$age_c <- df$age - mean(df$age)`",
      "`df$age_c <- mean(df$age)`",
      "`df$age_c <- scale(df$age)` (Note: `scale` standardizes by default, not just centering).",
      "`df$age_c <- df$age / sd(df$age)`"
    ],
    "answer": 0,
    "explanation": "**Centering**: Helps reduce multicollinearity for interaction terms.",
    "id": 274,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "`confint(model)` in R provides:",
    "options": [
      "The confidence intervals for the predicted values.",
      "The confidence intervals for the regression coefficients.",
      "The correlation matrix.",
      "The R-squared value."
    ],
    "answer": 1,
    "explanation": "**summary(model)**: Shows coefficients, R2, F-test.",
    "id": 275,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "When reporting multiple regression results, which statistics are essential?",
    "options": [
      "$b$ (or $\\beta$), SE, t, p,",
      "Only the p-values.",
      "Only the $R^2$.",
      "The raw data frame."
    ],
    "answer": 0,
    "explanation": "**Multiple Regression**: Predicting Y from multiple Xs.",
    "id": 276,
    "module": "Module 8: Interactions"
  },
  {
    "type": "tf",
    "question": "True or False: Correlation implies Causation.",
    "options": [
      "True",
      "False"
    ],
    "answer": 1,
    "explanation": "**Correlation vs Causation**: Correlation does NOT imply causation.",
    "id": 277,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "Which plot is used to detect outliers based on leverage and residuals?",
    "options": [
      "Residuals vs Leverage (Cook's distance) plot.",
      "Histogram of outcome.",
      "Boxplot of predictors.",
      "Scatterplot of X vs Y."
    ],
    "answer": 0,
    "explanation": "**Residual**: Observed Y - Predicted Y.",
    "id": 278,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "If `lm()` output shows `NA` for a coefficient, it usually means:",
    "options": [
      "The variable had no effect.",
      "Perfect multicollinearity (singularity) or the variable is a constant.",
      "The variable is categorical.",
      "The sample size is too big."
    ],
    "answer": 1,
    "explanation": "**lm()**: Linear Model function in R.",
    "id": 279,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "Interpreting `summary()`:** The bottom line says `F-statistic: 50 on 2 and 97 DF, p-value: < 2.2e-16`.\n**What does the `2 and 97 DF` refer to?**",
    "options": [
      "2 predictors ($df_{mod}$) and 97 residual degrees of freedom.",
      "2 observations and 97 variables.",
      "2 models compared.",
      "An error in calculation."
    ],
    "answer": 0,
    "explanation": "**F-statistic**: Tests if whole model is better than null model.",
    "id": 280,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "How do you interpret a standardized intercept?",
    "options": [
      "The value of Y when all predictors are at their mean (0 for standardized).",
      "It is always 0.",
      "It represents the correlation.",
      "It is the mean of Y."
    ],
    "answer": 0,
    "explanation": "**Intercept (b0)**: Predicted Y when X = 0.",
    "id": 281,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "In `ggplot2`, adding a regression line to a scatterplot is done with:",
    "options": [
      "`geom_smooth(method = \"lm\")`",
      "`geom_line()`",
      "`geom_bar()`",
      "`geom_boxplot()`"
    ],
    "answer": 0,
    "explanation": "**Scatterplot**: Best plot to visualize correlation.",
    "id": 282,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "A researcher reports: \"$b = 0.5, t(100) = 1.5, p = .14$\". What is the conclusion?",
    "options": [
      "The effect is significant.",
      "The effect is not statistically significant (p > 0.05).",
      "The sample size is too small.",
      "There is a strong relationship."
    ],
    "answer": 1,
    "explanation": "**Reporting Results**: b, SE, t, p, R2.",
    "id": 283,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "If you run a regression and the residuals are not normally distributed, what might you consider?",
    "options": [
      "Bootstrapping or transforming the dependent variable (e.g., log transformation).",
      "Using a t-test instead.",
      "Ignoring it; regression is robust.",
      "Increasing the number of predictors."
    ],
    "answer": 0,
    "explanation": "**Residual**: Observed Y - Predicted Y.",
    "id": 284,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "Which of the following is a measure of effect size in regression?",
    "options": [
      "p-value",
      "$f^2$ (Cohen's $f^2$) or",
      "Degrees of freedom.",
      "The intercept."
    ],
    "answer": 1,
    "explanation": "**Effect Size r**: 0.1 small, 0.3 medium, 0.5 large.",
    "id": 285,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "Parsimony in model building means:",
    "options": [
      "Adding as many variables as possible.",
      "Choosing the simplest model that explains the data adequately.",
      "Maximizing $R^2$ at all costs.",
      "Using complex interactions."
    ],
    "answer": 1,
    "explanation": "**Parsimony**: Simpler model is better (if fit is similar).",
    "id": 286,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "If you calculate `AIC` for Model A = 200 and Model B = 195, which model is preferred?",
    "options": [
      "Model A",
      "Model B",
      "Neither",
      "They are equivalent."
    ],
    "answer": 1,
    "explanation": "**AIC**: Akaike Information Criterion. Lower is better. Penalizes parameter count.",
    "id": 287,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "Interpreting Output:\n**Is multicollinearity a problem here?**",
    "options": [
      "Yes, values are \\> 1.",
      "No, values are well below 5 or 10.",
      "Yes, because they are not equal.",
      "Cannot tell."
    ],
    "answer": 1,
    "explanation": "**Multicollinearity**: High correlation between predictors. Bad.",
    "id": 288,
    "codeSnippet": "> vif(model)",
    "codeOutput": "age    income\n1.02345   1.05432",
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "To visualize a 3D regression plane (2 predictors, 1 outcome) in R, which packages were mentioned/useful?",
    "options": [
      "`scatterplot3d` or `rgl`.",
      "`dplyr`",
      "`tidyr`",
      "`stringr`"
    ],
    "answer": 0,
    "explanation": "**Regression Equation**: Y = b0 + b1*X + e. Intercept + Slope.",
    "id": 289,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "If you want to predict an outcome Y based on Group (A, B, C), and \"A\" is the reference, the coefficient for \"B\" represents:",
    "options": [
      "The mean of group B.",
      "The difference between mean of B and",
      "The difference between B and C.",
      "The average of all groups."
    ],
    "answer": 1,
    "explanation": "**Reference Group**: The group coded 0 on all dummies.",
    "id": 290,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "What is the range of values for $R^2$?",
    "options": [
      "$-\\infty$ to $+\\infty$",
      "-1 to 1",
      "0 to 1",
      "0 to 100"
    ],
    "answer": 2,
    "explanation": "**Probability Range**: Probabilities are always between 0 and 1.",
    "id": 291,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "What is the correct syntax to extract coefficients from a saved model object `fit`?",
    "options": [
      "`fit$coefficients` or `coef(fit)`.",
      "`fit$residuals`",
      "`fit$r.squared`",
      "`fit$p.value`"
    ],
    "answer": 0,
    "explanation": "**summary(model)**: Shows coefficients, R2, F-test.",
    "id": 292,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "A \"suppressor variable\" in multiple regression is one that:",
    "options": [
      "Has no correlation with the outcome but increases the predictive power of another variable by removing irrelevant variance.",
      "Decreases the $R^2$.",
      "Is an outlier.",
      "Violates assumptions."
    ],
    "answer": 0,
    "explanation": "**Multiple Regression**: Predicting Y from multiple Xs.",
    "id": 293,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "If you have 5 predictors and $N=20$, you likely have:",
    "options": [
      "High statistical power.",
      "Overfitting / poor power (rule of thumb violation).",
      "A perfect model.",
      "Multicollinearity."
    ],
    "answer": 1,
    "explanation": "**Adding Predictors**: R-squared always increases (or stays same). Adjusted R-squared might drop.",
    "id": 294,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "In a report, you write: \"$F(2, 147) = 4.56, p = .012$\". What does the \"2\" represent?",
    "options": [
      "Sample size.",
      "Regression degrees of freedom (number of predictors).",
      "Residual degrees of freedom.",
      "The intercept."
    ],
    "answer": 1,
    "explanation": "The first number in F(df1, df2) represents the degrees of freedom for the model (predictors).",
    "id": 295,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "To compare the relative importance of predictors in a model, you should look at:",
    "options": [
      "Unstandardized $b$ coefficients.",
      "Standardized $\\beta$ coefficients.",
      "The p-values only.",
      "The standard errors."
    ],
    "answer": 1,
    "explanation": "**summary(model)**: Shows coefficients, R2, F-test.",
    "id": 296,
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "If you perform a `log` transformation on Y, the relationship being modeled changes from linear to:",
    "options": [
      "Quadratic.",
      "Exponential/Multiplicative.",
      "Logistic.",
      "Cubic.",
      "Price decreases sales, Quality increases sales, and high Quality *buffers* (reduces) the negative effect of Price on Sales (because the interaction is positive).",
      "Price decreases sales, but Quality has no effect.",
      "The interaction is not significant ($p > .05$).",
      "Price increases sales when Quality is 0."
    ],
    "answer": 1,
    "explanation": "**Linear Term Interpretation**: Slope at the mean of X (if orthogonal) or at X=0 (if raw).",
    "id": 297,
    "codeSnippet": "Call:\nlm(formula = sales ~ price * quality, data = products)",
    "codeOutput": "Coefficients:\nEstimate Std. Error t value Pr(>|t|)\n(Intercept)    50.00       5.00    10.00   <2e-16 ***\nprice          -2.00       0.50    -4.00   1e-04 ***\nquality         5.00       1.00     5.00   2e-05 ***\nprice:quality   0.20       0.10     2.00   0.048 *",
    "module": "Module 8: Interactions"
  },
  {
    "type": "mc",
    "question": "Question: You hypothesize that the relationship between \"Hours of Sleep\" and \"Cognitive Performance\" follows an inverted U-shape (performance increases with sleep up to a point, then decreases). Which polynomial term must be included in your regression model to test this specific shape?",
    "options": [
      "First-order (Linear) only",
      "Second-order (Quadratic)",
      "Third-order (Cubic)",
      "Fourth-order (Quartic)"
    ],
    "answer": 1,
    "explanation": "An inverted U-shape is modeled with a negative quadratic term.",
    "id": 298,
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question: A cubic polynomial function ($y = ax^3 + bx^2 + cx + d$) allows for how many points of inflection in the regression line?",
    "options": [
      "0",
      "1",
      "2",
      "3"
    ],
    "answer": 2,
    "explanation": "A cubic polynomial has degree 3, allowing for up to 2 inflection points.",
    "id": 299,
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question: When including a quadratic term (`I(x^2)`) in a linear model in R, what other term *must* generally be included for the model coefficients to be chemically/mathematically interpretable?",
    "options": [
      "The cubic term (`I(x^3)`)",
      "The lower-order linear term (`x`)",
      "An interaction term",
      "A random intercept"
    ],
    "answer": 1,
    "explanation": "When including a higher-order term like x^2, the lower-order term x must be included to maintain hierarchical structure.",
    "id": 300,
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nBased on the sign of the quadratic term (`I(chocolate^2)`), how would you describe the shape of the relationship?",
    "options": [
      "U-shaped (Convex; happiness decreases then increases)",
      "Inverted U-shaped (Concave; happiness rises then falls).",
      "Linear positive",
      "Linear negative"
    ],
    "answer": 1,
    "explanation": "Negative quadratic coefficient indicates a downward-opening curve).",
    "id": 301,
    "codeSnippet": "Call:\nlm(formula = happiness ~ chocolate + I(chocolate^2), data = diet_data)",
    "codeOutput": "Coefficients:\n(Intercept)      chocolate  I(chocolate^2)\n10.50           2.30           -0.15",
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question: In the output from the previous question, if the coefficient for `I(chocolate^2)` were **positive** (e.g., +0.15), what would the curve look like?",
    "options": [
      "It opens downward (Inverted U).",
      "It opens upward (U-shaped).",
      "An S-curve.",
      "A straight line."
    ],
    "answer": 1,
    "explanation": "A positive quadratic coefficient results in a U-shaped (convex) curve.",
    "id": 302,
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "tf",
    "question": "Question:** **[Mock R Output]\nTrue or False: The quadratic trend is statistically significant at the $\\alpha = 0.05$ level.",
    "options": [
      "True",
      "False"
    ],
    "answer": 0,
    "explanation": "p-value 0.015 \\< 0.05).",
    "id": 303,
    "codeSnippet": "Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)      50.00     5.00    10.00  < 2e-16 ***\nTime              5.00     1.00     5.00  1.2e-05 ***\nI(Time^2)         0.50     0.20     2.50   0.015 *",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question: What is a primary statistical issue often encountered when adding raw polynomial terms (e.g., $x$, $x^2$, $x^3$) to a regression model?",
    "options": [
      "Heteroscedasticity",
      "Multicollinearity (high correlation between x, x^2, x^3).",
      "Non-normality of residuals",
      "Autocorrelation"
    ],
    "answer": 1,
    "explanation": "Raw polynomials are highly correlated with each other, causing multicollinearity.",
    "id": 304,
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question: Which R function creates **orthogonal polynomials** to reduce multicollinearity between polynomial terms?",
    "options": [
      "`ortho()`",
      "`I()`",
      "`poly()`",
      "`scale()`"
    ],
    "answer": 2,
    "explanation": "The poly() function generates orthogonal polynomials by default.",
    "id": 305,
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "tf",
    "question": "Question: True or False: Using orthogonal polynomials (e.g., `poly(x, 2)`) changes the overall fit ($R^2$) of the model compared to using raw polynomials (`x + I(x^2)`).",
    "options": [
      "True",
      "False"
    ],
    "answer": 1,
    "explanation": "The overall fit remains the same; only the coefficients and their correlations change).",
    "id": 306,
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nIf you used raw polynomials instead of `poly(x, 2)`, which value in this output would definitively remain the exact same?",
    "options": [
      "The estimate for `poly(x, 2)1`",
      "The estimate for `poly(x, 2)2`",
      "The t-value for the intercept",
      "The $R^2$ of the model (not shown,"
    ],
    "answer": 3,
    "explanation": "Coefficients change, but the variance explained ($R^2$) and overall model fit do not).",
    "id": 307,
    "codeSnippet": "Call: lm(formula = y ~ poly(x, 2), data = df)\n...\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  20.00     0.50     40.00  <2e-16\npoly(x, 2)1   5.00     1.00      5.00  <2e-16\npoly(x, 2)2  -3.00     1.00     -3.00   0.004",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question: You run a linear model (`m1`) and a quadratic model (`m2`). You compare them using `anova(m1, m2)`.\n**[Mock R Output]**\nWhich model should you choose based on this output?",
    "options": [
      "Model 1 (Linear)",
      "Model 2 (Quadratic)",
      "Neither fits well",
      "Not enough information"
    ],
    "answer": 1,
    "explanation": "The reduction in RSS is significant, p \\< .001).",
    "id": 308,
    "codeSnippet": "Model 1: y ~ x\nModel 2: y ~ x + I(x^2)\nRes.Df    RSS Df Sum of Sq      F    Pr(>F)\n1     98 1500.0\n2     97 1200.0  1     300.0  24.25 3.5e-06 ***",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question: If the p-value in the ANOVA table comparing a linear and quadratic model was **0.35**, what would you conclude?",
    "options": [
      "The quadratic term significantly improves model fit.",
      "The quadratic term does not significantly improve model fit.",
      "The linear term is not significant.",
      "You should try a cubic model."
    ],
    "answer": 1,
    "explanation": "**anova(m1, m2)**: Performs LRT in R for nested models.",
    "id": 309,
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question: \"Empirically\" deciding to use polynomial regression usually involves inspecting which plot for curve-like patterns?",
    "options": [
      "Histogram of the outcome",
      "Residuals vs. Fitted values plot",
      "Boxplot of the predictor",
      "Correlation matrix"
    ],
    "answer": 1,
    "explanation": "Residuals vs. Fitted values plots reveal non-linear patterns if a polynomial term is needed.",
    "id": 310,
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question: Which is **NOT** a valid reason/use case for polynomial regression?",
    "options": [
      "Theoretical prediction of a curve (e.g., Yerkes-Dodson).",
      "Residual plots show a non-linear pattern.",
      "To extrapolate predictions far outside the observed range of data.",
      "To account for non-linear trends within the range of your data."
    ],
    "answer": 2,
    "explanation": "Polynomials are notoriously bad at extrapolating outside observed data).",
    "id": 311,
    "module": "Module 10: Multiple Regression with Polynomials"
  },
  {
    "type": "mc",
    "question": "Question: In a mixed model, what distinguishes a **fixed effect** from a **random effect**?",
    "options": [
      "Fixed effects vary by subject; random effects are constant.",
      "Fixed effects estimate population-level parameters; random effects estimate subject-specific deviations.",
      "Fixed effects are for categorical variables; random effects are for continuous variables.",
      "Fixed effects are optional; random effects are mandatory."
    ],
    "answer": 1,
    "explanation": "Fixed effects estimate population means; random effects estimate subject-specific deviations.",
    "id": 312,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: Which of the following is a classic example of a variable often treated as a **random effect** in a Repeated Measures design?",
    "options": [
      "Treatment condition (Control vs. Experimental)",
      "Participant/Subject ID",
      "Age of participant",
      "Gender"
    ],
    "answer": 1,
    "explanation": "**Mad Men Example**: Correlation implies causation fallacy or 'Just Drunk' = Spurious/Chance.",
    "id": 313,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: Why do we use Mixed Models instead of standard Multiple Regression for nested data?",
    "options": [
      "To violate the assumption of linearity.",
      "To account for non-independence of observations (clustering).",
      "To calculate easier p-values.",
      "Because standard regression cannot handle continuous predictors."
    ],
    "answer": 1,
    "explanation": "Mixed models account for the correlation of residuals within groups/subjects.",
    "id": 314,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: Which R package is primarily used for running linear mixed-effects models (`lmer`)?",
    "options": [
      "`ggplot2`",
      "`dplyr`",
      "`lme4`",
      "`stats`"
    ],
    "answer": 2,
    "explanation": "lme4 is the standard package for linear mixed-effects models in R.",
    "id": 315,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: What is the correct syntax for a random intercept for `Subject` in an `lmer` formula?",
    "options": [
      "`(Subject | 1)`",
      "`(1 | Subject)`",
      "`random = Subject`",
      "`(1 + Subject)`"
    ],
    "answer": 1,
    "explanation": "The pipe | separates the random term from the grouping variable.",
    "id": 316,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: How would you specify a model with a fixed effect for `Condition`, a random intercept for `Subject`, AND a random slope for `Condition` by `Subject`?",
    "options": [
      "`y ~ Condition + (1 | Subject)`",
      "`y ~ Condition + (Condition | Subject)`",
      "`y ~ Condition + (1 | Subject) + (1 | Condition)`",
      "`y ~ Condition + (0 | Subject)`"
    ],
    "answer": 1,
    "explanation": "Note: `(Condition | Subject)` implicitly includes the intercept `1 + Condition`).",
    "id": 317,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: What does the term `(1 | School/Class)` imply in a mixed model formula?",
    "options": [
      "Random intercepts for School only.",
      "Random intercepts for Class only.",
      "Nested random effects: Classes nested within Schools.",
      "Crossed random effects between School and Class."
    ],
    "answer": 2,
    "explanation": "The slash / denotes nesting: Classes within Schools.",
    "id": 318,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nCalculate the Intraclass Correlation Coefficient (ICC).",
    "options": [
      "0.20",
      "0.80",
      "4.00",
      "0.25"
    ],
    "answer": 1,
    "explanation": "Calculation: $\\frac{400}{400 + 100} = 0.80$).",
    "id": 319,
    "codeSnippet": "Random effects:\nGroups   Name        Variance Std.Dev.\nSubject  (Intercept) 400.0    20.0\nResidual             100.0    10.0\nNumber of obs: 200, groups:  Subject, 20",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: What does an ICC of 0.80 (from the previous question) indicate?",
    "options": [
      "80% of the variance in the outcome is due to differences between groups (clusters).",
      "80% of the variance is due to measurement error (residual).",
      "The subjects are not correlated.",
      "You should definitely use a simple linear regression."
    ],
    "answer": 0,
    "explanation": "ICC is the proportion of total variance explained by the grouping structure (between-group variance).",
    "id": 320,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nWhat is the predicted value of the outcome for `Day = 0` (the intercept) for the *average* subject (ignoring random deviations)?",
    "options": [
      "10.00",
      "250.00",
      "260.00",
      "25.00"
    ],
    "answer": 1,
    "explanation": "The Intercept (250.00) represents the value when Day=0.",
    "id": 321,
    "codeSnippet": "Fixed effects:\n            Estimate Std. Error t value\n(Intercept)  250.00    10.00     25.00\nDay           10.00     2.00      5.00",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: In the output above (\\#24), what is the estimated change in the outcome for every 1-unit increase in Day?",
    "options": [
      "250.00",
      "25.00",
      "10.00",
      "2.00"
    ],
    "answer": 2,
    "explanation": "The slope for Day (10.00) represents the change per unit of time.",
    "id": 322,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nWhat does the correlation of -0.50 tell you?",
    "options": [
      "Subjects with higher baseline intercepts tend to have steeper positive slopes.",
      "Subjects with higher baseline intercepts tend to have lower/more negative slopes.",
      "The model did not converge. (additional context) (",
      "Day is negatively correlated with the outcome."
    ],
    "answer": 1,
    "explanation": "Negative correlation between Intercept and Slope).",
    "id": 323,
    "codeSnippet": "Random effects:\nGroups   Name        Variance Std.Dev. Corr\nSubject  (Intercept) 500.0    22.36\nDay          50.0     7.07    -0.50",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: Ideally, if you add a fixed predictor to a mixed model that explains a lot of variance in the outcome, what should happen to the **Random Intercept Variance** (compared to the empty/null model)?",
    "options": [
      "It should increase.",
      "It should stay exactly the same.",
      "It should decrease (variance is \"explained\" by the fixed effect).",
      "It should become negative."
    ],
    "answer": 2,
    "explanation": "Adding a significant predictor explains variance, reducing the unexplained random variance.",
    "id": 324,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: Which estimation method is generally preferred when comparing nested mixed models with **different fixed effects** using a Likelihood Ratio Test?",
    "options": [
      "REML (Restricted Maximum Likelihood)",
      "ML (Maximum Likelihood)",
      "OLS (Ordinary Least Squares)",
      "T-test"
    ],
    "answer": 1,
    "explanation": "REML is better for final parameter estimates, ML for comparing fixed effects).",
    "id": 325,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: If you see the warning \"Singular fit\" in R output for an `lmer` model, what does it usually mean?",
    "options": [
      "The model fits the data perfectly (R^2 = 1).",
      "The random effects structure is too complex for the data (variance near 0).",
      "You have too many outliers.",
      "You should add more random slopes."
    ],
    "answer": 1,
    "explanation": "Singular fit means the model is overparameterized, often with random variance near zero.",
    "id": 326,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: What is $R^2_{marginal}$ in the context of mixed models?",
    "options": [
      "Variance explained by the entire model (fixed + random).",
      "Variance explained by the fixed effects only.",
      "Variance explained by the random effects only.",
      "The error variance."
    ],
    "answer": 1,
    "explanation": "Marginal R-squared includes variance explained by fixed effects only.",
    "id": 327,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: What is $R^2_{conditional}$?",
    "options": [
      "Variance explained by the fixed effects only.",
      "Variance explained by the entire model (fixed + random effects).",
      "The p-value of the model.",
      "The correlation between random slopes and intercepts."
    ],
    "answer": 1,
    "explanation": "Conditional R-squared includes variance explained by both fixed and random effects.",
    "id": 328,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output - Model Comparison]\nWhich model is preferred based on this output?",
    "options": [
      "mod1 (fewer degrees of freedom).",
      "mod1 (higher AIC).",
      "mod2 (lower AIC and significant Likelihood Ratio Test).",
      "Neither, they are statistically equivalent."
    ],
    "answer": 2,
    "explanation": "Mod2 has a lower AIC and a significant LRT result, indicating better fit.",
    "id": 329,
    "codeSnippet": "Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(>Chisq)\nmod1   4 1200.0 1220.0 -596.0   1192.0\nmod2   5 1150.0 1175.0 -570.0   1140.0  52.0      1  5e-13 ***",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: Why do we typically use the `lmerTest` package in addition to `lme4`?",
    "options": [
      "To make better plots.",
      "To get p-values for the fixed effects (using Satterthwaite approximation).",
      "To run Bayesian models.",
      "To handle missing data."
    ],
    "answer": 1,
    "explanation": "lmerTest extends lme4 to provide p-values for t-tests.",
    "id": 330,
    "module": "Module 11: Mixed Models"
  },
  {
    "type": "mc",
    "question": "Question: A Growth Curve Model (GCM) is essentially a special case of which type of model?",
    "options": [
      "ANOVA",
      "Mixed-Effects Model (Multilevel Model).",
      "Chi-Square Test",
      "Logistic Regression"
    ],
    "answer": 1,
    "explanation": "Growth Curve Models are Multilevel Models applied to longitudinal data.",
    "id": 331,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question: What is the defining feature of the data required for Growth Curve Modeling?",
    "options": [
      "It must be categorical.",
      "It must be cross-sectional (one time point).",
      "It must be longitudinal (repeated measures over time).",
      "It must have no missing values."
    ],
    "answer": 2,
    "explanation": "Growth models require repeated measures (longitudinal data).",
    "id": 332,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question: In an **Unconditional Growth Model**, what is typically the only fixed predictor included?",
    "options": [
      "Treatment Group",
      "Time",
      "Gender",
      "Nothing (only the intercept)"
    ],
    "answer": 1,
    "explanation": "Unconditional growth models include Time to model the trajectory but no other predictors.",
    "id": 333,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question: What does a \"Conditional\" Growth Model include that an Unconditional one does not?",
    "options": [
      "Random intercepts.",
      "A time variable.",
      "Covariates or predictors of change (time-invariant or time-varying).",
      "Error terms."
    ],
    "answer": 2,
    "explanation": "Conditional models includes predictors (covariates) to explain variability in intercepts or slopes.",
    "id": 334,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question: If you want to model a non-linear trajectory over time in a GCM, what do you usually add?",
    "options": [
      "More subjects.",
      "Polynomial terms for Time (e.g., Time^2).",
      "More random effects.",
      "A control group."
    ],
    "answer": 1,
    "explanation": "Polynomial terms like Time^2 allow for modeling non-linear curves.",
    "id": 335,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nThe reference group is Group A. What is the slope (rate of growth) for **Group A**?",
    "options": [
      "100.00",
      "5.00",
      "3.00",
      "-2.00"
    ],
    "answer": 1,
    "explanation": "The main effect of Time represents the slope for the reference group).",
    "id": 336,
    "codeSnippet": "Fixed effects:\n            Estimate Std. Error t value\n(Intercept)  100.00     2.00     50.00\nTime           5.00     0.50     10.00\nGroupB        10.00     3.00      3.33\nTime:GroupB   -2.00     0.70     -2.85",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question: Using the output from Question \\#39, what is the slope (rate of growth) for **Group B**?",
    "options": [
      "5.00",
      "10.00",
      "3.00 (5.00 - 2.00)",
      "7.00 (10.00 - 3.00)"
    ],
    "answer": 2,
    "explanation": "Slope A + Interaction term = 5.00 + (-2.00) = 3.00).",
    "id": 337,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question: Using the output from Question \\#39, how do the intercepts differ between Group A and Group B?",
    "options": [
      "They don't differ.",
      "Group B starts 10.00 units higher than Group A.",
      "Group B starts 2.00 units lower.",
      "Group B starts 5.00 units higher."
    ],
    "answer": 1,
    "explanation": "The main effect of GroupB represents the difference at Time=0).",
    "id": 338,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nHow would you describe the average growth trajectory based on these fixed effects?",
    "options": [
      "Linear increase forever.",
      "Increases initially, but the rate of growth slows down (decelerates).",
      "Decreases initially, then increases (U-shape).",
      "Flat line."
    ],
    "answer": 1,
    "explanation": "Positive linear term, negative quadratic term).",
    "id": 339,
    "codeSnippet": "Formula: score ~ Time + I(Time^2) + (1 + Time | Subject)\nFixed effects:\n(Intercept)   50.0\nTime          10.0\nI(Time^2)     -1.5",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question: In the model `score ~ Time + (Time | Subject)`, what does the random effect `(Time | Subject)` allow for?",
    "options": [
      "Each subject to have their own starting point (intercept) ONLY.",
      "Each subject to have their own starting point AND their own rate of change.",
      "Each subject to have their own rate of change ONLY (intercept is fixed).",
      "Time to vary randomly. (additional context) (additio"
    ],
    "answer": 1,
    "explanation": "The random slope (Time | Subject) allows the effect of Time (rate of change) to vary across subjects.",
    "id": 340,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question: Why might you center the `Time` variable (e.g., set the start to 0) in a Growth Curve Model?",
    "options": [
      "To make the intercept interpretable as the status at the beginning of the study.",
      "To eliminate random effects.",
      "It is required by the `lmer` function.",
      "To increase the p-value."
    ],
    "answer": 0,
    "explanation": "Centering Time at 0 makes the intercept represent the status at the start of the study.",
    "id": 341,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nIf the variance for `Time` were 0, what would that imply?",
    "options": [
      "Everyone starts at the same score.",
      "Everyone changes at the exact same rate (no individual differences in slope).",
      "There is no measurement error.",
      "The model is invalid."
    ],
    "answer": 1,
    "explanation": "Zero variance in Time means all subjects change at the exact same rate.",
    "id": 342,
    "codeSnippet": "Random effects:\nGroups   Name        Variance\nSubject  (Intercept) 200.0\nTime         20.0",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "tf",
    "question": "Question: True or False: In a Growth Curve Model, the \"Random Intercept\" represents the variance in baseline performance across participants (assuming Time=0 is baseline).",
    "options": [
      "True",
      "False"
    ],
    "answer": 0,
    "explanation": "True. The random intercept captures individual differences in the starting value.",
    "id": 343,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question: What is the \"maximal\" random effects structure usually recommended for GCM (if it converges)?",
    "options": [
      "Random intercepts only.",
      "Random intercepts and random slopes for all within-subject factors.",
      "No random effects.",
      "Random slopes only."
    ],
    "answer": 1,
    "explanation": "Maximal structure includes random intercepts and random slopes for all within-subject factors.",
    "id": 344,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question: If your maximal model fails to converge, what is a common pragmatic next step?",
    "options": [
      "Give up and use ANOVA.",
      "Simplify the random effects (e.g.,",
      "Add more fixed effects.",
      "Increase the alpha level."
    ],
    "answer": 1,
    "explanation": "Simplifying the random effects structure is the standard approach to resolve convergence issues.",
    "id": 345,
    "module": "Module 12: Growth Curve Modeling"
  },
  {
    "type": "mc",
    "question": "Question: You want to compare the mean happiness scores of 3 different groups (A, B, C) and do not have repeated measures. Which linear model is equivalent to a One-Way ANOVA?",
    "options": [
      "`lm(happiness ~ 1)`",
      "`lm(happiness ~ group)`",
      "`lm(happiness ~ group + age)`",
      "`lmer(happiness ~ (1|group))`"
    ],
    "answer": 1,
    "explanation": "One-Way ANOVA is mathematically equivalent to a linear model with a categorical predictor.",
    "id": 346,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: You have data on students nested within classrooms. You want to predict test scores based on study time. Which model is most appropriate?",
    "options": [
      "Simple Linear Regression (`lm`)",
      "Multiple Regression (`lm` with dummies)",
      "Mixed-Effects Model (`lmer`) with random intercepts for Classrooms.",
      "Chi-square test."
    ],
    "answer": 2,
    "explanation": "Mixed models properly account for the nesting of students within classrooms.",
    "id": 347,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: You want to test if the relationship between Stress and Performance is curvilinear (U-shaped). What do you use?",
    "options": [
      "Interaction term.",
      "Polynomial regression (quadratic term).",
      "T-test.",
      "Random slopes."
    ],
    "answer": 1,
    "explanation": "A U-shape is a quadratic relationship, requiring a polynomial term.",
    "id": 348,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: You measured reaction time for the same subjects at 4 different time points. You want to see if reaction time decreases over time.",
    "options": [
      "Independent samples t-test.",
      "Growth Curve Model (Mixed Model with Time).",
      "Pearson Correlation.",
      "One-way ANOVA."
    ],
    "answer": 1,
    "explanation": "Repeated measures over time call for a Growth Curve Model.",
    "id": 349,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: Which statistic is best for comparing non-nested models (e.g., models with different dependent variables)?",
    "options": [
      "AIC",
      "ANOVA (Likelihood Ratio Test)",
      "You cannot directly compare them using AIC/BIC if outcomes differ.",
      "R-squared"
    ],
    "answer": 2,
    "explanation": "Fit indices like AIC require the same outcome data).",
    "id": 350,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: What is the \"Null Ritual\"?",
    "options": [
      "A statistical test for zero variance.",
      "The mindless checking of p < 0.05 without considering effect size or context.",
      "A method for handling missing data.",
      "Setting all intercepts to null."
    ],
    "answer": 1,
    "explanation": "**Null Hypothesis (H0)**: Assumption of No Effect, No Difference, or r = 0.",
    "id": 351,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "tf",
    "question": "Question: True or False: A p-value of 0.001 means the effect size is very large.",
    "options": [
      "True",
      "False"
    ],
    "answer": 1,
    "explanation": "It means the result is unlikely under the null; tiny effects can have tiny p-values in large samples).",
    "id": 352,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nWhat percent of the variance in the outcome is explained by the predictors?",
    "options": [
      "5.2%",
      "42%",
      "45%",
      "62%"
    ],
    "answer": 2,
    "explanation": "Multiple R-squared).",
    "id": 353,
    "codeSnippet": "> summary(model)",
    "codeOutput": "...\nResidual standard error: 5.2 on 48 degrees of freedom\nMultiple R-squared:  0.45,\tAdjusted R-squared:  0.42\nF-statistic: 19.6 on 2 and 48 DF,  p-value: 6.2e-07",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: In the output above (\\#56), is the overall model statistically significant?",
    "options": [
      "Yes, p \\< 0.05.",
      "No, p \\> 0.05.",
      "Cannot tell."
    ],
    "answer": 0,
    "explanation": "p-value 6.2e-07 is 0.00000062).",
    "id": 354,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nDoes this result suggest a violation of the normality assumption?",
    "options": [
      "Yes, because p \\> 0.05.",
      "Yes, because W is not 1.",
      "No, because p > 0.05, we fail to reject the null hypothesis of normality.",
      "No, Shapiro-Wilk is for homogeneity of variance."
    ],
    "answer": 2,
    "explanation": "A p-value > 0.05 indicates we do not reject the null hypothesis of normality.",
    "id": 355,
    "codeSnippet": "shapiro.test(residuals(model))\nW = 0.98, p-value = 0.65",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nIs there a multicollinearity problem here?",
    "options": [
      "No, all values are below 10.",
      "Yes, x3 has a VIF of 8.5, which is notably high (rule of thumb > 5 or 10).",
      "Yes, x1 is too low.",
      "VIF measures outliers, not collinearity."
    ],
    "answer": 1,
    "explanation": "VIF > 5 or 10 indicates potential multicollinearity issues.",
    "id": 356,
    "codeSnippet": "vif(model)\nx1    x2    x3\n1.2   1.1   8.5",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: A researcher finds that `Income` predicts `Happiness` linearly. However, they suspect that for very high incomes, happiness levels off (asymptotic). Which model limitation does polynomial regression (e.g., quadratic) share with linear regression regarding this specific \"leveling off\" theory?",
    "options": [
      "It cannot model curves.",
      "Polynomials tend to curve back down or",
      "It assumes equal variance."
    ],
    "answer": 1,
    "explanation": "Polynomials tend to curve back down or shoot up at the tails rather than truly plateauing (asymptotic).",
    "id": 357,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: You are analyzing data on chick weights over time (Growth Curve). You add `Diet` as a fixed effect. This turns your \"Unconditional Growth Model\" into a...?",
    "options": [
      "Conditional Growth Model.",
      "Random Intercept Model.",
      "Null Model.",
      "Logistic Model."
    ],
    "answer": 0,
    "explanation": "Adding a fixed effect to an unconditional growth model makes it a Conditional Growth Model.",
    "id": 358,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output - Interaction in GCM]\nReference is Diet1. What does `Time:Diet3` = 4.00 mean?",
    "options": [
      "Diet3 chicks are 4 grams heavier than Diet1 chicks at the start.",
      "Diet3 chicks grow at a rate 4 units *faster* per time unit than Diet1 (reference).",
      "Diet3 chicks grow at a rate of 4 units per time unit total.",
      "Diet3 has no effect. (additional context)"
    ],
    "answer": 1,
    "explanation": "The interaction term indicates the difference in slope relative to the reference group.",
    "id": 359,
    "codeSnippet": "Fixed effects:\nTime:Diet2    2.50   (p = 0.03)\nTime:Diet3    4.00   (p = 0.001)",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: You run a mixed model and get: `boundary (singular) fit: see ?isSingular`. What is a likely cause?",
    "options": [
      "The fixed effects are not significant.",
      "The random effects variance is estimated to be zero or effectively zero.",
      "You have too much data.",
      "The residuals are not normal."
    ],
    "answer": 1,
    "explanation": "Singular fit usually indicates that the random effects variance is estimated to be near zero or perfectly correlated.",
    "id": 360,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: Which plot is best for checking the **homoscedasticity** assumption?",
    "options": [
      "Q-Q Plot.",
      "Residuals vs. Fitted Plot.",
      "Histogram of Residuals.",
      "Scatterplot of X vs Y."
    ],
    "answer": 1,
    "explanation": "Looking for a \"funnel\" or random cloud, not a pattern).",
    "id": 361,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: Which plot is best for checking the **normality of residuals** assumption?",
    "options": [
      "Q-Q Plot.",
      "Residuals vs. Fitted Plot.",
      "Cook's Distance Plot.",
      "Boxplot of predictors."
    ],
    "answer": 0,
    "explanation": "A Q-Q plot compares the residuals' distribution to a theoretical normal distribution.",
    "id": 362,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nObservation 3 has a hat value of 0.60 (much higher than others). What does this indicate?",
    "options": [
      "It is an outlier in the Y direction.",
      "It has high leverage (outlier in X space).",
      "It has high influence (Cook's D).",
      "It is perfectly normal."
    ],
    "answer": 1,
    "explanation": "Hat values measure leverage, which identifies outliers in the predictor space.",
    "id": 363,
    "codeSnippet": "hatvalues(model)\n1      2      3 ...\n0.05   0.02   0.60",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: To formally test if a specific influential point is changing your coefficients significantly, what metric would you look at?",
    "options": [
      "R-squared",
      "Cook's Distance",
      "AIC",
      "The intercept"
    ],
    "answer": 1,
    "explanation": "Cook's Distance measures the influence of a data point on the model coefficients.",
    "id": 364,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: In a polynomial regression `y ~ x + I(x^2)`, you decide to center `x` (i.e., `x_centered = x - mean(x)`). What effect does this typically have?",
    "options": [
      "It changes the shape of the curve.",
      "It changes the $R^2$ of the model.",
      "It reduces the correlation (multicollinearity) between the linear and quadratic terms.",
      "It makes the model non-linear."
    ],
    "answer": 2,
    "explanation": "Centering variables reduces structural multicollinearity between the linear and polynomial terms.",
    "id": 365,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nWhat does \"Satterthwaite's method\" refer to?",
    "options": [
      "A method for estimating degrees of freedom in mixed models.",
      "A method for calculating R-squared.",
      "A method for outlier detection.",
      "A type of plot."
    ],
    "answer": 0,
    "explanation": "Satterthwaite's method is used to estimate effective degrees of freedom for t-tests in mixed models.",
    "id": 366,
    "codeSnippet": "Linear mixed model fit by REML\nt-tests use Satterthwaite's method",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: If you are analyzing repeated measures data (e.g., 3 time points per person) using `lm()` (standard regression), which assumption are you definitely violating?",
    "options": [
      "Linearity.",
      "Independence of observations (residuals are correlated).",
      "Normality.",
      "Homoscedasticity."
    ],
    "answer": 1,
    "explanation": "Standard regression assumes independence, which is violated by repeated measures on the same subjects.",
    "id": 367,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: What is the range of possible values for the Intraclass Correlation Coefficient (ICC)?",
    "options": [
      "-1 to 1",
      "0 to 1",
      "-infinity to +infinity",
      "0 to 100"
    ],
    "answer": 1,
    "explanation": "ICC ranges from 0 to 1, representing the proportion of variance explained by the grouping structure.",
    "id": 368,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: An ICC of 0 indicates:",
    "options": [
      "Perfect clustering (all variance is between groups).",
      "No clustering (all variance is within groups/residual).",
      "The model is broken.",
      "Strong negative correlation."
    ],
    "answer": 1,
    "explanation": "An ICC of 0 means there is no variance between groups; all variance is within groups.",
    "id": 369,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: You want to predict `Vocabulary` growth in children (ages 2 to 10). You expect growth to be fast at first and then slow down. Which polynomial term coefficient should be **negative** in your model?",
    "options": [
      "The Intercept",
      "The Linear Time slope",
      "The Quadratic Time slope (`I(Time^2)`).",
      "The Random Intercept"
    ],
    "answer": 2,
    "explanation": "A negative quadratic term creates a downward curvature, representing deceleration.",
    "id": 370,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nWhich model is better according to AIC?",
    "options": [
      "The Additive model (+)",
      "The Interaction model (\\*)"
    ],
    "answer": 1,
    "explanation": "Lower AIC is better).",
    "id": 371,
    "codeSnippet": "Formula: weight ~ Time + Diet\nAIC: 5500\nFormula: weight ~ Time * Diet\nAIC: 5400",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: When reporting a Growth Curve Analysis, what is crucial to include besides the p-values?",
    "options": [
      "The raw data for every participant.",
      "The type of polynomial used, the random effects structure, and fit indices.",
      "The name of the computer used.",
      "Only the R-squared."
    ],
    "answer": 1,
    "explanation": "Reporting requirements include the model structure, random effects, and parameter estimates, not just significance.",
    "id": 372,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: In `ggplot2`, which geometric object is useful for adding the fitted regression line (including polynomial fits) to a plot?",
    "options": [
      "`geom_point()`",
      "`geom_smooth()`",
      "`geom_bar()`",
      "`geom_boxplot()`"
    ],
    "answer": 1,
    "explanation": "geom_smooth adds a smoothed conditional mean, often a regression line.",
    "id": 373,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output - Orthogonal Polynomials]\nCan you interpret the coefficient `50.0` as \"the increase in outcome for a 1-unit increase in Time\"?",
    "options": [
      "Yes.",
      "No, because orthogonal polynomials are on a transformed scale, not the raw units."
    ],
    "answer": 1,
    "explanation": "Orthogonal polynomials are transformed and uncorrelated, so coefficients don't represent simple unit changes in the raw variable.",
    "id": 374,
    "codeSnippet": "Coefficients:\n(Intercept)  10.0\npoly(Time,2)1 50.0\npoly(Time,2)2 10.0",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: Which function from the `performance` package can you use to check model performance metrics (like $R^2$, AIC, ICC) all at once?",
    "options": [
      "`summary()`",
      "`model_performance()` or",
      "`plot()`",
      "`anova()`"
    ],
    "answer": 1,
    "explanation": "The performance package provides tools like model_performance() to check multiple metrics.",
    "id": 375,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nIs the predictor significant?",
    "options": [
      "Yes, p = 0.008.",
      "No.",
      "The null model is better.",
      "AIC suggests the null model."
    ],
    "answer": 0,
    "explanation": "P-value < 0.05 indicates statistical significance.",
    "id": 376,
    "codeSnippet": "anova(m_null, m_full)\nModels:\nm_null: outcome ~ 1 + (1 | group)\nm_full: outcome ~ predictor + (1 | group)\nDf    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)\nm_null  3 1050.0 1060.0 -522.0\nm_full  4 1045.0 1058.0 -518.5   7.0      1   0.008 **",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: What is a \"Random Slope\"?",
    "options": [
      "It allows the relationship between a predictor and outcome to vary across groups.",
      "It allows the mean of the outcome to vary across groups.",
      "It is the error term.",
      "It is a fixed constant."
    ],
    "answer": 0,
    "explanation": "Random slopes allow the effect of a predictor to vary by group/subject.",
    "id": 377,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: In the equation $Y_{si} = b_{0s} + b_{1s}Time_i + e_{si}$, if $b_{1s} = b_1 + S_{1s}$, what does $S_{1s}$ represent?",
    "options": [
      "The fixed slope for the whole population.",
      "The random deviation of subject $s$'s slope from the overall average slope.",
      "The intercept.",
      "The residual error."
    ],
    "answer": 1,
    "explanation": "S_1s is the subject-specific random deviation from the population fixed slope.",
    "id": 378,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: If you are studying \"Teams\" nested in \"Organizations,\" how would you specify nested random intercepts in `lmer`?",
    "options": [
      "`(1 | Organization) + (1 | Team)` (if Team IDs are unique across organizations).",
      "`(1 | Organization/Team)` (if Team IDs are recycled, e.g., Team 1 in Org A and Team 1 in Org B).",
      "Both A and B can be correct depending on coding.",
      "All of the above."
    ],
    "answer": 3,
    "explanation": "Both specifications can work depending on whether IDs are unique or recycled, making 'All of the above' the safest answer.",
    "id": 379,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: What does `REML=FALSE` (Maximum Likelihood) allow you to do that `REML=TRUE` does not?",
    "options": [
      "Get accurate variance estimates.",
      "Compare models with different **fixed effects** structure.",
      "Run the model faster.",
      "Ignore assumptions."
    ],
    "answer": 1,
    "explanation": "ML (REML=FALSE) is required when comparing models with different fixed effects using LRT.",
    "id": 380,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nThere is a high correlation (-0.85) between the `Time` and `Time2` (quadratic) estimates. What does this suggest?",
    "options": [
      "The model is wrong.",
      "This is essential multicollinearity; centering Time would reduce it.",
      "Time causes Time2.",
      "You should drop Time2."
    ],
    "answer": 1,
    "explanation": "High correlation between polynomial terms usually indicates essential multicollinearity; centering helps.",
    "id": 381,
    "codeSnippet": "Correlation of Fixed Effects:\n(Intr)  Time\nTime  -0.40\nTime2  0.10   -0.85",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: Why is it dangerous to interpret the main effect of `Time` in a model where the interaction `Time:Diet` is significant?",
    "options": [
      "Because the effect of Time depends on which Diet the subject is in.",
      "Because Time is not real.",
      "Because the p-values are wrong.",
      "You should always interpret main effects regardless of interactions."
    ],
    "answer": 0,
    "explanation": "An interaction means the effect of one variable changes depending on the level of the other.",
    "id": 382,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: A researcher uses a 10th-order polynomial for a dataset with 20 points. What is the likely problem?",
    "options": [
      "Underfitting.",
      "Overfitting (modeling noise).",
      "Perfect fit (good thing).",
      "Linearity."
    ],
    "answer": 1,
    "explanation": "Using a high-degree polynomial on small data fits noise, leading to overfitting.",
    "id": 383,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: If the `t-value` for a fixed effect in `lmer` is 1.5, is it statistically significant at alpha=0.05?",
    "options": [
      "Yes.",
      "No (Needs to be \\> \\~1.96).",
      "Yes, because it's positive."
    ],
    "answer": 1,
    "explanation": "A t-value of 1.5 corresponds to a p-value > 0.05; significance usually requires |t| >= 1.96.",
    "id": 384,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: What does the \"Intercept\" represent in a model using orthogonal polynomials `poly(x, 2)`?",
    "options": [
      "The value of Y when x = 0.",
      "The grand mean of Y.",
      "It has no meaning.",
      "The slope."
    ],
    "answer": 1,
    "explanation": "With orthogonal polynomials, the intercept represents the grand mean of the response.",
    "id": 385,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nWhat can you conclude about the effect of Treatment?",
    "options": [
      "It is statistically significant.",
      "It is not statistically significant (95% CI includes 0).",
      "The effect size is exactly 0.",
      "The effect is negative."
    ],
    "answer": 1,
    "explanation": "If the confidence interval includes zero, the effect is not statistically significant.",
    "id": 386,
    "codeSnippet": "Confidence Intervals (95%)\nVariable      2.5%    97.5%\nTreatment     -0.5     2.3",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: In the context of the ChickWeight data used in class, what was the dependent variable?",
    "options": [
      "Time",
      "Diet",
      "Weight",
      "Chick ID"
    ],
    "answer": 2,
    "explanation": "In the ChickWeight dataset, Weight is the outcome variable.",
    "id": 387,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: You want to check if a random slope for `Day` improves your model `m0`.\n`m0 <- lmer(y ~ Day + (1|Subj))`\n`m1 <- lmer(y ~ Day + (1 + Day|Subj))`\n`anova(m0, m1)` yields p = 0.60.\nWhat do you do?",
    "options": [
      "Keep the random slope (m1) because more complex is better.",
      "Stick with the simpler model (m0) because the random slope did not significantly improve fit.",
      "Change the fixed effects."
    ],
    "answer": 1,
    "explanation": "If the more complex model (m1) is not significantly better (p > 0.05), prefer the simpler model (m0).",
    "id": 388,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: What does the function `coef(model)` return for a mixed model?",
    "options": [
      "Only the fixed effects.",
      "Only the variance components.",
      "The sum of fixed effects + random effects for each group (BLUPs).",
      "The p-values."
    ],
    "answer": 2,
    "explanation": "coef() returns the sum of fixed and random effects for each group.",
    "id": 389,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: What does the function `fixef(model)` return?",
    "options": [
      "The fixed effects estimates (population averages).",
      "The random effects.",
      "The residuals."
    ],
    "answer": 0,
    "explanation": "fixef() extracts only the fixed effects parameters.",
    "id": 390,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nIf you add random slopes and the RMSE drops to 25.0, does the model fit better or worse?",
    "options": [
      "Worse (higher error).",
      "Better (lower error)."
    ],
    "answer": 1,
    "explanation": "Lower RMSE indicates smaller prediction errors, hence a better fit.",
    "id": 391,
    "codeSnippet": "Data: sleepstudy\nFormula: Reaction ~ Days + (1 | Subject)\nRMSE: 30.0",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: In the \"Politeness\" data example from class, \"Attitude\" (polite vs informal) was a:",
    "options": [
      "Random Effect.",
      "Fixed Effect.",
      "Outcome variable.",
      "Covariate."
    ],
    "answer": 1,
    "explanation": "Attitude was a manipulated condition, thus a Fixed Effect.",
    "id": 392,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: In the \"Politeness\" data example, \"Scenario\" (different situations like asking for a favor) was modeled as:",
    "options": [
      "Fixed Effect.",
      "Random Intercept.",
      "Outcome.",
      "Not included."
    ],
    "answer": 1,
    "explanation": "Scenario was a random factor, specifically a Random Intercept.",
    "id": 393,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "tf",
    "question": "Question: True or False: You can typically use `lmer` to analyze data with only 1 observation per subject.",
    "options": [
      "True.",
      "False (You generally need multiple observations per group to estimate random effects/variance within group)."
    ],
    "answer": 1,
    "explanation": "Mixed models require multiple observations per group to separate within-group and between-group variance.",
    "id": 394,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: Which is a valid reason to use a **conditional** R-squared?",
    "options": [
      "To see how much variance the fixed effects explain alone.",
      "To see how much variance the *entire* model (including random effects) explains.",
      "To calculate p-values. (additional cont"
    ],
    "answer": 1,
    "explanation": "Conditional R-squared measures variance explained by both fixed and random effects.",
    "id": 395,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question: If your residual plot shows a \"fan\" shape (spread increases as fitted values increase), which assumption is violated?",
    "options": [
      "Normality.",
      "Homoscedasticity (Homogeneity of Variance).",
      "Linearity.",
      "Independence."
    ],
    "answer": 1,
    "explanation": "A fan shape in residuals indicates heteroscedasticity, violating the homoscedasticity assumption.",
    "id": 396,
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Question:** **[Mock R Output]\nBecause this uses orthogonal polynomials, what is the correlation between the estimates for `Poly(Time,2)1` (linear component) and `Poly(Time,2)2` (quadratic component)?",
    "options": [
      "High positive.",
      "High negative.",
      "Zero (by definition of orthogonality).",
      "Unknown."
    ],
    "answer": 2,
    "explanation": "Orthogonal polynomials are constructed to be uncorrelated with each other.",
    "id": 397,
    "codeSnippet": "(Intercept)   100\nPoly(Time,2)1  10\nPoly(Time,2)2  -5",
    "codeOutput": "Output hidden for quiz mode",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "type": "mc",
    "question": "Based on the following R output for a simple linear regression predicting `sales` from `adverts`, what is the predicted increase in sales for every 1 unit increase in `adverts`?",
    "options": [
      "134.14 units",
      "0.096 units",
      "9.56 units",
      "0.33 units"
    ],
    "answer": 1,
    "explanation": "**Regression Coefficient**: The estimate for `adverts` (0.096) represents the slope: the change in the outcome for a 1-unit change in the predictor.",
    "id": 398,
    "codeSnippet": "Call:\nlm(formula = sales ~ adverts, data = album2)\n\nCoefficients:\n(Intercept)      adverts  \n  134.140        0.096  ",
    "codeOutput": "Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1.341e+02  7.537e+00  17.799   <2e-16 ***\nadverts     9.612e-02  9.632e-03   9.979   <2e-16 ***",
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "In the summary output below, what does the `Multiple R-squared: 0.3346` tell us?",
    "options": [
      "The model explains approximately 33.5% of the variance in sales.",
      "The correlation between sales and adverts is 0.33.",
      "The model has 33.5% error.",
      "The p-value is 0.335."
    ],
    "answer": 0,
    "explanation": "**R-squared**: Represents the proportion of variance in the outcome explained by the model (0.3346 = 33.46%).",
    "id": 399,
    "codeSnippet": "> summary(model)\n...\nResidual standard error: 65.99 on 198 degrees of freedom\nMultiple R-squared:  0.3346,\tAdjusted R-squared:  0.3313 \nF-statistic: 99.59 on 1 and 198 DF,  p-value: < 2.2e-16",
    "module": "Module 5: Intro to Regression (Simple Linear)"
  },
  {
    "type": "mc",
    "question": "You run a regression predicting `vocal_pitch` from `gender` (where Female = 0, Male = 1). Based on the output, what is the average vocal pitch for the Female group?",
    "options": [
      "226.33 Hz",
      "128.33 Hz",
      "98.00 Hz",
      "0.00 Hz"
    ],
    "answer": 0,
    "explanation": "**Categorical Intercept**: When the predictor is 0 (Female), the Intercept (226.33) represents the mean of that group.",
    "id": 400,
    "codeSnippet": "Call:\nlm(formula = pitch ~ gender, data = df)\n\nCoefficients:\n(Intercept)      gender1  \n     226.33       -98.00  ",
    "module": "Module 6: Categorical Predictors"
  },
  {
    "type": "mc",
    "question": "In the same model (pitch ~ gender), what does the coefficient for `gender1` (-98.00) represent?",
    "options": [
      "The average pitch of the Male group.",
      "The difference in average pitch between the Male group and the Female group.",
      "The standard deviation of the Male group.",
      "The intercept for the Female group."
    ],
    "answer": 1,
    "explanation": "**Dummy Coding Slope**: The coefficient represents the difference in means between the category coded as 1 and the reference category (0). Mean(Male) = Intercept + Slope = 226.33 - 98.00 = 128.33.",
    "id": 401,
    "codeSnippet": "Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   226.33       8.43   26.85 2.15e-06 ***\ngender1       -98.00      11.92   -8.22 0.00119 ** ",
    "module": "Module 6: Categorical Predictors"
  },
  {
    "type": "mc",
    "question": "Referencing the output of `contrasts(df$gender)` below, which category is the Reference Group?",
    "options": [
      "Female",
      "Male",
      "Both",
      "Neither"
    ],
    "answer": 0,
    "explanation": "**Reference Group**: The category row with all zeros (or the one not shown as a column, depending on display) is the reference. Here, 'Female' maps to 0.",
    "id": 402,
    "codeSnippet": "> contrasts(df$gender)\n       Male\nFemale    0\nMale      1",
    "module": "Module 6: Categorical Predictors"
  },
  {
    "type": "mc",
    "question": "If you center the predictor `Age` (subtracting the mean) and rerun the regression `pitch ~ Age_Cent`, how does the interpretation of the Intercept change?",
    "options": [
      "It doesn't change.",
      "It now represents the predicted pitch when Age is 0 (newborn).",
      "It now represents the predicted pitch for a person of *average* age.",
      "The intercept becomes zero."
    ],
    "answer": 2,
    "explanation": "**Centering**: When X is centered, 0 represents the mean. Thus, the intercept becomes the predicted Y at the mean of X.",
    "id": 403,
    "codeSnippet": "> df$Age_Cent <- scale(df$Age, center=TRUE, scale=FALSE)\n> lm(pitch ~ Age_Cent, data = df)",
    "module": "Module 6: Categorical Predictors"
  },
  {
    "type": "mc",
    "question": "Based on the F-statistic output below, is the overall model statistically significant?",
    "options": [
      "Yes, p < 0.05",
      "No, p > 0.05",
      "Cannot tell without the Alpha level.",
      "Yes, because the F-statistic is negative."
    ],
    "answer": 0,
    "explanation": "**F-test Significance**: The p-value (6.2e-07) is far less than 0.05, indicating the model explains significantly more variance than the null model.",
    "id": 404,
    "codeSnippet": "Residual standard error: 5.2 on 48 degrees of freedom\nMultiple R-squared:  0.45\nF-statistic: 19.6 on 2 and 48 DF,  p-value: 6.2e-07",
    "module": "Module 13: Course Wrap-up & Integration"
  },
  {
    "id": 405,
    "type": "mc",
    "question": "Which of the following arguably represents the most critical reason for checking the assumption of independence of errors in a multiple regression analysis?",
    "options": [
      "It ensures that the residuals are normally distributed, which allows for accurate confidence interval construction.",
      "It confirms that the variance of the residuals is constant across all levels of the predicted values.",
      "It prevents the underestimation of standard errors, which would otherwise lead to an inflated Type I error rate.",
      "It guarantees that there is a linear relationship between each predictor variable and the outcome variable."
    ],
    "answer": 2,
    "explanation": "The assumption of independence of errors is crucial because if errors are correlated (e.g., in time-series or clustered data), the standard errors of the coefficients will likely be underestimated. This makes the coefficients appear more significant than they actually are, increasing the risk of Type I errors (false positives).",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 406,
    "type": "mc",
    "question": "You are examining a residual plot (residuals vs. fitted values) for a multiple regression model. You observe a distinct funnel shape where the spread of the residuals increases as the fitted values increase. What does this pattern indicate?",
    "options": [
      "The assumption of linearity has been violated.",
      "The assumption of homoscedasticity has been violated.",
      "The assumption of normality of residuals has been violated.",
      "The assumption of no perfect multicollinearity has been violated."
    ],
    "answer": 1,
    "explanation": "A funnel or cone shape in the residual vs. fitted values plot is a classic sign of heteroscedasticity (violation of homoscedasticity). It means the variance of the errors is not constant across all levels of the predicted value.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 407,
    "type": "mc",
    "question": "In the context of multiple regression, what does the Variance Inflation Factor (VIF) primarily measure?",
    "options": [
      "The proportion of variance in the outcome variable explained by the fixed effects alone.",
      "The extent to which the variance of a regression coefficient is increased due to multicollinearity.",
      "The influence of a single observation on the overall regression coefficients.",
      "The distance of an observation from the centroid of the predictor variables."
    ],
    "answer": 1,
    "explanation": "The VIF measures how much the variance of an estimated regression coefficient is increased (\"inflated\") because of Collinearity. A VIF of 1 indicates no correlation, while high values (e.g., > 10) suggest problematic multicollinearity.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 408,
    "type": "mc",
    "question": "A researcher finds that the residuals of their regression model are not normally distributed. The sample size is N = 500. How should the researcher proceed?",
    "options": [
      "They must transform the outcome variable immediately to achieve normality.",
      "They should switch to a non-parametric test because validity is compromised.",
      "They can likely proceed with the analysis as the assumption is robust with large samples.",
      "They should remove all outliers that are contributing to the non-normality."
    ],
    "answer": 2,
    "explanation": "The assumption of normality of residuals is least critical when the sample size is large (e.g., > 30-50, and certainly 500) due to the Central Limit Theorem, which ensures that the sampling distribution of the coefficients will be approximately normal even if the errors are not.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 409,
    "type": "mc",
    "question": "Which of the following diagnostic plots is most appropriate for checking the linearity assumption in multiple regression?",
    "options": [
      "A Q-Q plot of the residuals.",
      "A plot of residuals versus observation order.",
      "A plot of residuals versus fitted values.",
      "A histogram of the standardized residuals."
    ],
    "answer": 2,
    "explanation": "A plot of residuals versus fitted values (or residuals vs. predictors) is the primary tool for checking linearity. Patterns such as curves or U-shapes in this plot suggest a non-linear relationship that the linear model has failed to capture.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 410,
    "type": "mc",
    "question": "You calculate the Durbin-Watson statistic for your regression model and obtain a value of 0.5. What does this suggest?",
    "options": [
      "The residuals are independent and uncorrelated.",
      "There is a strong positive autocorrelation in the residuals.",
      "There is a strong negative autocorrelation in the residuals.",
      "The assumption of homoscedasticity is met."
    ],
    "answer": 1,
    "explanation": "The Durbin-Watson statistic ranges from 0 to 4. A value close to 2 indicates independence. Values approaching 0 indicate positive autocorrelation, while values approaching 4 indicate negative autocorrelation. 0.5 suggests strong positive autocorrelation.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 411,
    "type": "mc",
    "question": "Considering the distinction between Marginal R (Rm) and Conditional R (Rc) in mixed-effects models, which statement is correct?",
    "options": [
      "Rm represents the variance explained by both fixed and random effects.",
      "Rc is always smaller than Rm.",
      "The difference between Rc and Rm represents the variance explained by random effects.",
      "Rm is used to assess the total fit of the model including clustering."
    ],
    "answer": 2,
    "explanation": "Marginal R (Rm) explains variance due to fixed effects only. Conditional R (Rc) explains variance due to both fixed and random effects. Therefore, the difference (Rc - Rm) represents the proportion of variance attributable to the random effects (structure/clustering).",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 412,
    "type": "mc",
    "question": "Which of the following is an effective strategy for addressing a violation of the homoscedasticity assumption?",
    "options": [
      "Adding an interaction term to the model.",
      "Using Weighted Least Squares (WLS) estimation.",
      "Centering all continuous predictor variables.",
      "Increasing the sample size of the study."
    ],
    "answer": 1,
    "explanation": "Weighted Least Squares (WLS) is a specific method designed to handle heteroscedasticity by assigning less weight to observations with higher variance. Transforming the outcome (e.g., log transform) or using robust standard errors are other common solutions.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 413,
    "type": "mc",
    "question": "What does a \"tolerance\" value of 0.05 indicate in a multiple regression analysis?",
    "options": [
      "The predictor has only 5% of its variance shared with other predictors.",
      "There is no issue with multicollinearity for this predictor.",
      "The predictor is almost perfectly correlated with other predictors, indicating a problem.",
      "The spread of residuals is within 5% of the mean predicted value."
    ],
    "answer": 2,
    "explanation": "Tolerance is calculated as 1/VIF. A low tolerance (e.g., < 0.1 or < 0.2) indicates high multicollinearity. A value of 0.05 corresponds to a VIF of 20, which suggests severe multicollinearity.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 414,
    "type": "mc",
    "question": "When checking for influential points, which metric measures the influence of an observation on the fitted values?",
    "options": [
      "DFBETAS",
      "DFFITS",
      "Variance Inflation Factor",
      "Mahalanobis Distance"
    ],
    "answer": 1,
    "explanation": "DFFITS measures the influence of an observation on the fitted (predicted) values. DFBETAS measures influence on the regression coefficients themselves. Cook's Distance is a global measure of influence.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 415,
    "type": "mc",
    "question": "In a study where students are nested within classrooms, you find that Rm = 0.10 and Rc = 0.45. What is the most appropriate interpretation?",
    "options": [
      "The fixed effects (predictors) are very strong, explaining most of the variance.",
      "The random effects (classrooms) account for a substantial portion of the variance (35%).",
      "The model is a poor fit because the Marginal R is so low.",
      "There is no significant clustering effect, so a simple linear regression would suffice."
    ],
    "answer": 1,
    "explanation": "The difference between Rc (0.45) and Rm (0.10) is 0.35. This means that the random effects (grouping by classroom) explain an additional 35% of the variance in the outcome beyond what the fixed predictors explain, indicating substantial clustering.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 416,
    "type": "mc",
    "question": "Which plot is specifically designed to check the assumption of normality of residuals?",
    "options": [
      "Scale-location plot",
      "Residuals vs. Leverage plot",
      "Q-Q (Quantile-Quantile) plot",
      "Component-residual plot"
    ],
    "answer": 2,
    "explanation": "A Q-Q plot compares the quantiles of the residuals to the quantiles of a theoretical normal distribution. If the points fall approximately on the diagonal line, the assumption of normality is met.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 417,
    "type": "mc",
    "question": "Why is \"No Perfect Multicollinearity\" a required assumption for the mathematical calculation of regression coefficients?",
    "options": [
      "High correlation between predictors biases the calculation of the intercept.",
      "Perfect correlation makes it impossible to invert the design matrix, preventing estimation.",
      "It ensures that the residuals have a mean of exactly zero.",
      "It guarantees that there are no outliers in the predictor variables."
    ],
    "answer": 1,
    "explanation": "If there is perfect multicollinearity, the matrix of predictors (X'X) is singular and cannot be inverted. This makes it mathematically impossible to calculate unique Ordinary Least Squares (OLS) estimates for the coefficients.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 418,
    "type": "mc",
    "question": "Which of the following scenarios would most likely trigger a violation of the independence assumption?",
    "options": [
      "Measuring the height and weight of 100 randomly selected individuals.",
      "Measuring the test scores of students from 50 different schools.",
      "Measuring the reaction time of the same participant across 20 different trials.",
      "Measuring the daily sales of a company over the course of a year."
    ],
    "answer": 2,
    "explanation": "Repeated measures designs (measuring the same person multiple times) and time-series data (option D) are the most common sources of non-independence. Students in schools (option B) also violate independence but are typically handled with mixed models. The repeated trials (option C) are a classic case where errors are likely correlated within the subject.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 419,
    "type": "mc",
    "question": "If a researcher identifies a violation of the linearity assumption based on a residual plot, what is a potential remedy?",
    "options": [
      "Using robust standard errors.",
      "Adding polynomial terms (e.g., X) to the model.",
      "Removing the intercept from the model.",
      "Calculating variance inflation factors."
    ],
    "answer": 1,
    "explanation": "Nonlinear patterns in the residuals suggest that the relationship between the predictor and outcome is curvilinear. Adding polynomial terms (quadratic, cubic) allows the model to fit this curve, often correcting the linearity violation.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 420,
    "type": "mc",
    "question": "What does the component-residual (CR) plot help to diagnose?",
    "options": [
      "The presence of outliers in the Y variable.",
      "The linearity of the relationship between a specific predictor and the outcome, controlling for others.",
      "The overall normality of the model residuals.",
      "The leverage values for each observation."
    ],
    "answer": 1,
    "explanation": "Component-residual (or partial residual) plots are used to check the linearity assumption for a specific predictor variable in a multiple regression model, showing the relationship between that predictor and the outcome after \"adjusting\" for the other predictors.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 421,
    "type": "mc",
    "question": "A \"scale-location\" plot (square root of standardized residuals vs. fitted values) is primarily used to check which assumption?",
    "options": [
      "Normality",
      "Linearity",
      "Homoscedasticity",
      "Independence"
    ],
    "answer": 2,
    "explanation": "The scale-location plot shows the spread of residuals across the range of fitted values. A horizontal line suggests homoscedasticity (equal variance). A sloped line suggests heteroscedasticity (spread changes with value).",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 422,
    "type": "mc",
    "question": "Which of the following values for Cook's distance would generally be considered indicative of an influential point?",
    "options": [
      "Values > 0.05",
      "Values > 1",
      "Values < 4/n",
      "Values close to 0"
    ],
    "answer": 1,
    "explanation": "A common rule of thumb is that a Cook's distance greater than 1 indicates a highly influential point. Another stricter cutoff is 4/n. Values close to 0 indicate little influence.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 423,
    "type": "mc",
    "question": "When checking for outliers using standardized residuals, what cutoff is commonly used to identify potential outliers?",
    "options": [
      "Absolute value > 1.96",
      "Absolute value > 3",
      "Absolute value > 0.5",
      "Absolute value > 10"
    ],
    "answer": 1,
    "explanation": "Standardized (or studentized) residuals follow a t-distribution. Values with an absolute value greater than 3 are typically considered outliers, as they lie 3 standard deviations away from the mean residual (0).",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 424,
    "type": "mc",
    "question": "The assumption of \"No Endogeneity\" implies that:",
    "options": [
      "The predictor variables are uncorrelated with the error term.",
      "The predictor variables are uncorrelated with each other.",
      "The error term has a variance of 1.",
      "The outcome variable is normally distributed."
    ],
    "answer": 0,
    "explanation": "Endogeneity occurs when a predictor is correlated with the error term (e.g., due to omitted variable bias). The assumption that predictors are exogenous (uncorrelated with errors) is necessary for causal interpretation of coefficients.",
    "module": "Study-Guide-1",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 425,
    "type": "mc",
    "question": "What is the primary advantage of using the Interquartile Range (IQR) method for outlier detection over Z-scores?",
    "options": [
      "It is computationally easier to calculate.",
      "It is more robust to non-normal distributions and extreme values.",
      "It allows for the identification of multivariate outliers.",
      "It assumes the data follows a perfect normal distribution."
    ],
    "answer": 1,
    "explanation": "The IQR method uses quartiles (Q1 and Q3) which are not influenced by extreme values, unlike the mean and standard deviation used in Z-scores. This makes the IQR method more robust, especially for skewed distributions.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 426,
    "type": "mc",
    "question": "Which of the following measures quantifies the leverage of an observation in a regression analysis?",
    "options": [
      "Studentized Residuals",
      "Hat Values",
      "Cook's Distance",
      "DFFITS"
    ],
    "answer": 1,
    "explanation": "Leverage is measured by \"hat values\" (from the hat matrix). It indicates how far an observation's predictor values are from the mean of the predictor values. High leverage points have unusual combinations of predictors.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 427,
    "type": "mc",
    "question": "When is it most appropriate to \"winsorize\" an outlier?",
    "options": [
      "When you want to completely remove the observation from the dataset to improve R.",
      "When the outlier represents a clear data entry error that cannot be corrected.",
      "When you want to reduce the influence of an extreme value while retaining the observation.",
      "When the sample size is very large and the outlier has no impact."
    ],
    "answer": 2,
    "explanation": "Winsorizing involves replacing extreme values with less extreme values (e.g., the 95th percentile). This approach is useful when you want to keep the data point (preserving sample size) but reduce its disproportionate impact on the model estimates.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 428,
    "type": "mc",
    "question": "In a regression model `Y = b0 + b1*X1 + b2*X2 + b3*(X1*X2)`, what does the coefficient `b3` represent?",
    "options": [
      "The average effect of X1 on Y.",
      "The change in the effect of X1 on Y for a one-unit increase in X2.",
      "The effect of X1 when X2 is zero.",
      "The correlation between X1 and X2."
    ],
    "answer": 1,
    "explanation": "In an interaction model, the interaction coefficient (`b3`) represents the change in the slope of one predictor (X1) for a one-unit increase in the moderator (X2). It quantifies how the relationship between X1 and Y depends on X2.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 429,
    "type": "mc",
    "question": "You run a \"Simple Slopes Analysis\" after finding a significant interaction. What is the main goal of this analysis?",
    "options": [
      "To test if the interaction term itself is significant.",
      "To determine the effect of the predictor at specific, meaningful levels of the moderator.",
      "To remove the non-significant main effects from the model.",
      "To check for multicollinearity between the main effects and the interaction term."
    ],
    "answer": 1,
    "explanation": "Simple slopes analysis allows you to probe a significant interaction by testing the significance of the predictor's slope at specific values of the moderator (e.g., mean, +1 SD, -1 SD). This helps interpret *where* and *how* the effect changes.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 430,
    "type": "mc",
    "question": "Which technique specifically identifies the exact range of moderator values for which the effect of the predictor is statistically significant?",
    "options": [
      "Simple Slopes Analysis",
      "The Pick-a-Point Approach",
      "The Johnson-Neyman Technique",
      "Hierarchical Linear Modeling"
    ],
    "answer": 2,
    "explanation": "The Johnson-Neyman technique (or regions of significance) calculates the precise values of the moderator where the simple slope of the predictor transitions from non-significant to significant, providing a more comprehensive view than testing just a few points.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 431,
    "type": "mc",
    "question": "What is a key benefit of centering continuous variables before creating an interaction term?",
    "options": [
      "It transforms the distribution of the variables to be normal.",
      "It reduces the multicollinearity between the main effects and the interaction term.",
      "It increases the R-squared of the model.",
      "It eliminates the need for checking homoscedasticity."
    ],
    "answer": 1,
    "explanation": "Interaction terms (product of two variables) are often highly correlated with the main effects (the variables themselves). Mean centering the variables before multiplying them reduces this structural multicollinearity, stabilizing the coefficient estimates and making the intercept more interpretable.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 432,
    "type": "mc",
    "question": "In the model `Salary = 30000 + 2000(Experience) + 1500(Education) + 300(Experience  Education)`, how do you interpret the coefficient `300`?",
    "options": [
      "For every year of Education, starting salary increases by $300.",
      "The effect of Experience on Salary increases by $300 for each additional year of Education.",
      "Experience and Education are correlated by 0.30.",
      "People with both Experience and Education earn $300 more on average."
    ],
    "answer": 1,
    "explanation": "The interaction coefficient (300) indicates that the \"return on experience\" is not constant; it depends on education. specifically, for each additional unit of Education, the slope of Experience (the salary increase per year of experience) goes up by 300.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 433,
    "type": "mc",
    "question": "If an interaction between a categorical variable (Treatment vs. Control) and a continuous variable (Age) is NOT significant, what does this imply?",
    "options": [
      "The treatment is ineffective for all ages.",
      "The effect of the treatment is constant across all ages.",
      "You should drop the main effects of Treatment and Age from the model.",
      "There is a non-linear relationship between Age and the outcome."
    ],
    "answer": 1,
    "explanation": "A non-significant interaction suggests that the slopes are parallel. This means the relationship between Treatment and the outcome does not depend on Age; the treatment effect is the same for young and old participants.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 434,
    "type": "mc",
    "question": "Mahalanobis Distance is particularly useful for detecting outliers in which scenario?",
    "options": [
      "When you have a single predictor variable.",
      "When checking for non-normality of residuals.",
      "When you need to account for correlations between multiple variables.",
      "When the sample size is small (< 30)."
    ],
    "answer": 2,
    "explanation": "Mahalanobis distance measures the distance of a point from the centroid of a multivariate distribution, taking into account the covariance (correlations) among the variables. It is ideal for detecting multivariate outliers that might not be outliers on any single variable alone.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 435,
    "type": "mc",
    "question": "What is the primary reason for performing a sensitivity analysis with and without outliers?",
    "options": [
      "To allow you to choose the result that best fits your hypothesis.",
      "To accurately calculate the required sample size for future studies.",
      "To assess the robustness of your results and determine if outliers drive the conclusion.",
      "To prove that the outliers were result of measurement error."
    ],
    "answer": 2,
    "explanation": "Sensitivity analysis (comparing results with and without outliers) allows researchers to see how much influence the outliers have. If the main conclusions remain the same, the results are \"robust.\" If they change, the outliers are influential and must be handled/discussed carefully.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 436,
    "type": "mc",
    "question": "In a \"Categorical  Continuous\" interaction model, what does the coefficient for the categorical variable (main effect) represent?",
    "options": [
      "The average difference between groups across all levels of the continuous variable.",
      "The difference between groups when the continuous variable is zero.",
      "The difference in slopes between the groups.",
      "The overall intercept of the model."
    ],
    "answer": 1,
    "explanation": "In a regression with an interaction, the main effects are \"conditional.\" The coefficient for the categorical variable represents the group difference strictly when the other predictor (continuous) is 0 (or at its mean if centered). It is no longer the \"average\" difference.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 437,
    "type": "mc",
    "question": "Visual inspection of a boxplot allows you to identify outliers as points that fall:",
    "options": [
      "Beyond the whiskers (typically 1.5 * IQR).",
      "Outside the 95% confidence interval of the mean.",
      "More than 2 standard deviations from the median.",
      "Outside the range of the normal distribution curve."
    ],
    "answer": 0,
    "explanation": "In a standard boxplot, the \"whiskers\" extend to the most extreme data point that is no more than 1.5 times the Interquartile Range (IQR) from the box. Points beyond these whiskers are plotted individually as potential outliers.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 438,
    "type": "mc",
    "question": "Why might you choose to conduct a \"Pick-a-Point\" analysis instead of the Johnson-Neyman technique?",
    "options": [
      "When the Johnson-Neyman technique fails to converge.",
      "When testing specific, theoretically meaningful values (e.g., clinically significant cutoffs) is more relevant than finding a region.",
      "When the moderator is a categorical variable.",
      "When you want to avoid calculating standard errors."
    ],
    "answer": 1,
    "explanation": "While Johnson-Neyman is more comprehensive, Pick-a-Point is often chosen when there are specific values of the moderator that have practical meaning (e.g., a \"pass\" score on a test, or a specific dosage), making the interpretation more relevant to the applied context.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 439,
    "type": "mc",
    "question": "If you observe a \"U-shape\" in your residual plot, it suggests that:",
    "options": [
      "You have a significant interaction effect.",
      "You are missing a quadratic trend (non-linearity) in your model.",
      "Your residuals are perfectly normal.",
      "You have severe multicollinearity."
    ],
    "answer": 1,
    "explanation": "A U-shaped (or inverted U-shaped) pattern in the residuals vs. fitted values plot is a classic indicator that the linear model has failed to capture a non-linear (curvilinear) relationship. Adding a squared term (X) is often the solution.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 440,
    "type": "mc",
    "question": "When interpreting a regression model with an interaction `Y ~ X1 * X2`, if the interaction term is significant, why should you be cautious about interpreting the main effects?",
    "options": [
      "The main effects are likely calculated incorrectly.",
      "The main effects are meaningless because they only apply when the other variable is 0.",
      "Statistical significance of main effects is always inflated in interaction models.",
      "You must remove the main effects to interpret the interaction."
    ],
    "answer": 1,
    "explanation": "When an interaction is present, the \"main effect\" coefficients are actually \"simple effects\" conditional on the other variable being 0. Interpreting them as global \"main effects\" (like in ANOVA) is misleading because the effect of X1 represents its effect *specifically* for the case where X2=0.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 441,
    "type": "mc",
    "question": "Which of the following is NOT a valid reason to remove an outlier?",
    "options": [
      "It is a data entry error (e.g., age = 150).",
      "It is a result of equipment malfunction.",
      "The participant does not belong to the target population (e.g., took the wrong test).",
      "The observation reduces the statistical significance of your primary hypothesis."
    ],
    "answer": 3,
    "explanation": "Removing an outlier simply to achieve significance (p-hacking) is a violation of research ethics. Removal must be justified by valid reasons like error, measurement failure, or population ineligibility, not by the desire for a specific result.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 442,
    "type": "mc",
    "question": "Standardized residuals are generally considered more reliable than raw residuals for outlier detection because:",
    "options": [
      "They are always positive, making comparison easier.",
      "They account for the units of measurement, allowing for a standard cutoff (like 3).",
      "They automatically correct for heteroscedasticity.",
      "They are not influenced by the mean of the distribution."
    ],
    "answer": 1,
    "explanation": "Raw residuals depend on the scale of the variable (e.g., dollars vs. millions of dollars). Standardized residuals divide the raw residual by its standard deviation, putting them on a Z-score-like scale where universal cutoffs (like >3) can be applied regardless of the original units.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 443,
    "type": "mc",
    "question": "For a continuous moderator in a simple slopes analysis, what are the standard default values used to probe the interaction?",
    "options": [
      "Minimum, Median, Maximum",
      "Mean, Mean + 1 SD, Mean - 1 SD",
      "25th, 50th, and 75th percentiles",
      "0, 1, and 100"
    ],
    "answer": 1,
    "explanation": "The most common convention (suggested by Aiken & West) for probing interactions with continuous moderators is to test the simple slope at the Mean, one Standard Deviation above the Mean (+1 SD), and one Standard Deviation below the Mean (-1 SD).",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 444,
    "type": "mc",
    "question": "Which influence diagnostic combines information about both the size of the residual (outlier-ness) and the leverage (unusual predictors)?",
    "options": [
      "Tolerance",
      "VIF",
      "Cook's Distance",
      "Eigenvalues"
    ],
    "answer": 2,
    "explanation": "Cook's Distance is a summary measure of influence that accounts for both how far an observation is from the regression line (large residual) and how far it is from the center of the predictor space (high leverage). High Cook's D indicates a point that substantially changes the model estimates.",
    "module": "Study-Guide-2",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 445,
    "type": "mc",
    "question": "In a multiple regression model, what is the key difference between Dummy Coding and Unweighted Effect Coding regarding the intercept?",
    "options": [
      "In Dummy Coding, the intercept represents the grand mean; in Effect Coding, it represents the reference group mean.",
      "In Dummy Coding, the intercept represents the reference group mean; in Effect Coding, it represents the grand mean.",
      "In Dummy Coding, the intercept is always 0; in Effect Coding, it is the mean of the first group.",
      "There is no difference; the intercept always represents the value when all predictors are zero."
    ],
    "answer": 1,
    "explanation": "In Dummy Coding (reference group coding), the intercept is the mean of the specific reference group (coded as 0s). In Unweighted Effect Coding, the intercept represents the unweighted grand mean of all the group means combined.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 446,
    "type": "mc",
    "question": "When using Orthogonal Polynomials (e.g., `poly(X, 2)` in R) in a regression model, what is the interpretation of the linear term coefficient?",
    "options": [
      "It represents the instantaneous slope of the curve at X=0.",
      "It represents the correlation between X and X^2.",
      "It represents the overall linear trend in the data, independent of the quadratic trend.",
      "It represents the value of Y when X is at its minimum."
    ],
    "answer": 2,
    "explanation": "A key property of orthogonal polynomials is that the terms are uncorrelated. This means the linear coefficient captures the \"linear\" component of the relationship, the quadratic coefficient captures the \"U-shape,\" and they can be interpreted (and tested) independently of each other.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 447,
    "type": "mc",
    "question": "You are analyzing a study with 4 distinct treatment groups. If you want to use Dummy Coding, how many dummy variables must you create to include \"Treatment\" in your regression model?",
    "options": [
      "1",
      "3",
      "4",
      "5"
    ],
    "answer": 1,
    "explanation": "To represent a categorical variable with *k* levels, you need *k-1* dummy variables. One group serves as the reference and is defined by having 0 on all dummy variables. Therefore, for 4 groups, you need 4-1 = 3 dummy variables.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 448,
    "type": "mc",
    "question": "What characteristic must a set of contrasts possess to be considered \"orthogonal\"?",
    "options": [
      "The sum of the weights for each contrast must equal 1.",
      "The sum of the cross-products of the weights for any pair of contrasts must equal zero.",
      "The contrasts must allow for overlapping comparisons between groups.",
      "All contrast coefficients must be positive."
    ],
    "answer": 1,
    "explanation": "Orthogonality implies independence. For contrasts to be orthogonal, two conditions must be met: (1) the weights within each contrast sum to zero, and (2) the sum of the products of the corresponding weights for any two contrasts equals zero.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 449,
    "type": "mc",
    "question": "Why might a researcher choose to use \"Raw\" polynomials (e.g., `X + X^2`) instead of Orthogonal polynomials?",
    "options": [
      "Raw polynomials reduce multicollinearity between terms.",
      "Raw polynomials provide p-values that are easier to interpret.",
      "Raw polynomials allow for a direct, intuitive prediction equation (e.g., Y = a + bX + cX^2).",
      "Raw polynomials automatically center the data."
    ],
    "answer": 2,
    "explanation": "While orthogonal polynomials are better for hypothesis testing due to lack of correlation, raw polynomials produce coefficients that correspond directly to the standard algebraic equation of a curve, making it easier to write out the formula for prediction or plotting outside of R.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 450,
    "type": "mc",
    "question": "In a regression with weighted effect coding, what does the intercept represent?",
    "options": [
      "The mean of the largest group.",
      "The simple average of group means (unweighted grand mean).",
      "The weighted grand mean (accounting for sample sizes of groups).",
      "The difference between the first and last group."
    ],
    "answer": 2,
    "explanation": "Weighted effect coding is useful when group sizes differ significantly. The intercept reflects the weighted average of the group means (which equals the sample mean of Y), ensuring that larger groups contribute more to the \"baseline\" estimate.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 451,
    "type": "mc",
    "question": "If the quadratic term (`X^2`) in a polynomial regression is negative and significant, what shape does the relationship take?",
    "options": [
      "A \"U\" shape (convex).",
      "An \"Inverted U\" shape (concave).",
      "An S-shape.",
      "A straight line with a negative slope."
    ],
    "answer": 1,
    "explanation": "The sign of the squared term determines the concavity. A positive coefficient indicates a U-shape (opening up). A negative coefficient indicates an inverted U-shape (opening down), representing a relationship that rises to a peak and then falls.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 452,
    "type": "mc",
    "question": "When interpreting the coefficients from a model using Helmert Contrasts, what is each level compared to?",
    "options": [
      "The reference group (first level).",
      "The grand mean of all levels.",
      "The mean of the subsequent levels.",
      "The mean of the previous levels."
    ],
    "answer": 2,
    "explanation": "Helmert contrasts compare each level of a factor to the mean of the *subsequent* levels. (Reverse Helmert compares to the mean of *previous* levels). This is useful for ordinal variables or sequential stages.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 453,
    "type": "mc",
    "question": "What is the \"Reference Group Problem\" in dummy coding?",
    "options": [
      "If accurate dummy codes are not created, the model will fail to run.",
      "The choice of reference group is arbitrary, but it determines the meaning of all other coefficients.",
      "The reference group must always be the group with the largest sample size.",
      "The reference group cannot have a variance of zero."
    ],
    "answer": 1,
    "explanation": "In dummy coding, all coefficients are differences relative to the chosen reference group. Changing the reference group changes all the coefficient values (though not the model fit). Researchers must consciously choose a meaningful reference (e.g., Control) to make interpretation easy.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 454,
    "type": "mc",
    "question": "If you include both linear (`poly(X,1)`) and quadratic (`poly(X,2)`) orthogonal terms in a model, and only the quadratic term is significant, what does this imply?",
    "options": [
      "The relationship is a perfect straight line.",
      "The relationship is purely symmetrical U-shaped (or inverted U), with no overall linear trend.",
      "You should remove the linear term and re-run the model.",
      "The quadratic term is invalid because the linear term must be significant first."
    ],
    "answer": 1,
    "explanation": "Because orthogonal terms are independent, a non-significant linear term means there is no overall \"tilt\" or linear trend across the data. A significant quadratic term means there is a curve. Together, this implies a symmetrical curve (parabola) centered roughly in the middle of the X range.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 455,
    "type": "mc",
    "question": "Which coding scheme allows you to test specific, a priori hypotheses about differences between combinations of groups (e.g., \"Treatments A & B vs. Control\")?",
    "options": [
      "Dummy Coding",
      "Effect Coding",
      "Contrast Coding",
      "Probability Coding"
    ],
    "answer": 2,
    "explanation": "Contrast coding gives the researcher total control to define specific comparisons (contrasts) based on theory. You can assign weights to pool groups together (e.g., -1 for Control, 0.5 for A, 0.5 for B) to test complex hypotheses directly.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 456,
    "type": "mc",
    "question": "In simple dummy coding with \"Placebo\" as the reference group, what does the coefficient for the \"Drug A\" dummy variable tell you?",
    "options": [
      "The total effect of Drug A on the outcome.",
      "The difference in the mean outcome between Drug A and the Placebo group.",
      "The difference between Drug A and the overall average of all groups.",
      "The slope of the relationship between dose and outcome for Drug A."
    ],
    "answer": 1,
    "explanation": "The coefficient for a dummy variable represents the specific difference in means: Mean(Drug A) - Mean(Reference). It tests whether Drug A is significantly different from the Placebo group.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 457,
    "type": "mc",
    "question": "When using polynomial regression, why is it typically recommended to fit lower-order terms (e.g., X) even if your hypothesis is about the higher-order term (e.g., X^2)?",
    "options": [
      "It is required by the software; R will error otherwise.",
      "Lower-order terms are necessary to correctly center the curve and allow for a shifted vertex.",
      "Higher-order terms are always correlated with the intercept.",
      "It ensures the residuals are normally distributed."
    ],
    "answer": 1,
    "explanation": "A model with only X^2 (`Y = b0 + b2*X^2`) forces the vertex (turning point) of the parabola to be exactly at X=0. Including the linear term X allows the curve to shift left or right, fitting the data wherever the peak/trough actually occurs.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 458,
    "type": "mc",
    "question": "For a categorical variable with 3 levels, what is the sum of the weights for any single contrast in a valid contrast coding scheme?",
    "options": [
      "1",
      "0",
      "N (sample size)",
      "It depends on the specific contrast."
    ],
    "answer": 1,
    "explanation": "A fundamental rule for any valid contrast is that the sum of the weights assigned to the groups must equal zero. This ensures that the contrast represents a comparison of differences (e.g., +1 vs -1) rather than just a sum of means.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 459,
    "type": "mc",
    "question": "You fit a cubic model (`X + X^2 + X^3`) and find that the cubic term is significant. What shape characterizes this relationship?",
    "options": [
      "A simple U-shape.",
      "An S-shape with two potential turning points.",
      "An exponential growth curve.",
      "A flat line."
    ],
    "answer": 1,
    "explanation": "A cubic function allows for two changes in direction (two turning points). This creates an S-shape or a \"wave\" pattern. A significant cubic term essentially adds a \"bend\" to the parabola of the quadratic model.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 460,
    "type": "mc",
    "question": "When using Unweighted Effect Coding with 3 groups, if the coefficients for Group 1 and Group 2 are `b1` and `b2`, how do you calculate the effect for the 3rd group (the reference/omitted group)?",
    "options": [
      "It is simply 0.",
      "It is `-(b1 + b2)`.",
      "It is `(b1 + b2) / 2`.",
      "It is `1 - (b1 + b2)`."
    ],
    "answer": 1,
    "explanation": "In effect coding, the effects must sum to zero across all groups. Therefore, the effect of the omitted group is the negative sum of the estimated coefficients for the other groups: Effect_3 = -(Effect_1 + Effect_2).",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 461,
    "type": "mc",
    "question": "How does multicollinearity affect the interpretation of \"Raw\" polynomial coefficients compared to \"Orthogonal\" ones?",
    "options": [
      "It has no effect; interpretations are identical.",
      "Raw coefficients are highly correlated, so the \"Linear\" term no longer represents the simple linear trend but rather the slope at X=0.",
      "Orthogonal coefficients are harder to interpret because they don't relate to the data.",
      "Raw coefficients always possess lower standard errors."
    ],
    "answer": 1,
    "explanation": "Because X and X^2 are correlated, in a raw polynomial model, the coefficient for X is conditional: it is the slope of the curve specifically where X=0 (the intercept). In orthogonal polynomials, the linear coefficient represents the independent linear trend across the whole dataset.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 462,
    "type": "mc",
    "question": "Which of the following is true about the F-test for a block of dummy variables representing a single categorical predictor with 4 categories?",
    "options": [
      "It tests if any single dummy variable is significant.",
      "It tests the global null hypothesis that there are no differences among any of the 4 group means.",
      "It tests if the intercept is significantly different from zero.",
      "It tests if the reference group is different from the grand mean."
    ],
    "answer": 1,
    "explanation": "The \"omnibus\" F-test for the set of dummy variables effectively acts like a One-Way ANOVA. It tests whether the categorical variable *as a whole* explains significant variance. A significant result means *at least* one group differs from another.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 463,
    "type": "mc",
    "question": "In R, what is the default coding scheme applied to factor variables in linear models (`lm`)?",
    "options": [
      "Effect Coding (`contr.sum`)",
      "Helmert Coding (`contr.helmert`)",
      "Dummy Coding (`contr.treatment`)",
      "Orthogonal Coding (`contr.poly`)"
    ],
    "answer": 2,
    "explanation": "By default, R uses `contr.treatment`, which corresponds to dummy coding. The first level of the factor (alphabetically or numerically) is automatically set as the reference group.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 464,
    "type": "mc",
    "question": "What is the primary benefit of using polynomial regression over transforming the data (e.g., log transformation) to handle non-linearity?",
    "options": [
      "Polynomials always provide a better fit (higher R^2).",
      "Polynomials allow you to model non-monotonic relationships (e.g., curves that go up and then down).",
      "Polynomials require fewer degrees of freedom.",
      "Transformations are mathematically invalid for regression."
    ],
    "answer": 1,
    "explanation": "Most standard transformations (log, sqrt) are monotonic (they keep going up or keep going down). They can linearize a curve, but they cannot model a relationship that changes direction (like an inverted U-shape peak). Polynomials (quadratic terms) are specifically designed for these non-monotonic bends.",
    "module": "Study-Guide-3",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 465,
    "type": "mc",
    "question": "When comparing non-nested models (e.g., Model A with predictors X1, X2 vs. Model B with predictors X3, X4), which statistic is most appropriate to use?",
    "options": [
      "The Likelihood Ratio Test (LRT).",
      "The F-statistic from the ANOVA table.",
      "The Akaike Information Criterion (AIC).",
      "The difference in R-squared values."
    ],
    "answer": 2,
    "explanation": "The Likelihood Ratio Test and F-tests are only valid for nested models (where one model is a subset of the other). For non-nested models, Information Criteria like AIC (or BIC) are the standard tools for comparison, as they balance model fit (likelihood) with model complexity.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 466,
    "type": "mc",
    "question": "What is the primary interpretation of the Intraclass Correlation (ICC) in a random intercept model?",
    "options": [
      "The correlation between the fixed effects and the random effects.",
      "The proportion of total variance in the outcome that is attributable to differences between groups (clusters).",
      "The correlation between two different predictor variables.",
      "The percentage of variance explained by the fixed predictors alone."
    ],
    "answer": 1,
    "explanation": "The ICC (Intraclass Correlation) ranges from 0 to 1 and represents the ratio of between-group variance to total variance. An ICC of 0.20 means that 20% of the variation in the outcome is due to systematic differences between the groups (e.g., classrooms, schools).",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 467,
    "type": "mc",
    "question": "In the context of the Likelihood Ratio Test, what distribution is used to determine the statistical significance of the difference between two models?",
    "options": [
      "The t-distribution.",
      "The F-distribution.",
      "The Chi-square distribution.",
      "The Normal distribution."
    ],
    "answer": 2,
    "explanation": "The test statistic for the Likelihood Ratio Test is -2 times the difference in log-likelihoods. This statistic follows a Chi-square distribution with degrees of freedom equal to the difference in the number of parameters between the two models.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 468,
    "type": "mc",
    "question": "When fitting a mixed model in R using `lmer`, why is it important to use `REML = FALSE` (Maximum Likelihood) when comparing models with different fixed effects?",
    "options": [
      "REML estimates are biased for fixed effects.",
      "REML assumes the random effects are zero.",
      "REML maximizes the likelihood of the residuals, making it invalid for comparing models with different fixed structures.",
      "Maximum Likelihood is faster to compute."
    ],
    "answer": 2,
    "explanation": "REML (Restricted Maximum Likelihood) estimates variance components by factoring out the fixed effects. If you change the fixed effects, the \"data\" REML uses effectively changes, making the likelihoods incomparable. Maximum Likelihood (ML) must be used for LRTs involving fixed effects.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 469,
    "type": "mc",
    "question": "A \"Random Intercept\" model implies that:",
    "options": [
      "The relationship (slope) between the predictor and outcome is identical for all groups, but the groups have different starting points.",
      "The relationship (slope) between the predictor and outcome varies across groups.",
      "The intercept is constrained to be zero for all groups.",
      "The model does not include any fixed predictors, only random noise."
    ],
    "answer": 0,
    "explanation": "In a random intercept model, each group gets its own intercept (shifting the regression line up or down), but the slope of the predictor lines is forced to be parallel (identical) across all groups.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 470,
    "type": "mc",
    "question": "What does a difference in AIC of greater than 10 (AIC > 10) typically indicate?",
    "options": [
      "The two models are statistically equivalent.",
      "The model with the higher AIC has essentially no support compared to the better model.",
      "The model with the lower AIC is overfitted.",
      "You should use the BIC instead."
    ],
    "answer": 1,
    "explanation": "Rules of thumb for AIC interpretation suggest that a AIC < 2 indicates substantial support for both models (they are similar), while a AIC > 10 provides very strong evidence against the model with the higher AIC, meaning the lower AIC model is clearly superior.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 471,
    "type": "mc",
    "question": "Which of the following is required for two models to be considered \"nested\"?",
    "options": [
      "They must define the same random effects structure.",
      "The parameters of the simpler model must be a subset of the parameters of the more complex model.",
      "They must have the same R-squared value.",
      "They must be fitted on different datasets."
    ],
    "answer": 1,
    "explanation": "Nested models exist when one model can be derived simply by constraining parameters of the other model to zero. For example, `Y ~ X1` is nested within `Y ~ X1 + X2`. If the predictors are completely different, they are non-nested.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 472,
    "type": "mc",
    "question": "In a mixed model output, if the variance of the \"School\" random effect is 0, what does this suggest?",
    "options": [
      "The model failed to converge.",
      "There is no variation in the outcome between schools; a standard regression model would suffice.",
      "The fixed effects explain all the variance.",
      "The schools are all identical in size."
    ],
    "answer": 1,
    "explanation": "A variance of 0 for the random intercept means that there are no systematic differences between the groups (schools). The multi-level structure is unnecessary, and the data could likely be analyzed using standard OLS regression without violating independence assumptions (regarding clustering).",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 473,
    "type": "mc",
    "question": "What is the formula for calculating AIC?",
    "options": [
      "AIC = -2(Log-Likelihood) + 2(k)",
      "AIC = R^2 + k",
      "AIC = 2(Log-Likelihood) - 2(k)",
      "AIC = Log-Likelihood / n"
    ],
    "answer": 0,
    "explanation": "AIC rewards goodness of fit (high log-likelihood, making the first term more negative) but penalizes complexity (k = number of parameters). The formula is designed to find the sweet spot between fit and parsimony.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 474,
    "type": "mc",
    "question": "You fit a random intercept and random slope model `(1 + X | Group)`. You find a strong negative correlation between the random intercepts and random slopes (-0.80). What does this mean?",
    "options": [
      "Groups with higher intercepts tends to have steeper positive slopes.",
      "Groups with higher intercepts tends to have flatter (or more negative) slopes.",
      "The model is misidentified and should be simplified.",
      "The random effects are independent."
    ],
    "answer": 1,
    "explanation": "A negative correlation means that as the intercept increases, the slope decreases. For example, in a learning study, students who start with high scores (high intercept) might show slower improvement (lower slope) than students who start low (ceiling effect or regression to the mean).",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 475,
    "type": "mc",
    "question": "Why is the Likelihood Ratio Test inappropriate for comparing a model with predictors X1 & X2 against a model with predictors X3 & X4?",
    "options": [
      "Because the sample sizes might be different.",
      "Because the models are non-nested.",
      "Because you cannot calculate log-likelihood for multiple regression.",
      "Because the error terms are not normally distributed."
    ],
    "answer": 1,
    "explanation": "The mathematical properties of the standard Likelihood Ratio Test (Chi-square distribution of the difference) only hold when the models are nested. For non-nested comparisons, this test statistic does not follow a known distribution, making the p-value invalid.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 476,
    "type": "mc",
    "question": "Which component of a mixed model captures the \"Between-Group\" variance?",
    "options": [
      "The residual variance.",
      "The fixed efffects coefficients.",
      "The variance of the random intercept.",
      "The standard error of the intercept."
    ],
    "answer": 2,
    "explanation": "In a random intercept model, the total variance is partitioned into \"variance of the random intercept\" (Between-Group) and \"residual variance\" (Within-Group). The random intercept variance quantifies how much the group means differ from each other.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 477,
    "type": "mc",
    "question": "What is the main goal of the \"Principle of Parsimony\" in model selection?",
    "options": [
      "To select the model with the highest possible R-squared.",
      "To select the simplest model that adequately explains the data.",
      "To include every possible interaction term to avoid bias.",
      "To maximize the log-likelihood regardless of complexity."
    ],
    "answer": 1,
    "explanation": "Parsimony (Occam's Razor) prefers simpler explanations. In statistical modeling, this means if two models fit the data equally well, the one with fewer parameters (less complex) is preferred to avoid overfitting and improve generalizability.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 478,
    "type": "mc",
    "question": "If you perform a Likelihood Ratio Test and get a p-value of 0.001, what is the conclusion?",
    "options": [
      "The simpler model is better.",
      "The more complex model provides a significantly better fit than the simpler model.",
      "The models are statistically equivalent.",
      "The more complex model is overfitted."
    ],
    "answer": 1,
    "explanation": "A significant p-value (e.g., < .05) in an LRT rejects the null hypothesis that the additional parameters are zero. It indicates that the improvement in model fit (reduction in deviance) achieved by adding the parameters is too large to be due to chance.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 479,
    "type": "mc",
    "question": "Log-Likelihood values are typically:",
    "options": [
      "Positive and between 0 and 1.",
      "Negative, with values closer to 0 (less negative) indicating better fit.",
      "Negative, with values further from 0 (more negative) indicating better fit.",
      "Zero."
    ],
    "answer": 1,
    "explanation": "Log-likelihoods are usually negative numbers (log of a probability < 1). A \"higher\" likelihood means a number closer to zero (e.g., -500 is better than -600). It measures how probable the observed data is given the model parameters.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 480,
    "type": "mc",
    "question": "When adding a \"Random Slope\" to a mixed model, what assumption are you relaxing?",
    "options": [
      "You are no longer assuming the residuals are normal.",
      "You are no longer assuming that the effect of the predictor is the same for every group.",
      "You are no longer assuming that the intercepts vary.",
      "You are removing the fixed effect of the predictor."
    ],
    "answer": 1,
    "explanation": "A fixed slope model assumes the effect of X on Y is universal (same coefficient for everyone). A random slope model relaxes this, allowing the slope coefficient to be different for each group (e.g., the treatment works better in School A than School B).",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 481,
    "type": "mc",
    "question": "Which of the following is a limitation of using R-squared for model comparison?",
    "options": [
      "It cannot be calculated for linear regression.",
      "It always increases (or stays the same) when you add more predictors, even if they are useless.",
      "It penalizes complex models too heavily.",
      "It implies causation."
    ],
    "answer": 1,
    "explanation": "R-squared is monotonically increasing; adding any variable, no matter how nonsensical, will slightly raise R-squared. This makes it a poor metric for choosing the \"best\" model, necessitating Adjusted R-squared or AIC which penalize complexity.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 482,
    "type": "mc",
    "question": "In a mixed model `lmer(score ~ time + (1 | student))`, what does the term `(1 | student)` specify?",
    "options": [
      "A fixed effect for student.",
      "A random intercept for each student.",
      "A random slope for time.",
      "A random intercept and random slope."
    ],
    "answer": 1,
    "explanation": "The `1` represents the intercept. The `|` represents the grouping structure. So `(1 | student)` translates to \"allow the intercept (1) to vary randomly by student.\"",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 483,
    "type": "mc",
    "question": "When comparing models using Akaike Weights, what does a weight of 0.75 for Model A imply?",
    "options": [
      "Model A explains 75% of the variance.",
      "There is a 75% probability that Model A is the best model among the set of candidate models considered.",
      "Model A is 75% better than the null model.",
      "The p-value for Model A is 0.75."
    ],
    "answer": 1,
    "explanation": "Akaike Weights transform the raw AIC differences into conditional probabilities. A weight of 0.75 means that, given the specific set of models tested, there is a 75% chance that Model A is the K-L information-best model.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 484,
    "type": "mc",
    "question": "Which scenario is the ideal use-case for a Mixed Linear Model?",
    "options": [
      "Determining the effect of Age on Salary in a single cross-sectional survey of 100 people.",
      "Predicting Coin Flips (Binary outcome).",
      "Analyzing the effect of a teaching method where students are nested within different classrooms.",
      "Analyzing time-series stock data to forecast future prices."
    ],
    "answer": 2,
    "explanation": "Mixed models are specifically designed for hierarchical or nested data structures (like students in classrooms) where observations within the same group are likely correlated (violating independence). Option B requires Logistic regression; Option D requires Time Series analysis.",
    "module": "Study-Guide-4",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 485,
    "type": "mc",
    "question": "What is the defining characteristic of a \"Growth Curve Model\"?",
    "options": [
      "It must use non-linear regression.",
      "It models the trajectory of change within individuals over time using repeated measures.",
      "It only applies to economic data (GDP growth).",
      "It assumes that all individuals change at exactly the same rate."
    ],
    "answer": 1,
    "explanation": "Growth curve models (or longitudinal multilevel models) use repeated observations of the same individuals to estimate both an average trajectory of change (fixed effect) and individual variations in that trajectory (random effects).",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 486,
    "type": "mc",
    "question": "In a linear growth model with \"Time\" coded as 0, 1, 2, 3..., what does the intercept represent?",
    "options": [
      "The rate of change per unit of time.",
      "The average outcome at the final time point.",
      "The average \"initial status\" or baseline value of the outcome.",
      "The residual error variance."
    ],
    "answer": 2,
    "explanation": "When time is coded such that the first observation is 0, the intercept term corresponds to the predicted value of the outcome when Time=0. This conceptually represents the \"starting point\" or initial status of the trajectory.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 487,
    "type": "mc",
    "question": "If you include a quadratic time term (`Time^2`) in a growth model and its coefficient is positive, what does this indicate about the trajectory?",
    "options": [
      "The rate of growth is constant.",
      "The growth is decelerating (slowing down).",
      "The growth is accelerating (curving upward) over time.",
      "The relationship is not significant."
    ],
    "answer": 2,
    "explanation": "In a curve, the quadratic term determines the concavity. A positive coefficient for a squared term indicates a \"U\" shape or upward curvature. If the linear trend is positive, a positive quadratic term means the growth is getting faster and faster (acceleration).",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 488,
    "type": "mc",
    "question": "In a mixed model, what does it mean if the Residual Variance is very large compared to the Random Intercept Variance?",
    "options": [
      "Most of the variability in the data is due to differences *between* groups.",
      "Most of the variability in the data is due to differences *within* groups (noise or individual fluctuation).",
      "The model is a perfect fit.",
      "The ICC is close to 1."
    ],
    "answer": 1,
    "explanation": "Residual variance captures within-group variability. If this is large relative to the random intercept (between-group) variability, it means the groups are actually quite similar to each other, and most of the \"action\" is happening at the individual level within groups (low ICC).",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 489,
    "type": "mc",
    "question": "How is a \"Time-Varying Covariate\" different from a standard predictor in a growth model?",
    "options": [
      "It is constant for each individual (e.g., Gender).",
      "It changes value at each measurement point for the same individual (e.g., Daily Stress).",
      "It is the outcome variable.",
      "It is used to define the random slopes."
    ],
    "answer": 1,
    "explanation": "A time-varying covariate (Level 1 predictor) allows for checking whether fluctuations in a predictor *at a specific time point* predict the outcome *at that same time point*, over and above the general growth trend. E.g., \"Is regression on days with higher stress separate from the overall trend?\"",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 490,
    "type": "mc",
    "question": "You fit a growth model and find that the random slope variance for \"Time\" is not significantly different from zero. What does this suggest?",
    "options": [
      "There is no change over time on average.",
      "Everyone changes at approximately the same rate.",
      "The starting points (intercepts) are all the same.",
      "You should add a cubic term."
    ],
    "answer": 1,
    "explanation": "The random slope variance quantifies individual differences in the rate of change. If it is zero (or non-significant), it implies that although there may be an average trend (fixed effect), every individual follows that same trend line parallel to each other; there is no heterogeneity in growth rates.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 491,
    "type": "mc",
    "question": "What is the interpretation of the fixed effect for \"Treatment\" in a growth model `Outcome ~ Time + Treatment + Time*Treatment`?",
    "options": [
      "It represents the difference in rates of change between groups.",
      "It represents the difference between groups at Time = 0 (Initial Status).",
      "It represents the difference between groups at the final time point.",
      "It represents the average difference across all time points."
    ],
    "answer": 1,
    "explanation": "In the presence of an interaction with Time (where Time 0 is baseline), the main effect of Treatment represents the group difference specifically when Time = 0. Thus, it tests whether the treatment and control groups started at different levels before the intervention began.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 492,
    "type": "mc",
    "question": "Which of the following data structures is best handled by a mixed-effects model rather than a repeated-measures ANOVA?",
    "options": [
      "Balanced data with no missing values.",
      "Data where every participant is measured exactly 3 times at the same intervals.",
      "Unbalanced data where participants are measured at different irregular time points or have missing observations.",
      "Cross-sectional data."
    ],
    "answer": 2,
    "explanation": "One of the biggest advantages of mixed models (specifically growth curves) over RM-ANOVA is flexibility. They can handle missing data (under MAR) and participants having measurements at different times (e.g., Month 1, 3 vs Month 2, 4) without equating them or dropping cases.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 493,
    "type": "mc",
    "question": "In a growth curve model, if the correlation between the Intercept and the Slope is positive, what does it mean?",
    "options": [
      "Individuals with higher starting values tend to grow/increase faster.",
      "Individuals with higher starting values tend to grow slower (or decline).",
      "There is no relationship between starting point and growth rate.",
      "The model is invalid."
    ],
    "answer": 0,
    "explanation": "A positive correlation implies that as one variable increases, so does the other. High Intercept (start high) is associated with High Slope (steep increase). \"The rich get richer\" effect.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 494,
    "type": "mc",
    "question": "When would you center the \"Time\" variable at the *midpoint* of the study (e.g., -2, -1, 0, 1, 2)?",
    "options": [
      "When you want the intercept to represent the average outcome at the middle of the trajectory rather than the beginning.",
      "When you want to remove the effect of time.",
      "It is required for convergence.",
      "When you have missing data."
    ],
    "answer": 0,
    "explanation": "The intercept always represents the expected value when all predictors are 0. By centering time such that 0 is in the middle, the intercept becomes the estimated outcome at the halfway point of the study, which can sometimes be more stable or meaningful than the initial status.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 495,
    "type": "mc",
    "question": "In R's `lmer` syntax, how do you denote that you want a random intercept for \"Subject\" separate from a random intercept for \"Item\" (Crossed Random Effects)?",
    "options": [
      "`(1 | Subject / Item)`",
      "`(1 | Subject) + (1 | Item)`",
      "`(1 | Subject * Item)`",
      "`(Subject | Item)`"
    ],
    "answer": 1,
    "explanation": "Crossed random effects (where subjects see multiple items, and items are seen by multiple subjects, independently) are specified by adding two separate random intercept terms: `(1 | Subject)` for participant variance and `(1 | Item)` for stimulus variance.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 496,
    "type": "mc",
    "question": "What is the \"Spaghetti Plot\" useful for in longitudinal analysis?",
    "options": [
      "Checking for outliers in the residuals.",
      "Visualizing individual trajectories of change for all participants simultaneously.",
      "Plotting the single average regression line.",
      "Testing for normality."
    ],
    "answer": 1,
    "explanation": "A spaghetti plot displays the raw data lines for every single individual in the dataset on one graph. It allows for a visual assessment of individual variability in starting points and slopes before fitting a formal model.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 497,
    "type": "mc",
    "question": "Which of the following implies that a \"Fixed Effects Only\" model (standard OLS) would be inappropriate for your data?",
    "options": [
      "The Intraclass Correlation (ICC) is 0.01.",
      "The Likelihood Ratio Test comparing the mixed model to the null model is non-significant.",
      "The Intraclass Correlation (ICC) is 0.45.",
      "The residuals are normally distributed."
    ],
    "answer": 2,
    "explanation": "An ICC of 0.45 indicates that 45% of the variance is at the group level (clustering). This is a substantial violation of independence. Ignoring this structure with OLS would lead to severely biased standard errors (Type I error inflation).",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 498,
    "type": "mc",
    "question": "If you find that a quadratic growth model fits significantly better than a linear growth model, but the quadratic term is small, what precaution should you take?",
    "options": [
      "Discard the quadratic model as trivial.",
      "Plot the trajectory to ensure the curve is meaningful and not just a result of overfitting a small fluctuation.",
      "Use the linear model because it is simpler (Parsimony).",
      "Transform the time variable to a log scale."
    ],
    "answer": 1,
    "explanation": "Statistical significance does not always equal practical significance. With large samples, even tiny curvatures can be \"significant.\" Visualizing the fitted curve ensures that the non-linearity is a real, substantive feature of the data (e.g., a real peak or plateau) rather than a negligible wiggle.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 499,
    "type": "mc",
    "question": "What is the \"Unconditional Mean Model\" (or Empty Model) used for in the first step of building a mixed model?",
    "options": [
      "To test the hypothesis that the means are zero.",
      "To calculate the baseline ICC and determine if multi-level modeling is necessary.",
      "To estimate the fixed effect of the primary predictor.",
      "To check for outliers."
    ],
    "answer": 1,
    "explanation": "The unconditional mean model contains no predictors, only the random intercept. It partitions the variance into \"Between\" and \"Within\" components, allowing you to calculate the ICC. If ICC is negligible, you might stop there.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 500,
    "type": "mc",
    "question": "Which statement accurately describes the interpretation of the intercept in a model with \"Grand Mean Centered\" predictors?",
    "options": [
      "The expected outcome for the reference group.",
      "The expected outcome for an individual with average values on all predictors.",
      "The average outcome of the entire sample (ignoring predictors).",
      "The value of Y when X is zero (which may be impossible)."
    ],
    "answer": 1,
    "explanation": "When predictors are centered around the grand mean, a value of 0 represents the average person. Therefore, the intercept represents the predicted outcome for an \"average\" participant, which is often more interpretable than when X=0 is meaningless (e.g., Age=0 or IQ=0).",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 501,
    "type": "mc",
    "question": "In a study with students nested in schools, adding a school-level predictor (e.g., \"School Funding\") helps to explain which variance component?",
    "options": [
      "The Residual Variance (Within-School).",
      "The Random Intercept Variance (Between-School).",
      "The Total Variance equally.",
      "It depends on the ICC."
    ],
    "answer": 1,
    "explanation": "A Level-2 predictor (School Funding) is constant for all students in a school. Therefore, it can only explain differences *between* schools. It should reduce the variance of the random intercept component.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 502,
    "type": "mc",
    "question": "What is the problem with \"Complete Pooling\" (ignoring groups) in data analysis?",
    "options": [
      "It overfits the data.",
      "It assumes that group membership is irrelevant and ignores potential correlation of errors.",
      "It has too many parameters to estimate.",
      "It prevents the calculation of R-squared."
    ],
    "answer": 1,
    "explanation": "Complete pooling treats all data as one big independent sample. If clustering exists (e.g., students in classes), this violates the independence assumption, leading to standard errors that are too small and p-values that are too optimistic.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 503,
    "type": "mc",
    "question": "What is \"No Pooling\" (running separate regressions for each group)?",
    "options": [
      "It is the gold standard for nested data.",
      "It fails to utilize the shared information across groups, often resulting in noisy estimates for small groups.",
      "It assumes all groups are identical.",
      "It is computationally impossible."
    ],
    "answer": 1,
    "explanation": "No pooling means estimating a separate model for every single school/group. While valid, it ignores the commonalities. Small groups will have wild, unreliable estimates. Mixed models (\"Partial Pooling\") strike the balance by borrowing strength across groups.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 504,
    "type": "mc",
    "question": "If predictors in a mixed model are correlated with the random effects (Assumption of No Endogeneity violation at the group level), what is a potential solution?",
    "options": [
      "Use a fixed effects model (include dummy variables for every group) instead of a random effects model.",
      "Ignore the correlation; mixed models are robust.",
      "Add more random slopes.",
      "Increase the sample size."
    ],
    "answer": 0,
    "explanation": "One of the key assumptions of random effects models is that the random effects are uncorrelated with the fixed predictors. If this is violated (e.g., \"School Quality\" unmeasured random effect is correlated with \"Student SES\"), the estimates are biased. Fixed effects models (controlling for group ID) remove this bias by controlling for all time-invariant group characteristics.",
    "module": "Study-Guide-5",
    "codeSnippet": null,
    "codeOutput": null
  },
  {
    "id": 505,
    "type": "mc",
    "question": "**[Mock R Output]** Based on the `lmer` output below, what is the estimated variance of the random intercepts for the `School` grouping factor?",
    "options": [
      "5.040",
      "25.40",
      "80.15",
      "4.331"
    ],
    "answer": 1,
    "explanation": "The 'Variance' column for the School (Intercept) group shows 25.40.",
    "module": "Interpreting-R-output",
    "codeSnippet": "Linear mixed model fit by REML ['lmerMod']\nFormula: MathScore ~ SES + (1 | School)\n   Data: student_data\n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n School   (Intercept) 25.40    5.040   \n Residual             80.15    8.953   \nNumber of obs: 2000, groups:  School, 50\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  50.123      0.821   61.05\nSES           4.331      0.150   28.87",
    "codeOutput": null
  },
  {
    "id": 506,
    "type": "mc",
    "question": "**[Mock R Output]** Using the same output above, calculate the Intraclass Correlation Coefficient (ICC) for Schools.",
    "options": [
      "0.24",
      "0.32",
      "0.76",
      "0.05"
    ],
    "answer": 0,
    "explanation": "ICC = Variance_School / (Variance_School + Variance_Residual) = 25.40 / (25.40 + 80.15)  25.40 / 105.55  0.24.",
    "module": "Interpreting-R-output",
    "codeSnippet": "Random effects:\n Groups   Name        Variance Std.Dev.\n School   (Intercept) 25.40    5.040   \n Residual             80.15    8.953",
    "codeOutput": null
  },
  {
    "id": 507,
    "type": "mc",
    "question": "**[Mock R Output]** In the following `anova()` comparison between two nested models, is the more complex model (`model2`) significantly better than the simpler model (`model1`)?",
    "options": [
      "No, because the AIC of model2 is lower.",
      "No, because the p-value is greater than 0.05.",
      "Yes, because the p-value (0.000624) is less than 0.05 and AIC is lower.",
      "Cannot be determined from this output."
    ],
    "answer": 2,
    "explanation": "The Chi-squared test yields a highly significant p-value (<.001), and the AIC decreased from 1500.2 to 1490.5, indicating `model2` is the better fit.",
    "module": "Interpreting-R-output",
    "codeSnippet": "> anova(model1, model2)\nData: df\nModels:\nmodel1: Y ~ X1\nmodel2: Y ~ X1 + X2\n       npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nmodel1    3 1500.2 1512.5 -747.10   1494.2                         \nmodel2    4 1490.5 1505.8 -741.25   1482.5 11.703  1   0.000624 ***\n---\nSignif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1",
    "codeOutput": null
  },
  {
    "id": 508,
    "type": "mc",
    "question": "**[Mock R Output]** What does the coefficient for `treatTreatment` represent in the following dummy-coded regression output?",
    "options": [
      "The average pain score for the Treatment group.",
      "The average pain score for the Control (reference) group.",
      "The difference in average pain score between the Treatment group and the Control group.",
      "The sum of pain scores for both groups."
    ],
    "answer": 2,
    "explanation": "In dummy coding, the coefficient for the non-reference group represents the difference in means relative to the Reference group (Intercept).",
    "module": "Interpreting-R-output",
    "codeSnippet": "Call:\nlm(formula = Pain ~ treat, data = clinical_trial)\n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     6.500      0.400  16.250   <2e-16 ***\ntreatTreatment -2.100      0.566  -3.710   0.0003 ***",
    "codeOutput": null
  },
  {
    "id": 509,
    "type": "mc",
    "question": "**[Mock R Output]** Based on the `summary()` of the regression model below, which predictor variable is NOT statistically significant at the alpha = 0.05 level?",
    "options": [
      "Age",
      "Weight",
      "Height",
      "All are significant"
    ],
    "answer": 1,
    "explanation": "The p-value for Weight is 0.689, which is much larger than 0.05, indicating it is not statistically significant.",
    "module": "Interpreting-R-output",
    "codeSnippet": "Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   10.52       1.20    8.76   < 0.001\nAge            0.45       0.10    4.50   < 0.001\nWeight        -0.02       0.05   -0.40     0.689\nHeight         1.21       0.55    2.20     0.028",
    "codeOutput": null
  },
  {
    "id": 510,
    "type": "mc",
    "question": "**[Mock R Output]** A researcher runs a stepwise AIC model selection. Based on the output below, which model is the \"best\" among those tested (has the lowest AIC)?",
    "options": [
      "model_all",
      "model_dropX1",
      "model_dropX2",
      "model_dropX3"
    ],
    "answer": 1,
    "explanation": "AIC values should be minimized. `model_dropX1` has the lowest AIC (448.10) among the candidates.",
    "module": "Interpreting-R-output",
    "codeSnippet": "df      AIC\nmodel_all      5   450.20\nmodel_dropX1   4   448.10\nmodel_dropX2   4   455.60\nmodel_dropX3   4   449.90",
    "codeOutput": null
  },
  {
    "id": 511,
    "type": "mc",
    "question": "**[Mock R Output]** Interpret the interaction term `AgexConditionB` in this multiple regression output. (Variable `Condition` is dummy coded with Reference = A).",
    "options": [
      "The effect of Age on Score is 0.30 units stronger in Condition B than in Condition A.",
      "The effect of Age on Score is 0.30 units weaker in Condition B than in Condition A.",
      "The average Score in Condition B is 0.30 units higher than in A.",
      "Age explains 30% more variance in Condition B."
    ],
    "answer": 0,
    "explanation": "The interaction coefficient (0.30) represents the difference in slopes. It means separate slope for Condition B = Slope A (0.50) + Interaction (0.30) = 0.80. Thus, the age effect is stronger.",
    "module": "Interpreting-R-output",
    "codeSnippet": "Call:\nlm(formula = Score ~ Age * Condition, data = exp_data)\n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)     20.00       2.00   10.00   < .001\nAge              0.50       0.10    5.00   < .001\nConditionB       5.00       2.50    2.00     .046\nAge:ConditionB   0.30       0.12    2.50     .013",
    "codeOutput": null
  },
  {
    "id": 512,
    "type": "mc",
    "question": "**[Mock R Output]** What does the `Correlation of Fixed Effects` matrix in the `lmer` output tell you?",
    "options": [
      "The correlation between the random intercepts and random slopes.",
      "The correlation between the sampling distributions of the estimates for the Intercept and Time.",
      "The correlation between the observed variable Time and the Outcome.",
      "It indicates strong multicollinearity that invalidates the model."
    ],
    "answer": 1,
    "explanation": "This matrix shows the correlation between the *estimates* of the fixed parameters (the uncertainty), not the variables themselves. A high correlation here doesn't necessarily mean multicollinearity between predictors, but rather that the estimates are not independent.",
    "module": "Interpreting-R-output",
    "codeSnippet": "Correlation of Fixed Effects:\n     (Intr)\nTime -0.45",
    "codeOutput": null
  },
  {
    "id": 513,
    "type": "mc",
    "question": "**[Mock R Output]** In the following output for a polynomial regression `lm(Y ~ poly(X, 2))`, what does the `2` indicate in the command?",
    "options": [
      "We are controlling for 2 distinct covariates.",
      "We are fitting a second-degree (quadratic) polynomial.",
      "We are running the model on 2 separate groups.",
      "We are raising X to the power of 1/2."
    ],
    "answer": 1,
    "explanation": "The function `poly(X, degree)` generates orthogonal polynomial terms up to the specified degree. `2` means a quadratic model (Linear + Squared terms).",
    "module": "Interpreting-R-output",
    "codeSnippet": "> model <- lm(Y ~ poly(X, 2), data = df)\n> summary(model)\n...\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)   100.00       1.00  100.00   <2e-16\npoly(X, 2)1    15.00       1.00   15.00   <2e-16\npoly(X, 2)2    -5.00       1.00   -5.00   1.2e-06",
    "codeOutput": null
  },
  {
    "id": 514,
    "type": "mc",
    "question": "**[Mock R Output]** Look at the Shapiro-Wilk test result for the residuals of a regression model. Is the assumption of normality met?",
    "options": [
      "No, because p > 0.05.",
      "Yes, because p > 0.05, we fail to reject the null hypothesis of normality.",
      "No, because W is close to 1.",
      "Yes, because the p-value is significant."
    ],
    "answer": 1,
    "explanation": "A non-significant p-value (p > .05) in Shapiro-Wilk means we do NOT have evidence to reject the null hypothesis that distribution is normal. Thus, the assumption is arguably met (or at least not significantly violated).",
    "module": "Interpreting-R-output",
    "codeSnippet": "> shapiro.test(residuals(model))\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(model)\nW = 0.995, p-value = 0.452",
    "codeOutput": null
  },
  {
    "id": 515,
    "type": "mc",
    "question": "**[Mock R Output]** Which observation would you flag as a potential outlier based on the standardized residuals (`rstandard`) output below?",
    "options": [
      "Observation 2",
      "Observation 3",
      "Observation 4",
      "Observation 6"
    ],
    "answer": 1,
    "explanation": "Observation 3 has a standardized residual of 3.45. Values > 3 (or < -3) are typically flagged as potential outliers.",
    "module": "Interpreting-R-output",
    "codeSnippet": "> head(rstandard(model))\n         1          2          3          4          5          6 \n 0.1243566 -0.5632121  3.4521002  1.2003310 -0.9822311  0.0012312",
    "codeOutput": null
  },
  {
    "id": 516,
    "type": "mc",
    "question": "**[Mock R Output]** In the output for `ncvTest()` (Non-constant Variance Score Test), what does a significant p-value indicate?",
    "options": [
      "The assumption of linearity is violated.",
      "The assumption of normality is violated.",
      "The assumption of homoscedasticity is violated (heteroscedasticity is present).",
      "The assumption of independence is violated."
    ],
    "answer": 2,
    "explanation": "The null hypothesis of `ncvTest` is constant variance (homoscedasticity). A significant p-value (< .05) rejects this, indicating heteroscedasticity.",
    "module": "Interpreting-R-output",
    "codeSnippet": "> car::ncvTest(model)\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 12.55, Df = 1, p = 0.000396",
    "codeOutput": null
  },
  {
    "id": 517,
    "type": "mc",
    "question": "**[Mock R Output]** Consider the output from a regression with Effect Coding (Weights sum to 0). Predictor `Prog` has 3 levels: A, B, C. `Prog1` compares A vs Grand Mean. `Prog2` compares B vs Grand Mean. What is the effect of level C?",
    "options": [
      "+1.0",
      "-5.0",
      "+5.0",
      "0.0"
    ],
    "answer": 0,
    "explanation": "In effect coding, the effects sum to zero. Effect_A (2.0) + Effect_B (-3.0) + Effect_C = 0. Therefore, Effect_C = -(2.0 - 3.0) = -(-1.0) = +1.0.",
    "module": "Interpreting-R-output",
    "codeSnippet": "Coefficients:\n(Intercept)  50.0\nProg1        2.0\nProg2        -3.0",
    "codeOutput": null
  },
  {
    "id": 518,
    "type": "mc",
    "question": "**[Mock R Output]** Based on the `confint()` output below, for which predictor does the 95% Confidence Interval include zero (implying non-significance)?",
    "options": [
      "X1",
      "X2",
      "X3",
      "Intercept"
    ],
    "answer": 1,
    "explanation": "The interval for X2 spans from -0.50 to +0.80. Since it crosses zero, we cannot exclude the possibility that the true effect is zero at the 95% confidence level.",
    "module": "Interpreting-R-output",
    "codeSnippet": "> confint(model)\n                2.5 %     97.5 %\n(Intercept)  4.50321   10.23122\nX1           1.20011    2.45033\nX2          -0.50431    0.80331\nX3          -3.21000   -1.10022",
    "codeOutput": null
  },
  {
    "id": 519,
    "type": "mc",
    "question": "**[Mock R Output]** You run a Durbin-Watson test to check for autocorrelation. Based on the result, is there evidence of positive autocorrelation?",
    "options": [
      "No, D-W statistic is close to 2.",
      "Yes, D-W statistic is significantly less than 2 (closer to 0).",
      "Yes, D-W statistic is significantly greater than 2 (closer to 4).",
      "No, the p-value is not significant."
    ],
    "answer": 1,
    "explanation": "A Durbin-Watson statistic near 2 indicates no autocorrelation. Values approaching 0 indicate positive autocorrelation. 0.68 is low, and the p-value is significant.",
    "module": "Interpreting-R-output",
    "codeSnippet": "> durbinWatsonTest(model)\n lag Autocorrelation D-W Statistic p-value\n   1       0.6543212      0.685432   0.002",
    "codeOutput": null
  },
  {
    "id": 520,
    "type": "mc",
    "question": "**[Mock R Output]** What represents the \"Slope\" of the random effects in this `ranef()` output?",
    "options": [
      "The `(Intercept)` column.",
      "The `SES` column.",
      "The row names (1, 2, 3).",
      "There is no random slope in this model."
    ],
    "answer": 1,
    "explanation": "The `SES` column shows the random effect estimates for the SES predictor for each school. This indicates that the slope of SES varies by school.",
    "module": "Interpreting-R-output",
    "codeSnippet": "> ranef(mixed_model)\n$School\n   (Intercept)      SES\n1    2.1032122  0.50321\n2   -1.2033110 -0.10231\n3    0.4431221  0.00032",
    "codeOutput": null
  },
  {
    "id": 521,
    "type": "mc",
    "question": "**[Mock R Output]** Which R function was likely used to produce the following output comparing two models?",
    "options": [
      "`summary()`",
      "`anova()`",
      "`plot()`",
      "`predict()`"
    ],
    "answer": 1,
    "explanation": "The `anova()` function in R is the standard method for performing a Likelihood Ratio Test (LRT) comparison between nested mixed models.",
    "module": "Interpreting-R-output",
    "codeSnippet": "Likelihood ratio test\n\nModel 1: score ~ 1 + (1 | student)\nModel 2: score ~ time + (1 | student)\n      #Df  LogLik Df  Chisq Pr(>Chisq)    \nModel 1 3 -2450.2                         \nModel 2 4 -2400.5  1 99.421  < 2.2e-16 ***",
    "codeOutput": null
  },
  {
    "id": 522,
    "type": "mc",
    "question": "**[Mock R Output]** In the following output for a growth model, what indicates that individual variability in growth rates (slopes) is small or non-existent?",
    "options": [
      "The Residual variance is 25.0.",
      "The (Intercept) variance is 100.2.",
      "The Time variance is 0.00001.",
      "The correlation is 0.01."
    ],
    "answer": 2,
    "explanation": "The variance for the random slope (`Time`) is extremely close to zero, suggesting that essentially everyone is changing at the same rate, and a random slope may not be needed.",
    "module": "Interpreting-R-output",
    "codeSnippet": "Random effects:\n Groups   Name        Variance  Std.Dev. Corr \n Subject  (Intercept) 100.20000 10.0100       \n          Time          0.00001  0.0032  0.01 \n Residual              25.00000  5.0000",
    "codeOutput": null
  },
  {
    "id": 523,
    "type": "mc",
    "question": "**[Mock R Output]** Based on the `vif()` output, which variable is most problematic for multicollinearity?",
    "options": [
      "Age",
      "SES",
      "Income",
      "Education"
    ],
    "answer": 2,
    "explanation": "Income has a VIF of 12.40. A VIF > 10 is a common threshold for determining \"severe\" multicollinearity.",
    "module": "Interpreting-R-output",
    "codeSnippet": "> vif(model)\n   Age    SES Income Education \n  1.20   2.50  12.40      3.10",
    "codeOutput": null
  },
  {
    "id": 524,
    "type": "mc",
    "question": "**[Mock R Output]** In a logistic regression output ( `glm(family=\"binomial\")` ), how do you interpret the intercept of -2.30?",
    "options": [
      "The predicted probability of success when X=0 is -2.30.",
      "The predicted log-odds of success when X=0 is -2.30.",
      "The predicted odds ratio is -2.30.",
      "The threshold for classification is -2.30."
    ],
    "answer": 1,
    "explanation": "Logistic regression models the log-odds (logit). Therefore, the coefficients are in log-odds units. To get probability, you would calculate `exp(-2.30) / (1 + exp(-2.30))`.",
    "module": "Interpreting-R-output",
    "codeSnippet": "Coefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)   -2.30       0.50   -4.60  < 0.001\nX              0.80       0.20    4.00  < 0.001",
    "codeOutput": null
  }
]