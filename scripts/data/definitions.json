{
    "Module 1: Introduction & Probability": {
        "Binomial Distribution": "Describes the probability of k successes in n binary trials. Use `dbinom(k, n, p)` for exact probability and `pbinom(k, n, p)` for cumulative.",
        "R function prefixes": "`r` (random), `d` (density/probability), `p` (cumulative), `q` (quantile). e.g., `rnorm` generates random data.",
        "PDF Area": "The total area under a Probability Density Function curve is always 1.",
        "pnorm(z)": "Returns the cumulative probability (area to the left) for a Z-score. `pnorm(1.96)` is approx 0.975.",
        "Normal Distribution": "Symmetric, bell-shaped continuous distribution defined by mean and SD. approx 68% data within 1 SD, 95% within 2 SD.",
        "dbinom vs pbinom": "`dbinom` is for EXACT outcomes (e.g., exactly 4 heads). `pbinom` is for CUMULATIVE (e.g., 4 or fewer heads).",
        "Standard Normal (Z)": "Normal distribution with Mean = 0 and SD = 1.",
        "Distributions for Categorical Data": "Chi-square is often used for categorical/count data analysis.",
        "Skewness": "Measure of asymmetry. Positive skew = tail to right. Negative skew = tail to left.",
        "set.seed()": "Initializes random number generator for reproducibility. Same seed = same random numbers.",
        "Probability Range": "Probabilities are always between 0 and 1.",
        "Kurtosis": "Measure of tailedness. High kurtosis (leptokurtic) = heavy tails, potential outliers.",
        "qnorm(p)": "Inverse CDF. Finds the Z-score for a given cumulative probability. `qnorm(0.975)` is approx 1.96.",
        "Elementary Event": "A single outcome of an experiment (e.g., rolling a 5).",
        "F-distribution": "Used in ANOVA and regression to compare variances or models. Ratio of two chi-squares.",
        "Expected Value vs Observation": "Differences between observed data and expected value are due to sampling error (chance), assuming H0 is true.",
        "Continuous vs Discrete": "Normal is continuous. Binomial/Poisson are discrete.",
        "SD and Curve Shape": "Larger SD = flatter, wider curve. Smaller SD = taller, narrower curve.",
        "dbinom size argument": "`size` refers to the number of trials (n), not the sample size of the study.",
        "Purpose of Probability": "To quantify uncertainty and estimate likelihood of sample data given a hypothesis.",
        "Empirical Rule": "68-95-99.7 rule for Normal distributions.",
        "Law of Large Numbers": "As sample size increases, sample mean converges to population mean.",
        "z.test package": "`BSDA` package contains `z.test`.",
        "Symmetric Distribution Center": "Mean = Median = Mode.",
        "Sum of Probabilities": "Sum of all possible outcomes in sample space is 1."
    },
    "Module 2: Sampling Theory": {
        "Target Population": "The entire group you want to draw conclusions about.",
        "Sample": "The subset of the population you actually collect data from.",
        "Sampling Frame": "The list of reachable individuals from which the sample is drawn.",
        "Simple Random Sampling": "Every member of population has equal chance of selection.",
        "Convenience Sampling": "Selecting participants based on availability (e.g., students in cafeteria). Prone to bias.",
        "Snowball Sampling": "Participants recruit others. Useful for hard-to-reach populations.",
        "Stratified Sampling": "Dividing population into strata (subgroups) and sampling from each to ensure representation.",
        "WEIRD Samples": "Western, Educated, Industrialized, Rich, Democratic. Biased subset of humanity.",
        "Statistic vs Parameter": "Statistic describes Sample (Latin letters, M, s). Parameter describes Population (Greek letters, μ, σ).",
        "Standard Error (SEM)": "SD of the sampling distribution of the mean. `SEM = SD / sqrt(N)`. Decreases as N increases.",
        "Central Limit Theorem": "Sampling distribution of the mean becomes Normal as N increases, regardless of population shape.",
        "Population Mean Symbol": "μ (Mu).",
        "Sample SD Symbol": "s.",
        "Standard Error Definition": "The standard deviation of the sampling distribution.",
        "Confidence Interval (CI)": "Interval that captures the true parameter in 95% of repeated samples (for 95% CI). NOT '95% chance parameter is here'.",
        "Sampling Without Replacement": "Individuals cannot be selected twice.",
        "Sampling Bias": "Systematic error where sample does not represent population.",
        "CI and Replication": "In 100 reps, 5 CIs are expected to miss the mean (for 95% CI).",
        "Inference Goal": "Use sample statistics to estimate population parameters.",
        "SEM Formula": "SEM = σ / √N.",
        "Goal of Inferential Stats": "Generalize from sample to population.",
        "CI Functions in R": "`t.test()` gives CIs. `lsr::ciMean()` also used.",
        "SEM and Precision": "Large SEM = Imprecise estimate. Small SEM = Precise.",
        "Sample Size and CI Width": "Larger N -> Smaller SEM -> Narrower (more precise) CI.",
        "Taco Literacy Example": "Large discrepancies in percentages between samples suggest Sampling Variance or Bias.",
        "Non-probability Sampling": "Convenience, Snowball, Quota. (Random is Probability sampling)."
    },
    "Module 3: Hypothesis Testing": {
        "Null Hypothesis (H0)": "Assumption of No Effect, No Difference, or r = 0.",
        "Type I Error (Alpha)": "False Positive. Rejecting H0 when it's True. Risk = α.",
        "Type II Error (Beta)": "False Negative. Failing to reject H0 when it's False. Risk = β.",
        "p-value vs Alpha": "If p < α, Reject H0. If p > α, Fail to Reject.",
        "Test for Unknown Sigma": "T-test (uses sample s instead of population σ).",
        "Cohen's d": "Effect size for means. Standardized difference. 0.2 small, 0.5 medium, 0.8 large.",
        "Cohen's d Magnitude": "0.8 is Large.",
        "p-value inputs": "Needs Test Statistic and DF. Does NOT need Alpha to be calculated (only to be interpreted).",
        "Multiple Comparisons": "Increases Family-wise Error Rate (risk of at least one Type I error).",
        "Bonferroni Correction": "Adjusts alpha (α / number of tests) to control Family-wise error.",
        "Strength of Evidence": "Smaller p-value = Stronger evidence against H0.",
        "Statistical Power": "Probability of correctly rejecting false H0. Power = 1 - β.",
        "Critical Region": "The area in the tails where we reject H0. Split between tails for 2-tailed.",
        "T-distribution Shape": "Heavier tails than Normal to account for uncertainty in s estimating σ.",
        "t.test()": "Function for T-test in R.",
        "CI including 0": "For difference of means, if CI includes 0, result is Non-significant.",
        "Directional Hypothesis": "Requires One-tailed test (e.g., A > B).",
        "Alternative Hypothesis (H1)": "The research hypothesis (There is an effect).",
        "cohen.d Package": "`lsr` or `psych` packages often used.",
        "Sample Size and Significance": "Large N makes even tiny effects significant.",
        "Alpha Level": "Typically 0.05.",
        "p-value Definition": "Prob of observing data this extreme assuming H0 is true.",
        "Mad Men Example": "Correlation implies causation fallacy or 'Just Drunk' = Spurious/Chance.",
        "Reporting T-test": "Include t-value, df, and p-value.",
        "Z-test Requirement": "Requires known Population SD (σ)."
    },
    "Module 4: Correlation": {
        "Covariance": "Unstandardized measure of joint variability. Hard to interpret magnitude.",
        "Pearson's r Range": "-1 to +1.",
        "Strong Correlation": "Closer to -1 or +1. 0.85 is Strong.",
        "cor() function": "Calculates correlation in R.",
        "R-squared (Variance Explained)": "r^2. Proportion of variance in Y shared with X.",
        "Spearman's Rho": "Non-parametric rank correlation. Good for outliers or ordinal data.",
        "Correlation vs Causation": "Correlation does NOT imply causation.",
        "Partial Correlation": "Correlation between X and Y controlling for Z on BOTH.",
        "Semi-Partial Correlation": "Correlation between X and Y controlling for Z on X ONLY (or Y only).",
        "cor.test()": "Tests significance of correlation.",
        "Correlation Matrix": "Table of correlations between all pairs.",
        "Linearity Assumption": "Pearson r assumes linear relationship.",
        "Scatterplot": "Best plot to visualize correlation.",
        "r = 0": "No LINEAR relationship (could be U-shaped).",
        "Standardized Covariance": "Covariance / (SD_x * SD_y) = r.",
        "ppcor Package": "Used for partial correlations (`pcor`).",
        "Bootstrap CI": "Robust CIs without normality assumption.",
        "Effect Size r": "0.1 small, 0.3 medium, 0.5 large.",
        "Fisher's Z": "Transform r to Z for normal distribution (used in CIs / comparing correlations).",
        "Curvilinear Data": "Pearson r underestimates relationship for curved data (e.g., U-shape).",
        "Spurious Correlation": "Third variable causes relationship (e.g. Ice cream & Shark attacks)."
    },
    "Module 5: Simple Linear Regression": {
        "Regression Equation": "Y = b0 + b1*X + e. Intercept + Slope.",
        "R-squared in Regression": "Proportion of variance in Y explained by the model.",
        "lm()": "Linear Model function in R.",
        "Slope (b1)": "Change in Y for 1 unit increase in X.",
        "Residual": "Observed Y - Predicted Y.",
        "Homoscedasticity": "Constant variance of residuals across X.",
        "Q-Q Plot": "Checks Normality of residuals.",
        "Intercept (b0)": "Predicted Y when X = 0.",
        "Linearity Check": "Residuals vs Fitted plot (should be random scatter).",
        "Standard Error of Slope": "Precision of slope estimate.",
        "summary(model)": "Shows coefficients, R2, F-test.",
        "Significant t-test for Slope": "Predictor significantly predicts outcome.",
        "Heteroscedasticity": "Fan shape in residuals. Variance changes.",
        "Durbin-Watson": "Tests for independent errors (autocorrelation).",
        "Independence Violation": "Clustered/Time-series data violates this.",
        "Prediction": "Plug X into equation to get Y.",
        "F-statistic": "Tests if whole model is better than null model.",
        "Beta Weights": "Standardized coefficients. Compare predictor strength.",
        "residuals()": "Extracts residuals.",
        "OLS Principle": "Minimizes Sum of Squared Residuals.",
        "Degrees of Freedom": "df_residual = N - k - 1 (or N-2 for simple).",
        "Leverage": "How far an X value is from the mean X."
    },
    "Module 6: Multiple Linear Regression": {
        "Multiple Regression": "Predicting Y from multiple Xs.",
        "Multicollinearity": "High correlation between predictors. Bad.",
        "Adjusted R-squared": "Penalizes for adding useless predictors.",
        "VIF": "Variance Inflation Factor. > 10 is bad.",
        "Partial Coefficient": "Slope of X1 holding X2 constant.",
        "Suppression": "Predictor becomes significant only when another is added.",
        "Marginal vs Conditional R2": "(In mixed models context, but: Marginal=Fixed, Conditional=Fixed+Random).",
        "Adding Predictors": "R-squared always increases (or stays same). Adjusted R-squared might drop.",
        "Cook's Distance": "Measures influence of a point.",
        "Standardized Regression": "Mean=0, SD=1 for all vars.",
        "Tolerance": "1 / VIF.",
        "Reporting Results": "b, SE, t, p, R2.",
        "Hierarchical Regression": "Adding blocks of predictors.",
        "Residual Standard Error": "Average distance of points from regression plane.",
        "Cross-validation": "Assess overfitting.",
        "Hat Matrix": "Related to Leverage.",
        "Ridge Regression": "Penalty method for Multicollinearity."
    },
    "Module 7: Outliers": {
        "Outlier Definition": "Unusual data point.",
        "IQR Method": "Outlier if < Q1 - 1.5*IQR or > Q3 + 1.5*IQR.",
        "Cook's Distance Cutoff": "D > 1 or 4/N often used.",
        "Standardized Residual Cutoff": "> 3 is extreme.",
        "Leverage vs Influence": "High leverage = extreme X. Influence = affects coefficients (Cook's D).",
        "boxplot.stats()": "Finds IQR outliers.",
        "DFBETAS": "Change in ONE coefficient if case removed.",
        "Handling Outliers": "Investigate first. Don't just delete.",
        "Studentized Residuals": "Residual divided by its standard error (deleted).",
        "Mahalanobis Distance": "Multivariate outlier distance.",
        "Robust Regression": "Less sensitive to outliers (e.g., RLM).",
        "Winsorizing": "Cap extreme values at a percentile.",
        "Grubbs Test": "Statistical test for single outlier.",
        "Dixon Q-test": "For small samples."
    },
    "Module 8: Interactions": {
        "Interaction Effect": "Effect of X1 depends on X2.",
        "Interaction Term": "X1 * X2.",
        "Simple Slopes": "Slope of X1 at specific levels of X2.",
        "Interpretation": "If interaction significant, main effects are conditional.",
        "Cross-over Interaction": "Lines cross. Effects reverse.",
        "Johnson-Neyman": "Finds regions of significance for moderator.",
        "Centering": "Helps reduce multicollinearity for interaction terms.",
        "Three-way Interaction": "Interaction depends on a 3rd variable.",
        "Plotting": "Visualizing slopes at different moderator levels.",
        "emmeans": "Package for simple slopes/marginal means.",
        "Moderation": "Synonym for interaction mechanism."
    },
    "Module 9: Categorical Variables": {
        "Dummy Coding": "0s and 1s. k-1 dummies for k levels.",
        "Reference Group": "The group coded 0 on all dummies.",
        "Intercept in Dummy": "Mean of Reference Group.",
        "Dummy Coefficient": "Difference between Group and Reference.",
        "Effect Coding": "Sum to 0. Intercept is Grand Mean.",
        "Contrast Coding": "Custom hypothesis tests.",
        "contrasts()": "R function to set coding.",
        "Helmert Coding": "Compare level to mean of subsequent levels.",
        "Polynomial Coding": "For ordered categories (Trend).",
        "Model Matrix": "Shows how R codes variables.",
        "Post-hoc Tests": "Compare all pairs (Tukey).",
        "Bonferroni in Post-hoc": "Corrects for multiple tests.",
        "Factorial Design": "Multiple categorical IVs."
    },
    "Module 10: Polynomial Regression": {
        "Polynomial Regression": "Used for non-linear (curved) relationships.",
        "Quadratic Term": "X^2. Creates a parabola (1 bend).",
        "Cubic Term": "X^3. Creates an S-shape (2 bends).",
        "poly(X, degree)": "Creates orthogonal polynomials in R.",
        "Orthogonal Polynomials": "Uncorrelated terms. Reduces multicollinearity.",
        "Raw Polynomials": "Correlated terms (X, X^2). suffer from multicollinearity.",
        "I(X^2)": "Wrapper to tell R to calculate square inside formula.",
        "Linear Term Interpretation": "Slope at the mean of X (if orthogonal) or at X=0 (if raw).",
        "Overfitting": "Fitting noise instead of signal with too high degree.",
        "AIC/BIC": "Use to select optimal degree.",
        "Centering": "Essential for raw polynomials to reduce multicollinearity.",
        "Turning Point": "Where the curve changes direction (e.g. peak of parabola).",
        "Adjusted R-squared": "Use to check if extra term adds value."
    },
    "Module 11: Model Comparison": {
        "AIC": "Akaike Information Criterion. Lower is better. Penalizes parameter count.",
        "BIC": "Bayesian Information Criterion. Penalizes complexity more than AIC.",
        "Likelihood Ratio Test": "Compares nested models. 2*(LL_complex - LL_simple).",
        "anova(m1, m2)": "Performs LRT in R for nested models.",
        "Deviance": "-2 * LogLikelihood. Measure of misfit.",
        "Nested Models": "One model is a subset of the other (e.g., dropped a term).",
        "Cross-Validation": "Splitting data (Train/Test) to estimate generalization error.",
        "k-fold CV": "Split into k parts. Train on k-1, test on 1. Repeat.",
        "LOOCV": "Leave-One-Out CV. k = sample size.",
        "Underfitting": "Model too simple.",
        "Parsimony": "Simpler model is better (if fit is similar).",
        "Stepwise Selection": "Auto-selection (Forward/Backward). Can be biased.",
        "Model Averaging": "Weighted average of predictions from multiple models.",
        "Information Criteria": "AIC/BIC. Used for non-nested comparisons too."
    },
    "Module 12: Mixed Models": {
        "Mixed Model (HLM)": "Contains Fixed and Random effects. Handles nested/clustered data.",
        "Fixed Effects": "Constant across groups. Population average.",
        "Random Effects": "Vary across groups/subjects.",
        "Random Intercept": "(1|Group). Each group has own baseline.",
        "Random Slope": "(X|Group). Effect of X varies by group.",
        "Clustering/Nesting": "Observations within groups (Students in Classes).",
        "ICC": "Intraclass Correlation. Variance_between / Total_Variance. >0 means clustering matters.",
        "lmer()": "Function for mixed models (lme4 package).",
        "REML": "Restricted Maximum Likelihood. Unbiased variance estimates. Default.",
        "ML": "Maximum Likelihood. Use for comparing models with different Fixed Effects.",
        "Singular Fit": "Variance close to 0 or perfect correlation. Model too complex.",
        "BLUPs": "Best Linear Unbiased Predictors. The specific random effects for each group.",
        "lmerTest": "Package to get p-values for lmer (Satterthwaite approximation).",
        "Marginal R2": "Variance explained by Fixed effects.",
        "Conditional R2": "Variance explained by Fixed + Random effects."
    },
    "Module 13: Growth Curve Models": {
        "Growth Curve Model": "Mixed model for longitudinal data.",
        "Time as Predictor": "Models change over time.",
        "Linear Growth": "Intercept + Slope*Time.",
        "Unconditional Means Model": "Intercept-only (Null). Baseline for comparison.",
        "Unconditional Growth Model": "Time only (no other predictors).",
        "Random Intercept (Growth)": "Individuals differ in starting point.",
        "Random Slope (Growth)": "Individuals differ in rate of change.",
        "Centering Time": "Setting Time=0 point (e.g. Wave 1 vs Baseline). Interpretation changes.",
        "Within-Person Variance": "Variance over time for one person (Residual).",
        "Between-Person Variance": "Variance between people (Intercept/Slope variance).",
        "Quadratic Growth": "Acceleration/Deceleration.",
        "Spaghetti Plot": "Visualizing individual trajectories.",
        "Convergence Issues": "Common in complex growth models.",
        "Missing Data": "Mixed models handle MAR (Missing At Random) well."
    }
}