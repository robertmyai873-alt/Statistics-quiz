const fs = require('fs');
const path = require('path');

const questionsPath = path.join(__dirname, '../web-app/src/data/questions.json');
const questions = JSON.parse(fs.readFileSync(questionsPath, 'utf8'));

const fixes = {
    104: { 1: "It is continuous, symmetric, and \"bell-shaped\" (follows the Empirical Rule)." },
    109: { 1: "It ensures that random number generation is reproducible (same seed = same results)." },
    111: { 0: "There are more frequent extreme values (outliers) than in a normal distribution." },
    114: { 2: "ANOVA (Analysis of Variance) and Regression." },
    121: { 1: "As the number of trials increases, the sample mean gets closer to the expected value." },
    133: { 0: "Approach a normal distribution, regardless of the population distribution shape." },
    136: { 2: "If we replicated the study many times, 95% of the calculated intervals would contain the true mean." },
    142: { 1: "To use sample data to make generalizations about a larger population." },
    143: { 1: "`ciMean()` (from `lsr` package) or `t.test()`." },
    145: { 1: "The sample does not accurately reflect the population (some members have lower/higher probability of selection)." },
    146: { 1: "Become narrower (more precise)." },
    149: { 1: "There is no effect, no difference, or the relationship is zero." },
    150: { 0: "You reject the null hypothesis when it is actually true (False Positive)." },
    151: { 1: "You fail to reject the null hypothesis when it is actually false (False Negative)." },
    154: { 1: "Effect size (standardized mean difference)." },
    157: { 1: "Type I error (Family-wise error rate)." },
    160: { 0: "The probability of rejecting a false null hypothesis (1 - Beta)." },
    161: { 2: "The extreme 2.5% on both ends of the distribution." },
    162: { 1: "To account for the extra uncertainty introduced by estimating the population standard deviation from the sample." },
    164: { 1: "The difference is NOT statistically significant (at alpha = 0.05)." },
    167: { 1: "`psych` (specifically `cohen.d`) or `lsr`." },
    170: { 1: "The probability of obtaining the observed data (or more extreme) assuming the Null Hypothesis is true." },
    171: { 1: "The hypothesis was rejected (Null Hypothesis of no relationship was false), or it was a Spurious Correlation." },
    173: { 1: "The population standard deviation (sigma)." },
    176: { 2: "Correlation measures the strength and direction of a linear relationship between two variables." },
    177: { 1: "Covariance depends on the units of measurement, whereas correlation is standardized." },
    178: { 1: "The product of the standard deviations of the two variables." },
    179: { 1: "The proportion of variance in one variable that is predictable from the other variable." },
    182: { 0: "Only one of the variables (usually the predictor/IV) and the third variable." },
    185: { 0: "A third variable (confounder) that influences both observed variables." },
    186: { 1: "A one-tailed hypothesis that the correlation is positive." },
    191: { 0: "Every variable is perfectly correlated with itself (r = 1)." },
    203: { 0: "It represents the percent of variance in one variable shared with the other." },
    207: { 2: "$\\rho = 0$ (The population correlation coefficient is zero)." },
    208: { 1: "It controls for a third variable on *only* the predictor (or only the outcome), not both." },
    211: { 1: "No *linear* relationship exists between the variables." },
    212: { 2: "The correlation is not statistically significant (since the interval includes 0)." },
    214: { 2: "The Y-intercept (value of Y when X is 0)." },
    217: { 0: "The difference between the observed Y and the predicted Y ($Y_i - \\hat{Y}_i$)." },
    220: { 1: "The model explains significantly more variance than an intercept-only model." },
    221: { 2: "The total variability in the outcome variable (before accounting for predictors)." },
    222: { 2: "Residual Standard Error (RSE) or Standard Error of the Estimate." },
    225: { 1: "Compare the strength of predictors measured on different scales." },
    226: { 1: "`effectsize` or `QuantPsyc` (specifically `lm.beta`)." },
    228: { 1: "Because a predictor value of 0 might be impossible or meaningless in the real world." },
    230: { 1: "The predicted Y for a person with the average age." },
    235: { 1: "At least one regression assumption has been violated (e.g., Normality of residuals)." }, // Context dependent, but plausible
    236: { 2: "Independence of observations (needs study design context)." },
    237: { 1: "Data are collected from the same subjects (repeated measures) or clustered groups." },
    238: { 1: "A dashboard of visual plots checking regression assumptions (Normality, Homoscedasticity, etc.)." },
    239: { 1: "Assess how much regression coefficients change if a specific case is removed (measure of influence)." },
    242: { 1: "It changes the reference category for a factor variable." },
    243: { 1: "The variance of the residuals is constant across all levels of the predictor(s)." },
    248: { 1: "$X_1$ is a significant predictor, but $X_2$ is not." }, // Specific to missing output, generic guess
    250: { 1: "AIC (Akaike Information Criterion) or BIC." },
    251: { 1: "Adding `airplay` (Model 2) significantly reduced the residual sum of squares compared to Model 1." },
    254: { 0: "Points further out indicate better fit or higher values on that metric." },
    255: { 1: "The new predictor does not justify the added complexity (penalty for extra parameters outweighs fit improvement)." },
    257: { 1: "Will fail to estimate unique coefficients (Singularity / Perfect Multicollinearity)." },
    258: { 1: "You enter variables in blocks/steps based on theoretical order." },
    259: { 1: "The effect of one predictor on the outcome depends on the level of another predictor." },
    260: { 2: "The interaction effect (change in the slope of X associated with a 1-unit increase in Z)." },
    261: { 1: "`age * education` (or `age + education + age:education`)." },
    262: { 0: "The negative effect of `stress` on the outcome becomes *less negative* (weaker) as support increases." },
    263: { 1: "As conditional effects (simple slopes) when the other variable is 0." },
    264: { 1: "Probing an interaction by examining the slope of one predictor at specific levels of the moderator." },
    265: { 0: "`interactions` or `jtools`." },
    267: { 1: "It reduces multicollinearity between the main effects and the interaction term." },
    268: { 1: "A crossover (disordinal) or significant interaction." },
    269: { 0: "The slope of `Dose` for the Treatment group vs. the Control group." },
    273: { 0: "Find the specific range of the moderator where the effect of the focal predictor is significant." },
    275: { 1: "The confidence intervals for the regression coefficients." },
    278: { 0: "Residuals vs Leverage (Cook's distance) plot." },
    279: { 1: "Perfect multicollinearity (singularity) or the variable is a constant." },
    280: { 0: "2 predictors ($df_{mod}$) and 97 residual degrees of freedom." },
    281: { 0: "The value of Y when all predictors are at their mean (0 for standardized)." },
    283: { 1: "The effect is not statistically significant (p > 0.05)." },
    284: { 0: "Bootstrapping or transforming the dependent variable (e.g., log transformation)." },
    286: { 1: "Choosing the simplest model that explains the data adequately." },
    289: { 0: "`scatterplot3d` or `rgl`." },
    292: { 0: "`fit$coefficients` or `coef(fit)`." },
    293: { 0: "Has no correlation with the outcome but increases the predictive power of another variable by removing irrelevant variance." },
    294: { 1: "Overfitting / poor power (rule of thumb violation)." },
    295: { 1: "Regression degrees of freedom (number of predictors)." },
    301: { 1: "Inverted U-shaped (Concave; happiness rises then falls)." },
    304: { 1: "Multicollinearity (high correlation between x, x^2, x^3)." },
    309: { 1: "The quadratic term does not significantly improve model fit." },
    311: { 2: "To extrapolate predictions far outside the observed range of data." },
    312: { 1: "Fixed effects estimate population-level parameters; random effects estimate subject-specific deviations." },
    314: { 1: "To account for non-independence of observations (clustering)." },
    320: { 0: "80% of the variance in the outcome is due to differences between groups (clusters)." },
    323: { 1: "Subjects with higher baseline intercepts tend to have lower/more negative slopes." },
    324: { 2: "It should decrease (variance is \"explained\" by the fixed effect)." },
    326: { 1: "The random effects structure is too complex for the data (variance near 0)." },
    328: { 1: "Variance explained by the entire model (fixed + random effects)." },
    329: { 2: "mod2 (lower AIC and significant Likelihood Ratio Test)." },
    330: { 1: "To get p-values for the fixed effects (using Satterthwaite approximation)." },
    331: { 1: "Mixed-Effects Model (Multilevel Model)." },
    332: { 2: "It must be longitudinal (repeated measures over time)." },
    334: { 2: "Covariates or predictors of change (time-invariant or time-varying)." },
    335: { 1: "Polynomial terms for Time (e.g., Time^2)." },
    338: { 1: "Group B starts 10.00 units higher than Group A." },
    339: { 1: "Increases initially, but the rate of growth slows down (decelerates)." },
    340: { 1: "Each subject to have their own starting point AND their own rate of change." },
    341: { 0: "To make the intercept interpretable as the status at the beginning of the study." },
    342: { 1: "Everyone changes at the exact same rate (no individual differences in slope)." },
    344: { 1: "Random intercepts and random slopes for all within-subject factors." },
    347: { 2: "Mixed-Effects Model (`lmer`) with random intercepts for Classrooms." },
    348: { 1: "Polynomial regression (quadratic term)." },
    349: { 1: "Growth Curve Model (Mixed Model with Time)." },
    350: { 2: "You cannot directly compare them using AIC/BIC if outcomes differ." },
    351: { 1: "The mindless checking of p < 0.05 without considering effect size or context." },
    355: { 2: "No, because p > 0.05, we fail to reject the null hypothesis of normality." },
    356: { 1: "Yes, x3 has a VIF of 8.5, which is notably high (rule of thumb > 5 or 10)." },
    359: { 1: "Diet3 chicks grow at a rate 4 units *faster* per time unit than Diet1 (reference)." },
    360: { 1: "The random effects variance is estimated to be zero or effectively zero." },
    365: { 2: "It reduces the correlation (multicollinearity) between the linear and quadratic terms." },
    366: { 0: "A method for estimating degrees of freedom in mixed models." },
    367: { 1: "Independence of observations (residuals are correlated)." },
    369: { 1: "No clustering (all variance is within groups/residual)." },
    370: { 2: "The Quadratic Time slope (`I(Time^2)`)." },
    372: { 1: "The type of polynomial used, the random effects structure, and fit indices." },
    374: { 1: "No, because orthogonal polynomials are on a transformed scale, not the raw units." },
    377: { 0: "It allows the relationship between a predictor and outcome to vary across groups." },
    378: { 1: "The random deviation of subject $s$'s slope from the overall average slope." },
    380: { 1: "Compare models with different **fixed effects** structure." },
    381: { 1: "This is essential multicollinearity; centering Time would reduce it." },
    382: { 0: "Because the effect of Time depends on which Diet the subject is in." },
    386: { 1: "It is not statistically significant (95% CI includes 0)." },
    388: { 1: "Stick with the simpler model (m0) because the random slope did not significantly improve fit." },
    389: { 2: "The sum of fixed effects + random effects for each group (BLUPs)." },
    390: { 0: "The fixed effects estimates (population averages)." },
    395: { 1: "To see how much variance the *entire* model (including random effects) explains." },
    396: { 1: "Homoscedasticity (Homogeneity of Variance)." },
    397: { 2: "Zero (by definition of orthogonality)." }
};

let fixedCount = 0;
questions.forEach(q => {
    if (fixes[q.id]) {
        const questionFixes = fixes[q.id];
        Object.keys(questionFixes).forEach(optIndex => {
            // Verify that the old option matches what we expect (contains "...")
            if (q.options[optIndex].includes('...')) {
                q.options[optIndex] = questionFixes[optIndex];
                fixedCount++;
            } else {
                console.warn(`Skipping ID ${q.id} index ${optIndex}: text does not contain '...'`);
            }
        });
    }
});

console.log(`Applied manual fixes to ${fixedCount} items.`);
fs.writeFileSync(questionsPath, JSON.stringify(questions, null, 2));
